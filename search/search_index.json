{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hands-on tutorial for going from day-1 to production on DigitalOcean Kubernetes . Abstract The decision to adopt Kubernetes shouldn\u2019t be taken lightly. It\u2019s a complex system that requires hard work and dedication. In fact, when it comes to using Kubernetes, the best value comes from adopting a DevOps culture. Kubernetes isn\u2019t just installing something on a virtual machine (VM), it\u2019s a transformative way of viewing IT and enabling your teams. Automation is a direct path to becoming more productive. Kubernetes can make every aspect of your applications and tools accessible to automation, including role-based access controls, ports, databases, storage, networking, container builds, security scanning, and more. Kubernetes gives teams a well-documented and proven system that they can use to automate so that they can focus on outcomes. Startups and small-to-medium-sized businesses (SMBs) are uniquely positioned in their journeys. Unlike enterprises, they typically don\u2019t have dedicated departments for operations or security, and they usually run lean development teams. At the same time, SMBs are ultra-efficient, sometimes going from discovery phases to production in a matter of two or three weeks. When startups and SMBs consider adopting Kubernetes, they need to retain their ability to be flexible and pivot quickly while benefiting from the scalability and automation provided by Kubernetes. To achieve both without large and ongoing maintenance overhead, SMBs may consider the hands-free operations provided by Managed Kubernetes offerings. In this guide, you\u2019ll find recommendations for SMBs at any stage of the Kubernetes adoption journey. From the incipient stages of discovery and development through staging, production, and ultimately scaling applications, we\u2019ll offer a simple yet comprehensive approach to getting the most out of Kubernetes. Guide Overview This guide will teach you how to: Install and configure required tools to build applications running on DigitalOcean Kubernetes . Set up a DigitalOcean Container Registry for storing application images. Deploy the online boutique sample application consisting of several microservices, used as a demonstration through this guide. Perform local development for the online boutique sample application using Tilt . Set up a DigitalOcean Kubernetes cluster to perform remote development using Tilt. Set up staging and production environments to promote the online boutique sample application to upper environments. Set up CI/CD workflows using GitHub actions to build and deploy the online boutique sample application to each environment after a code change (or PR merge). Set up observability for each environment - logging, monitoring, alerting, etc. Automatically scale application workloads on demand. Please proceed with the Getting started -> Installing required tools section.","title":"Intro"},{"location":"#abstract","text":"The decision to adopt Kubernetes shouldn\u2019t be taken lightly. It\u2019s a complex system that requires hard work and dedication. In fact, when it comes to using Kubernetes, the best value comes from adopting a DevOps culture. Kubernetes isn\u2019t just installing something on a virtual machine (VM), it\u2019s a transformative way of viewing IT and enabling your teams. Automation is a direct path to becoming more productive. Kubernetes can make every aspect of your applications and tools accessible to automation, including role-based access controls, ports, databases, storage, networking, container builds, security scanning, and more. Kubernetes gives teams a well-documented and proven system that they can use to automate so that they can focus on outcomes. Startups and small-to-medium-sized businesses (SMBs) are uniquely positioned in their journeys. Unlike enterprises, they typically don\u2019t have dedicated departments for operations or security, and they usually run lean development teams. At the same time, SMBs are ultra-efficient, sometimes going from discovery phases to production in a matter of two or three weeks. When startups and SMBs consider adopting Kubernetes, they need to retain their ability to be flexible and pivot quickly while benefiting from the scalability and automation provided by Kubernetes. To achieve both without large and ongoing maintenance overhead, SMBs may consider the hands-free operations provided by Managed Kubernetes offerings. In this guide, you\u2019ll find recommendations for SMBs at any stage of the Kubernetes adoption journey. From the incipient stages of discovery and development through staging, production, and ultimately scaling applications, we\u2019ll offer a simple yet comprehensive approach to getting the most out of Kubernetes.","title":"Abstract"},{"location":"#guide-overview","text":"This guide will teach you how to: Install and configure required tools to build applications running on DigitalOcean Kubernetes . Set up a DigitalOcean Container Registry for storing application images. Deploy the online boutique sample application consisting of several microservices, used as a demonstration through this guide. Perform local development for the online boutique sample application using Tilt . Set up a DigitalOcean Kubernetes cluster to perform remote development using Tilt. Set up staging and production environments to promote the online boutique sample application to upper environments. Set up CI/CD workflows using GitHub actions to build and deploy the online boutique sample application to each environment after a code change (or PR merge). Set up observability for each environment - logging, monitoring, alerting, etc. Automatically scale application workloads on demand. Please proceed with the Getting started -> Installing required tools section.","title":"Guide Overview"},{"location":"01-getting-started/do-api-auth/","text":"Introduction This section will show you how to authenticate with the DigitalOcean API . To use the API, you\u2019ll first generate a personal access token. A personal access token allows you to use automation tools such as doctl to create and manage various DigitalOcean cloud resources, such as Kubernetes Clusters , Droplets , Container Registries , etc. Prerequisites To complete this section, you will need: A DigitalOcean account for accessing the DigitalOcean platform. A DigitalOcean personal access token for using the DigitalOcean API. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Authenticating with the DigitalOcean API First you will need to initialze doctl by running the following command: doctl auth init After entering your token doctl should be able to validate your token. Test to ensure that your account is configured for doctl to use: doctl auth list You should see a line containing your account and the \"current\" string next to it. For more info on this topic please see this Kubernetes Starter Kit Authentication Section . Next, you will learn how to create a DigitalOcean Container Registry to store all microservices Docker images used in this guide for demonstration.","title":"Authenticating with the DigitalOcean API"},{"location":"01-getting-started/do-api-auth/#introduction","text":"This section will show you how to authenticate with the DigitalOcean API . To use the API, you\u2019ll first generate a personal access token. A personal access token allows you to use automation tools such as doctl to create and manage various DigitalOcean cloud resources, such as Kubernetes Clusters , Droplets , Container Registries , etc.","title":"Introduction"},{"location":"01-getting-started/do-api-auth/#prerequisites","text":"To complete this section, you will need: A DigitalOcean account for accessing the DigitalOcean platform. A DigitalOcean personal access token for using the DigitalOcean API. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section.","title":"Prerequisites"},{"location":"01-getting-started/do-api-auth/#authenticating-with-the-digitalocean-api","text":"First you will need to initialze doctl by running the following command: doctl auth init After entering your token doctl should be able to validate your token. Test to ensure that your account is configured for doctl to use: doctl auth list You should see a line containing your account and the \"current\" string next to it. For more info on this topic please see this Kubernetes Starter Kit Authentication Section . Next, you will learn how to create a DigitalOcean Container Registry to store all microservices Docker images used in this guide for demonstration.","title":"Authenticating with the DigitalOcean API"},{"location":"01-getting-started/installing-required-tools/","text":"Introduction This section will show you how to install the required tools needed to complete this guide. You will focus on most popular tools used in the Kubernetes world. You will also install additional programs used for interacting with the DigitalOcean API, and local development enablement. Below is a complete list of the tools used in this guide: Kubectl - this the official Kubernetes client. Allows you to interact with the Kubernetes API, and to run commands against Kubernetes clusters. Helm - this is the package manager for Kubernetes. Behaves the same way as package managers used in Linux distributions, but for Kubernetes. Gained a lot of popularity, and it is a widely adopted solution for managing software packages installation and upgrade in Kubernetes. Doctl - allows you to interact with the DigitalOcean API via the command line. It supports most functionality found in the control panel. You can create, configure, and destroy DigitalOcean resources like Droplets, Kubernetes clusters, firewalls, load balancers, database clusters, domains, and more. Docker Desktop - enables you to build and share containerized applications and microservices using Docker. It has a GUI interface, and bundles a ready to run Kubernetes cluster to use for local development. Kustomize - Kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is. Tilt - eases local development by taking away the pain of time consuming Docker builds, watching files, and bringing environments up to date. Prerequisites To complete this section, you will need: Homebrew if you are using a macOS system. Curl package installed on your system. Installing Docker Desktop Depending on your operating system, you can install docker-desktop in the following ways: MacOS Linux - Ubuntu Install docker-desktop using Homebrew: brew install --cask docker Test to ensure you installed the latest version: docker version Update apt package index: sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker's official GPG key: sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg Set up the repository: echo \\ \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install docker desktop: sudo apt install docker-desktop Test to ensure you installed the latest version: docker version Installing Doctl Depending on your operating system, you can install doctl in the following ways: MacOS Linux Install doctl using Homebrew: brew install doctl Test to ensure you installed the latest version: doctl version Download the latest doctl package (check releases page): curl -LO https://github.com/digitalocean/doctl/releases/download/v1.79.0/doctl-1.79.0-linux-amd64.tar.gz Extract the doctl package: tar xf doctl-1.79.0-linux-amd64.tar.gz Set the executable flag, and make the doctl binary available in your path: chmod +x doctl sudo mv doctl /usr/local/bin Test to ensure you installed the latest version: doctl version Installing Kubectl Depending on your operating system, you can install kubectl in the following ways: MacOS Linux Install kubectl using Homebrew: brew install kubectl Test to ensure you installed the latest version: kubectl version --client Download the latest kubectl release: curl -LO \"https://dl.k8s.io/release/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" Set the executable flag, and make the kubectl binary available in your path: chmod +x kubectl sudo mv kubectl /usr/local/bin Test to ensure you installed the latest version: kubectl version --client Installing Helm Depending on your operating system, you can install helm in the following ways: MacOS Linux Install helm using Homebrew: brew install helm Test to ensure you installed the latest version: helm version Download the latest helm release: curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null sudo apt-get install apt-transport-https --yes echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm Test to ensure you installed the latest version: helm version Installing Kustomize Depending on your operating system, you can install kustomize in the following ways: MacOS Linux Install kustomize using Homebrew: brew install kustomize Test to ensure you installed the latest version: kustomize version Install kustomize by running the following script: curl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" | bash Test to ensure you installed the latest version: kustomize version Installing Tilt Depending on your operating system, you can install Tilt in the following ways: MacOS Linux Install tilt using curl: brew install tilt Test to ensure you installed the latest version: tilt version Install tilt using curl: curl -fsSL https://raw.githubusercontent.com/tilt-dev/tilt/master/scripts/install.sh | bash Test to ensure you installed the latest version: tilt version Next, you will learn how to authenticate with the DigitalOcean API to get the most out of the tools used in this guide to provision required cloud resources.","title":"Installing required tools"},{"location":"01-getting-started/installing-required-tools/#introduction","text":"This section will show you how to install the required tools needed to complete this guide. You will focus on most popular tools used in the Kubernetes world. You will also install additional programs used for interacting with the DigitalOcean API, and local development enablement. Below is a complete list of the tools used in this guide: Kubectl - this the official Kubernetes client. Allows you to interact with the Kubernetes API, and to run commands against Kubernetes clusters. Helm - this is the package manager for Kubernetes. Behaves the same way as package managers used in Linux distributions, but for Kubernetes. Gained a lot of popularity, and it is a widely adopted solution for managing software packages installation and upgrade in Kubernetes. Doctl - allows you to interact with the DigitalOcean API via the command line. It supports most functionality found in the control panel. You can create, configure, and destroy DigitalOcean resources like Droplets, Kubernetes clusters, firewalls, load balancers, database clusters, domains, and more. Docker Desktop - enables you to build and share containerized applications and microservices using Docker. It has a GUI interface, and bundles a ready to run Kubernetes cluster to use for local development. Kustomize - Kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is. Tilt - eases local development by taking away the pain of time consuming Docker builds, watching files, and bringing environments up to date.","title":"Introduction"},{"location":"01-getting-started/installing-required-tools/#prerequisites","text":"To complete this section, you will need: Homebrew if you are using a macOS system. Curl package installed on your system.","title":"Prerequisites"},{"location":"01-getting-started/installing-required-tools/#installing-docker-desktop","text":"Depending on your operating system, you can install docker-desktop in the following ways: MacOS Linux - Ubuntu Install docker-desktop using Homebrew: brew install --cask docker Test to ensure you installed the latest version: docker version Update apt package index: sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker's official GPG key: sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg Set up the repository: echo \\ \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install docker desktop: sudo apt install docker-desktop Test to ensure you installed the latest version: docker version","title":"Installing Docker Desktop"},{"location":"01-getting-started/installing-required-tools/#installing-doctl","text":"Depending on your operating system, you can install doctl in the following ways: MacOS Linux Install doctl using Homebrew: brew install doctl Test to ensure you installed the latest version: doctl version Download the latest doctl package (check releases page): curl -LO https://github.com/digitalocean/doctl/releases/download/v1.79.0/doctl-1.79.0-linux-amd64.tar.gz Extract the doctl package: tar xf doctl-1.79.0-linux-amd64.tar.gz Set the executable flag, and make the doctl binary available in your path: chmod +x doctl sudo mv doctl /usr/local/bin Test to ensure you installed the latest version: doctl version","title":"Installing Doctl"},{"location":"01-getting-started/installing-required-tools/#installing-kubectl","text":"Depending on your operating system, you can install kubectl in the following ways: MacOS Linux Install kubectl using Homebrew: brew install kubectl Test to ensure you installed the latest version: kubectl version --client Download the latest kubectl release: curl -LO \"https://dl.k8s.io/release/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" Set the executable flag, and make the kubectl binary available in your path: chmod +x kubectl sudo mv kubectl /usr/local/bin Test to ensure you installed the latest version: kubectl version --client","title":"Installing Kubectl"},{"location":"01-getting-started/installing-required-tools/#installing-helm","text":"Depending on your operating system, you can install helm in the following ways: MacOS Linux Install helm using Homebrew: brew install helm Test to ensure you installed the latest version: helm version Download the latest helm release: curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null sudo apt-get install apt-transport-https --yes echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm Test to ensure you installed the latest version: helm version","title":"Installing Helm"},{"location":"01-getting-started/installing-required-tools/#installing-kustomize","text":"Depending on your operating system, you can install kustomize in the following ways: MacOS Linux Install kustomize using Homebrew: brew install kustomize Test to ensure you installed the latest version: kustomize version Install kustomize by running the following script: curl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" | bash Test to ensure you installed the latest version: kustomize version","title":"Installing Kustomize"},{"location":"01-getting-started/installing-required-tools/#installing-tilt","text":"Depending on your operating system, you can install Tilt in the following ways: MacOS Linux Install tilt using curl: brew install tilt Test to ensure you installed the latest version: tilt version Install tilt using curl: curl -fsSL https://raw.githubusercontent.com/tilt-dev/tilt/master/scripts/install.sh | bash Test to ensure you installed the latest version: tilt version Next, you will learn how to authenticate with the DigitalOcean API to get the most out of the tools used in this guide to provision required cloud resources.","title":"Installing Tilt"},{"location":"01-getting-started/preparing-demo-application/","text":"Introduction In this section you will create the GitHub repository hosting the sample application used through this guide. It's a web-based e-commerce app (online boutique) consisting of 11 microservices of which only 9 are used by this guide. In terms of functionality, users can browse items, add them to the cart, and purchase them. The online boutique application is a clone of the original GoogleCloudPlatform project. It will be used as a demo for the Kubernetes adoption journey. The microservices-demo project from the DigitalOcean kubernetes-sample-apps repository has been stripped down to focus only on the major parts required by the adoption journey. For more information and architecture details, please visit the GoogleCloudPlatform GitHub repository. Why not create a fork? There are several reasons why a fork is not desired for the adoption journey completion: Each PR opened on your fork will point to the upstream repository by default. You will want to have PRs opened against your repo when testing each section of the adoption journey guide, especially in the continuous integration chapter. Full history of the upstream repo will be present in your fork as well. This can create lot of noise. All projects from the kubernetes-sample-apps upstream are pulled in your fork as well. You only care about microservices-demo. Next, you will start by creating your own GitHub repository hosting the online boutique demo application source code and configuration files. Prerequisites To complete this section you will need: A GitHub account you own. You can create one for free here . A Git client to perform operations on your GitHub repository. Usually bundled with your Linux distribution. Wget and unzip utilities. Usually bundled with your Linux distribution. Set Up Online Boutique GitHub Repository Navigate to Github website and log in using your GitHub account. In the upper-right corner of any page, use the + drop-down menu, and select New repository . Set the Repository name to microservices-demo . Click on the Create repository button. From the command line clone the newly created repository to your local machine (make sure to replace the <> placeholders accordingly): git clone git@github.com:<YOUR_GITHUB_USERNAME>/microservices-demo.git Change directory to your local clone: cd microservices-demo Run the following command to download a zip file of the entire kubernetes-sample-apps repo: wget https://github.com/digitalocean/kubernetes-sample-apps/archive/refs/heads/master.zip -O kubernetes-sample-apps.zip Unzip the microservices-demo project folder from the kubernetes-sample-apps.zip file: unzip kubernetes-sample-apps.zip 'kubernetes-sample-apps-master/microservices-demo/*' Info This will result in a kubernetes-sample-apps-master folder being created from the unzip process. Copy the content of the microservices-demo from the kubernetes-sample-apps-master to the current working directory: cp -r kubernetes-sample-apps-master/microservices-demo/* . Remove the kubernetes-sample-apps-master and kubernetes-sample-apps.zip : rm -rf kubernetes-sample-apps-master kubernetes-sample-apps.zip The repository structure should look like the following: \u251c\u2500\u2500 kustomize \u2502 \u251c\u2500\u2500 base \u2502 \u251c\u2500\u2500 dev \u2502 \u251c\u2500\u2500 prod \u2502 \u251c\u2500\u2500 staging \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 release-scripts \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 license_header.txt \u2502 \u251c\u2500\u2500 make-docker-images.sh \u2502 \u251c\u2500\u2500 make-release-artifacts.sh \u2502 \u2514\u2500\u2500 make-release.sh \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 cartservice \u2502 \u251c\u2500\u2500 checkoutservice \u2502 \u251c\u2500\u2500 currencyservice \u2502 \u251c\u2500\u2500 emailservice \u2502 \u251c\u2500\u2500 frontend \u2502 \u251c\u2500\u2500 paymentservice \u2502 \u251c\u2500\u2500 productcatalogservice \u2502 \u251c\u2500\u2500 recommendationservice \u2502 \u2514\u2500\u2500 shippingservice \u251c\u2500\u2500 tilt-resources \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 tilt_config.json \u2502 \u2514\u2500\u2500 local \u2502 \u2514\u2500\u2500 tilt_config.json \u251c\u2500\u2500 CODE_OF_CONDUCT.md \u251c\u2500\u2500 CONTRIBUTING.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u2514\u2500\u2500 Tiltfile Add, commit and push all the changes to your origin: git add . git commit -m \"Initial repository layout\" git push origin At this point your online boutique GitHub repository is prepared and ready to use through this guide. Next, it is important to set main branch protection as well. Set Up Main Branch Protection You should define branch protection rules to disable force pushing, prevent branches from being deleted, and optionally require status checks before merging. From Github , navigate to the main page of your repository. Under your repository name, click Settings . In the Code and automation section of the sidebar, click Branches . Next to Branch protection rules , click Add rule. Set the Branch name pattern to master : Tick the following options: Require a pull request before merging Dismiss stale pull request approvals when new commits are pushed Next, a quick introduction is given for the main project layout and important folders/files containing main configuration. Understanding Online Boutique Application Structure The e-commerce web application used in this guide is using the following layout (some parts are stripped down for simplicity): . \u251c\u2500\u2500 kustomize \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u251c\u2500\u2500 cartservice.yaml \u2502 \u2502 \u251c\u2500\u2500 checkoutservice.yaml \u2502 \u2502 \u251c\u2500\u2500 currencyservice.yaml \u2502 \u2502 \u251c\u2500\u2500 emailservice.yaml \u2502 \u2502 \u251c\u2500\u2500 frontend.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace.yaml \u2502 \u2502 \u251c\u2500\u2500 paymentservice.yaml \u2502 \u2502 \u251c\u2500\u2500 productcatalogservice.yaml \u2502 \u2502 \u251c\u2500\u2500 recommendationservice.yaml \u2502 \u2502 \u251c\u2500\u2500 redis.yaml \u2502 \u2502 \u2514\u2500\u2500 shippingservice.yaml \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 local \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 staging \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 release-scripts \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 cartservice \u2502 \u251c\u2500\u2500 checkoutservice \u2502 \u251c\u2500\u2500 currencyservice \u2502 \u251c\u2500\u2500 emailservice \u2502 \u251c\u2500\u2500 frontend \u2502 \u251c\u2500\u2500 loadgenerator \u2502 \u251c\u2500\u2500 paymentservice \u2502 \u251c\u2500\u2500 productcatalogservice \u2502 \u251c\u2500\u2500 recommendationservice \u2502 \u2514\u2500\u2500 shippingservice \u251c\u2500\u2500 tilt-resources \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 tilt_config.json \u2502 \u2514\u2500\u2500 local \u2502 \u2514\u2500\u2500 tilt_config.json \u251c\u2500\u2500 README.md \u2514\u2500\u2500 Tiltfile Explanation for the above project structure: kustomize - main folder containing project kustomizations. Project deployment is managed via Kustomize . Each environment is represented and managed via a Kustomize overlay folder - dev , staging , prod , etc. Overlays contain environment specific configuration over the base folder. The base contains common configuration across all environments. release-scripts - contains utility shell scripts used to create, build, tag and push project docker images. src - this is the main project folder containing source code for all application microservices. It also contains required Dockerfiles to build each component image. It is a standardized layout (except for cartservice component). You will find here each project unit tests as well (not all are implemented yet though). tilt-resources - Tilt configuration profiles for each environment supported by the sample application. Tiltfile - main project Tilt logic. You will learn more about Tilt in the development section . Next, you will learn how to create a DOCR (DigitalOcean Container Registry), and push the initial version for the online boutique sample application images.","title":"Preparing the demo application"},{"location":"01-getting-started/preparing-demo-application/#introduction","text":"In this section you will create the GitHub repository hosting the sample application used through this guide. It's a web-based e-commerce app (online boutique) consisting of 11 microservices of which only 9 are used by this guide. In terms of functionality, users can browse items, add them to the cart, and purchase them. The online boutique application is a clone of the original GoogleCloudPlatform project. It will be used as a demo for the Kubernetes adoption journey. The microservices-demo project from the DigitalOcean kubernetes-sample-apps repository has been stripped down to focus only on the major parts required by the adoption journey. For more information and architecture details, please visit the GoogleCloudPlatform GitHub repository. Why not create a fork? There are several reasons why a fork is not desired for the adoption journey completion: Each PR opened on your fork will point to the upstream repository by default. You will want to have PRs opened against your repo when testing each section of the adoption journey guide, especially in the continuous integration chapter. Full history of the upstream repo will be present in your fork as well. This can create lot of noise. All projects from the kubernetes-sample-apps upstream are pulled in your fork as well. You only care about microservices-demo. Next, you will start by creating your own GitHub repository hosting the online boutique demo application source code and configuration files.","title":"Introduction"},{"location":"01-getting-started/preparing-demo-application/#prerequisites","text":"To complete this section you will need: A GitHub account you own. You can create one for free here . A Git client to perform operations on your GitHub repository. Usually bundled with your Linux distribution. Wget and unzip utilities. Usually bundled with your Linux distribution.","title":"Prerequisites"},{"location":"01-getting-started/preparing-demo-application/#set-up-online-boutique-github-repository","text":"Navigate to Github website and log in using your GitHub account. In the upper-right corner of any page, use the + drop-down menu, and select New repository . Set the Repository name to microservices-demo . Click on the Create repository button. From the command line clone the newly created repository to your local machine (make sure to replace the <> placeholders accordingly): git clone git@github.com:<YOUR_GITHUB_USERNAME>/microservices-demo.git Change directory to your local clone: cd microservices-demo Run the following command to download a zip file of the entire kubernetes-sample-apps repo: wget https://github.com/digitalocean/kubernetes-sample-apps/archive/refs/heads/master.zip -O kubernetes-sample-apps.zip Unzip the microservices-demo project folder from the kubernetes-sample-apps.zip file: unzip kubernetes-sample-apps.zip 'kubernetes-sample-apps-master/microservices-demo/*' Info This will result in a kubernetes-sample-apps-master folder being created from the unzip process. Copy the content of the microservices-demo from the kubernetes-sample-apps-master to the current working directory: cp -r kubernetes-sample-apps-master/microservices-demo/* . Remove the kubernetes-sample-apps-master and kubernetes-sample-apps.zip : rm -rf kubernetes-sample-apps-master kubernetes-sample-apps.zip The repository structure should look like the following: \u251c\u2500\u2500 kustomize \u2502 \u251c\u2500\u2500 base \u2502 \u251c\u2500\u2500 dev \u2502 \u251c\u2500\u2500 prod \u2502 \u251c\u2500\u2500 staging \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 release-scripts \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 license_header.txt \u2502 \u251c\u2500\u2500 make-docker-images.sh \u2502 \u251c\u2500\u2500 make-release-artifacts.sh \u2502 \u2514\u2500\u2500 make-release.sh \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 cartservice \u2502 \u251c\u2500\u2500 checkoutservice \u2502 \u251c\u2500\u2500 currencyservice \u2502 \u251c\u2500\u2500 emailservice \u2502 \u251c\u2500\u2500 frontend \u2502 \u251c\u2500\u2500 paymentservice \u2502 \u251c\u2500\u2500 productcatalogservice \u2502 \u251c\u2500\u2500 recommendationservice \u2502 \u2514\u2500\u2500 shippingservice \u251c\u2500\u2500 tilt-resources \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 tilt_config.json \u2502 \u2514\u2500\u2500 local \u2502 \u2514\u2500\u2500 tilt_config.json \u251c\u2500\u2500 CODE_OF_CONDUCT.md \u251c\u2500\u2500 CONTRIBUTING.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u2514\u2500\u2500 Tiltfile Add, commit and push all the changes to your origin: git add . git commit -m \"Initial repository layout\" git push origin At this point your online boutique GitHub repository is prepared and ready to use through this guide. Next, it is important to set main branch protection as well.","title":"Set Up Online Boutique GitHub Repository"},{"location":"01-getting-started/preparing-demo-application/#set-up-main-branch-protection","text":"You should define branch protection rules to disable force pushing, prevent branches from being deleted, and optionally require status checks before merging. From Github , navigate to the main page of your repository. Under your repository name, click Settings . In the Code and automation section of the sidebar, click Branches . Next to Branch protection rules , click Add rule. Set the Branch name pattern to master : Tick the following options: Require a pull request before merging Dismiss stale pull request approvals when new commits are pushed Next, a quick introduction is given for the main project layout and important folders/files containing main configuration.","title":"Set Up Main Branch Protection"},{"location":"01-getting-started/preparing-demo-application/#understanding-online-boutique-application-structure","text":"The e-commerce web application used in this guide is using the following layout (some parts are stripped down for simplicity): . \u251c\u2500\u2500 kustomize \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u251c\u2500\u2500 cartservice.yaml \u2502 \u2502 \u251c\u2500\u2500 checkoutservice.yaml \u2502 \u2502 \u251c\u2500\u2500 currencyservice.yaml \u2502 \u2502 \u251c\u2500\u2500 emailservice.yaml \u2502 \u2502 \u251c\u2500\u2500 frontend.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace.yaml \u2502 \u2502 \u251c\u2500\u2500 paymentservice.yaml \u2502 \u2502 \u251c\u2500\u2500 productcatalogservice.yaml \u2502 \u2502 \u251c\u2500\u2500 recommendationservice.yaml \u2502 \u2502 \u251c\u2500\u2500 redis.yaml \u2502 \u2502 \u2514\u2500\u2500 shippingservice.yaml \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 local \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 staging \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 release-scripts \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 cartservice \u2502 \u251c\u2500\u2500 checkoutservice \u2502 \u251c\u2500\u2500 currencyservice \u2502 \u251c\u2500\u2500 emailservice \u2502 \u251c\u2500\u2500 frontend \u2502 \u251c\u2500\u2500 loadgenerator \u2502 \u251c\u2500\u2500 paymentservice \u2502 \u251c\u2500\u2500 productcatalogservice \u2502 \u251c\u2500\u2500 recommendationservice \u2502 \u2514\u2500\u2500 shippingservice \u251c\u2500\u2500 tilt-resources \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 tilt_config.json \u2502 \u2514\u2500\u2500 local \u2502 \u2514\u2500\u2500 tilt_config.json \u251c\u2500\u2500 README.md \u2514\u2500\u2500 Tiltfile Explanation for the above project structure: kustomize - main folder containing project kustomizations. Project deployment is managed via Kustomize . Each environment is represented and managed via a Kustomize overlay folder - dev , staging , prod , etc. Overlays contain environment specific configuration over the base folder. The base contains common configuration across all environments. release-scripts - contains utility shell scripts used to create, build, tag and push project docker images. src - this is the main project folder containing source code for all application microservices. It also contains required Dockerfiles to build each component image. It is a standardized layout (except for cartservice component). You will find here each project unit tests as well (not all are implemented yet though). tilt-resources - Tilt configuration profiles for each environment supported by the sample application. Tiltfile - main project Tilt logic. You will learn more about Tilt in the development section . Next, you will learn how to create a DOCR (DigitalOcean Container Registry), and push the initial version for the online boutique sample application images.","title":"Understanding Online Boutique Application Structure"},{"location":"01-getting-started/setup-docr/","text":"Introduction In this section you will learn you how to create a DigitalOcean Container Registry ( DOCR ) registry using doctl . A docker container registry is required to store all microservices-demo app images used in this guide. For each microservice you need a separate repository in the registry. The microservices-demo app consists of nine microservice , hence a total of nine Docker repositories will be created in the registry. Prerequisites To complete this section you will need: Doctl utility already installed, as explained in the Installing Required Tools -> Doctl section. Make sure that you're authenticated with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API section. Provisioning a DigitalOcean Container Registry for Microservices Development Following command will create a professional tier Docker registry in the nyc3 region, named microservices-demo : doctl registry create microservices-demo \\ --subscription-tier professional \\ --region nyc3 Notes The professional tier is required to store all docker images used in this guide which costs 20$/month . You can have only one registry endpoint per account in DOCR . A repository in a registry refers to a collection of container images using tags. It is recommended to use a region for your registry that is closest to you for faster image download/upload operations. Run the following command - doctl registry options available-regions to check available regions. Building and Pushing the Online Boutique Application Images to DOCR In this section, you will push to DOCR the first version ( v1.0.0 ) of the Online Boutique demo application. This step is required to perform the initial testing of the application in the upcoming sections of this guide. Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git From the command line, change directory to the microservices-demo folder (if not there already): cd microservices-demo Login to DOCR: doctl registry login Run the make-docker-images.sh script after setting required environment variables first: export REPO_PREFIX = \"registry.digitalocean.com/microservices-demo\" export TAG = \"v1.0.0\" ./release-scripts/make-docker-images.sh Info You will be pushing an initial release first to DOCR - v1.0.0 , and use that to deploy to the staging and production environments in the upcoming sections. Later on, GitHub Actions will take care of building, tagging and pushing images to DOCR . Next, you will setup the development environment and deploy the microservices-demo application to the associated DOKS cluster, as well as setting up ingress and monitoring .","title":"Set up a Digital Ocean container registry"},{"location":"01-getting-started/setup-docr/#introduction","text":"In this section you will learn you how to create a DigitalOcean Container Registry ( DOCR ) registry using doctl . A docker container registry is required to store all microservices-demo app images used in this guide. For each microservice you need a separate repository in the registry. The microservices-demo app consists of nine microservice , hence a total of nine Docker repositories will be created in the registry.","title":"Introduction"},{"location":"01-getting-started/setup-docr/#prerequisites","text":"To complete this section you will need: Doctl utility already installed, as explained in the Installing Required Tools -> Doctl section. Make sure that you're authenticated with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API section.","title":"Prerequisites"},{"location":"01-getting-started/setup-docr/#provisioning-a-digitalocean-container-registry-for-microservices-development","text":"Following command will create a professional tier Docker registry in the nyc3 region, named microservices-demo : doctl registry create microservices-demo \\ --subscription-tier professional \\ --region nyc3 Notes The professional tier is required to store all docker images used in this guide which costs 20$/month . You can have only one registry endpoint per account in DOCR . A repository in a registry refers to a collection of container images using tags. It is recommended to use a region for your registry that is closest to you for faster image download/upload operations. Run the following command - doctl registry options available-regions to check available regions.","title":"Provisioning a DigitalOcean Container Registry for Microservices Development"},{"location":"01-getting-started/setup-docr/#building-and-pushing-the-online-boutique-application-images-to-docr","text":"In this section, you will push to DOCR the first version ( v1.0.0 ) of the Online Boutique demo application. This step is required to perform the initial testing of the application in the upcoming sections of this guide. Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git From the command line, change directory to the microservices-demo folder (if not there already): cd microservices-demo Login to DOCR: doctl registry login Run the make-docker-images.sh script after setting required environment variables first: export REPO_PREFIX = \"registry.digitalocean.com/microservices-demo\" export TAG = \"v1.0.0\" ./release-scripts/make-docker-images.sh Info You will be pushing an initial release first to DOCR - v1.0.0 , and use that to deploy to the staging and production environments in the upcoming sections. Later on, GitHub Actions will take care of building, tagging and pushing images to DOCR . Next, you will setup the development environment and deploy the microservices-demo application to the associated DOKS cluster, as well as setting up ingress and monitoring .","title":"Building and Pushing the Online Boutique Application Images to DOCR"},{"location":"02-development/observability-dev/","text":"Introduction Note You will not install a full blown monitoring and logging solution on the development cluster such as Prometheus and Loki , but will do so for the staging and production clusters. On the development environment you will be configuring the Kubernetes Metrics Server and Kubernetes Dashboard tools. This way your cluster size can be small but sufficient that you can do your local development and deploy that to a Kubernetes cluster and test it. At the most basic level you will need to be able to troubleshoot your aplication if things go wrong. In this section, you will learn about the Kubernetes Metrics Server and the Kubernetes Dashboard . Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines. It collects resource metrics from Kubelets and exposes them in Kubernetes apiserver through Metrics API . Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself. Prerequisites To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. JQ command-line JSON processor installed on your machine. The online boutique sample application deployed to your cluster as explained in the Tilt remote development section. Installing the Kubernetes Metrics Server In this section you will install the community maintained Kubernetes Metrics Server. Please follow below steps to install it using Helm: Add the Metrics Server Helm repository: helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ Install the Kubernetes Metrics Server using Helm : helm upgrade --install metrics-server metrics-server/metrics-server \\ --namespace metrics-server \\ --create-namespace Note To check if the installation was successful, run the helm ls -n metrics-server command, and confirm the deployment status. Collect resource metrics from Kubernetes objects Resource metrics track the utilization and availability of critical resources such as CPU, memory, and storage. Kubernetes provides a Metrics API and a number of command line queries that allow you to retrieve snapshots of resource utilization. Query the Metrics API to retrieve current metrics from any node or pod (you can find your desired node or pod by running kubectl get nodes or kubectl get pods ): kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/<NODE_NAME> | jq kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/<NAMESPACE>/pods/<POD_NAME> | jq Info The Metrics API returns a JSON object, so (optionally) piping the response through jq displays the JSON in a more human-readable format. Retrieve compact metric snapshots from the Metrics API using kubectl top. The kubectl top command returns current CPU and memory usage for a cluster\u2019s pods or nodes, or for a particular pod or node if specified. kubectl top node The output should look like the following: NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% basicnp-766qo 280m 7% 1662Mi 24% basicnp-766qr 252m 6% 1561Mi 23% Or you can query resource utilization by pod in a particular namespace (you can use the microservices-demo-dev namespace you deployed in the Tilt Remote section): kubectl top pod --namespace microservices-demo-dev The output should look like the following: NAME CPU(cores) MEMORY(bytes) cartservice-84758f76f-cl9vm 8m 29Mi checkoutservice-5d8d8cfd5f-8hbjn 3m 16Mi currencyservice-5d5f698f87-kh4z8 4m 30Mi emailservice-f8795cc94-r7hwp 30m 40Mi frontend-6d45d8cc5d-59fmw 1m 20Mi paymentservice-995d69494-kcqn2 5m 30Mi productcatalogservice-556d4f9446-7sqp9 7m 16Mi recommendationservice-59f78c445b-5487v 40m 40Mi redis-cart-596c7658c4-lwf8g 3m 7Mi shippingservice-bfc488696-dkcpz 3m 15Mi See details about the resources that have been allocated to your nodes, rather than the current resource usage. The kubectl describe command provides a detailed breakdown of a specified pod or node. kubectl describe node <NODE_NAME> The output is quite verbose containing a full breakdown of the node\u2019s workloads, system info, and metadata such as labels and annotations. Click to expand node describe command output Name: basicnp-766qo Roles: <none> Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=s-4vcpu-8gb-amd beta.kubernetes.io/os=linux doks.digitalocean.com/node-id=018db650-0dd3-4e23-a387-d386a193456e doks.digitalocean.com/node-pool=basicnp doks.digitalocean.com/node-pool-id=45a15812-c08d-48f0-ae7d-61eb0ddc3e7c doks.digitalocean.com/version=1.24.4-do.0 failure-domain.beta.kubernetes.io/region=nyc1 kubernetes.io/arch=amd64 kubernetes.io/hostname=basicnp-766qo kubernetes.io/os=linux node.kubernetes.io/instance-type=s-4vcpu-8gb-amd region=nyc1 topology.kubernetes.io/region=nyc1 type=basic Annotations: alpha.kubernetes.io/provided-node-ip: 10.116.0.4 csi.volume.kubernetes.io/nodeid: {\"dobs.csi.digitalocean.com\":\"318312990\"} io.cilium.network.ipv4-cilium-host: 10.244.1.102 io.cilium.network.ipv4-health-ip: 10.244.1.76 io.cilium.network.ipv4-pod-cidr: 10.244.1.0/25 node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Mon, 26 Sep 2022 11:48:39 +0300 Taints: <none> Unschedulable: false Lease: HolderIdentity: basicnp-766qo AcquireTime: <unset> RenewTime: Wed, 28 Sep 2022 11:31:13 +0300 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- NetworkUnavailable False Mon, 26 Sep 2022 11:50:10 +0300 Mon, 26 Sep 2022 11:50:10 +0300 CiliumIsUp Cilium is running on this node MemoryPressure False Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:48:39 +0300 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:48:39 +0300 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:48:39 +0300 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:49:10 +0300 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 10.116.0.4 Hostname: basicnp-766qo ExternalIP: 159.223.163.15 Capacity: cpu: 4 ephemeral-storage: 165089200Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 8149728Ki pods: 110 Allocatable: cpu: 3900m ephemeral-storage: 152146206469 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 6694Mi pods: 110 System Info: Machine ID: a6b0d5360452428382d5b0c516caa546 System UUID: a6b0d536-0452-4283-82d5-b0c516caa546 Boot ID: 3345884c-f9af-4716-af86-17814d28ef96 Kernel Version: 5.10.0-0.bpo.15-amd64 OS Image: Debian GNU/Linux 10 (buster) Operating System: linux Architecture: amd64 Container Runtime Version: containerd://1.4.13 Kubelet Version: v1.24.4 Kube-Proxy Version: v1.24.4 ProviderID: digitalocean://318312990 Non-terminated Pods: (15 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- cert-manager cert-manager-ddd4d6ddf-zmpk4 0 (0%) 0 (0%) 0 (0%) 0 (0%) 47h ingress-nginx ingress-nginx-controller-778667bc4b-7lj4c 100m (2%) 0 (0%) 90Mi (1%) 0 (0%) 47h kube-system cilium-bv8x9 310m (7%) 100m (2%) 310Mi (4%) 75Mi (1%) 47h kube-system cilium-operator-6d485f4f69-fsv7x 100m (2%) 0 (0%) 150M (2%) 150M (2%) 47h kube-system coredns-9c8d9dc8c-9mzcd 100m (2%) 0 (0%) 150M (2%) 150M (2%) 47h kube-system cpc-bridge-proxy-dktvq 100m (2%) 0 (0%) 75Mi (1%) 0 (0%) 47h kube-system csi-do-node-trfdm 0 (0%) 0 (0%) 0 (0%) 0 (0%) 47h kube-system do-node-agent-vt9kn 102m (2%) 102m (2%) 80Mi (1%) 300Mi (4%) 47h kube-system konnectivity-agent-fnvp9 0 (0%) 0 (0%) 0 (0%) 0 (0%) 47h kube-system kube-proxy-9ff7c 0 (0%) 0 (0%) 125Mi (1%) 0 (0%) 47h microservices-demo-dev cartservice-84758f76f-cl9vm 200m (5%) 300m (7%) 128Mi (1%) 256Mi (3%) 45h microservices-demo-dev frontend-6d45d8cc5d-59fmw 100m (2%) 200m (5%) 64Mi (0%) 128Mi (1%) 45h microservices-demo-dev productcatalogservice-556d4f9446-7sqp9 100m (2%) 200m (5%) 64Mi (0%) 128Mi (1%) 45h microservices-demo-dev recommendationservice-59f78c445b-5487v 100m (2%) 200m (5%) 220Mi (3%) 450Mi (6%) 45h microservices-demo-dev redis-cart-596c7658c4-lwf8g 70m (1%) 125m (3%) 200Mi (2%) 256Mi (3%) 45h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1382m (35%) 1227m (31%) memory 1721869056 (24%) 1970381568 (28%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) Installing the Kubernetes Dashboard Note The Kubernetes Dashboard is already available as a managed solution for DigitalOcean customers after creating a Kubernetes Cluster. You are installing it separately due to the lack of CPU and memory usage metrics not displayed in the managed solution and those metrics are very important to monitor. Please see this support case for more details. In this section you will install the community maintained Kubernetes Dashboard . Please follow below steps to install it using kubectl: Install the Kubernetes Dashboard using kubectl : kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml Note To check if the installation was successful, run the kubectl get pods -n kubernetes-dashboard command, and confirm that the pods are running. In a new terminal window start the kubectl proxy : kubectl proxy Launch a web browser and open the Kubernetes Dashboard Login page. Then, choose the Kubeconfig option, and provide your cluster's config file to log in. Note To get the config file navigate to your DigitalOcean cloud console, go to Kubernetes , select your cluster and from the Configuration section, click on Download Config File . After successfully logging in, you should be presented with the main dashboard landing page: Next, you can check metric summaries for each pod, node, and namespace in your cluster. Editing Kubernetes objects is also possible, such as scaling up/down deployments, change image version for pods, etc. Going further it is also possible to inspect log streams for pods. From the navigation bar at the top of the Pods view, click on the Logs tab to access a pod log stream directly in your web browser. In case of pods comprised of multiple containers, you have the option to inspect each container logs. Finally, you can Exec into a pod container from the same page. Kuberentes events are also viewable from the Kubernetes Dashboard . From the left menu click on the Events view. Events will be displayed and stored for 1 hour. Next, you will learn how to provision and configure the staging environment for the online boutique sample application. Besides DOKS setup and the sample app deployment, you will also configure a full observability stack comprised of logging, monitoring and alerting via Slack. Usually, a staging environment should be pretty close (if not similar) to a production environment.","title":"Observability"},{"location":"02-development/observability-dev/#introduction","text":"Note You will not install a full blown monitoring and logging solution on the development cluster such as Prometheus and Loki , but will do so for the staging and production clusters. On the development environment you will be configuring the Kubernetes Metrics Server and Kubernetes Dashboard tools. This way your cluster size can be small but sufficient that you can do your local development and deploy that to a Kubernetes cluster and test it. At the most basic level you will need to be able to troubleshoot your aplication if things go wrong. In this section, you will learn about the Kubernetes Metrics Server and the Kubernetes Dashboard . Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines. It collects resource metrics from Kubelets and exposes them in Kubernetes apiserver through Metrics API . Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.","title":"Introduction"},{"location":"02-development/observability-dev/#prerequisites","text":"To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. JQ command-line JSON processor installed on your machine. The online boutique sample application deployed to your cluster as explained in the Tilt remote development section.","title":"Prerequisites"},{"location":"02-development/observability-dev/#installing-the-kubernetes-metrics-server","text":"In this section you will install the community maintained Kubernetes Metrics Server. Please follow below steps to install it using Helm: Add the Metrics Server Helm repository: helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ Install the Kubernetes Metrics Server using Helm : helm upgrade --install metrics-server metrics-server/metrics-server \\ --namespace metrics-server \\ --create-namespace Note To check if the installation was successful, run the helm ls -n metrics-server command, and confirm the deployment status.","title":"Installing the Kubernetes Metrics Server"},{"location":"02-development/observability-dev/#collect-resource-metrics-from-kubernetes-objects","text":"Resource metrics track the utilization and availability of critical resources such as CPU, memory, and storage. Kubernetes provides a Metrics API and a number of command line queries that allow you to retrieve snapshots of resource utilization. Query the Metrics API to retrieve current metrics from any node or pod (you can find your desired node or pod by running kubectl get nodes or kubectl get pods ): kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/<NODE_NAME> | jq kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/<NAMESPACE>/pods/<POD_NAME> | jq Info The Metrics API returns a JSON object, so (optionally) piping the response through jq displays the JSON in a more human-readable format. Retrieve compact metric snapshots from the Metrics API using kubectl top. The kubectl top command returns current CPU and memory usage for a cluster\u2019s pods or nodes, or for a particular pod or node if specified. kubectl top node The output should look like the following: NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% basicnp-766qo 280m 7% 1662Mi 24% basicnp-766qr 252m 6% 1561Mi 23% Or you can query resource utilization by pod in a particular namespace (you can use the microservices-demo-dev namespace you deployed in the Tilt Remote section): kubectl top pod --namespace microservices-demo-dev The output should look like the following: NAME CPU(cores) MEMORY(bytes) cartservice-84758f76f-cl9vm 8m 29Mi checkoutservice-5d8d8cfd5f-8hbjn 3m 16Mi currencyservice-5d5f698f87-kh4z8 4m 30Mi emailservice-f8795cc94-r7hwp 30m 40Mi frontend-6d45d8cc5d-59fmw 1m 20Mi paymentservice-995d69494-kcqn2 5m 30Mi productcatalogservice-556d4f9446-7sqp9 7m 16Mi recommendationservice-59f78c445b-5487v 40m 40Mi redis-cart-596c7658c4-lwf8g 3m 7Mi shippingservice-bfc488696-dkcpz 3m 15Mi See details about the resources that have been allocated to your nodes, rather than the current resource usage. The kubectl describe command provides a detailed breakdown of a specified pod or node. kubectl describe node <NODE_NAME> The output is quite verbose containing a full breakdown of the node\u2019s workloads, system info, and metadata such as labels and annotations. Click to expand node describe command output Name: basicnp-766qo Roles: <none> Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=s-4vcpu-8gb-amd beta.kubernetes.io/os=linux doks.digitalocean.com/node-id=018db650-0dd3-4e23-a387-d386a193456e doks.digitalocean.com/node-pool=basicnp doks.digitalocean.com/node-pool-id=45a15812-c08d-48f0-ae7d-61eb0ddc3e7c doks.digitalocean.com/version=1.24.4-do.0 failure-domain.beta.kubernetes.io/region=nyc1 kubernetes.io/arch=amd64 kubernetes.io/hostname=basicnp-766qo kubernetes.io/os=linux node.kubernetes.io/instance-type=s-4vcpu-8gb-amd region=nyc1 topology.kubernetes.io/region=nyc1 type=basic Annotations: alpha.kubernetes.io/provided-node-ip: 10.116.0.4 csi.volume.kubernetes.io/nodeid: {\"dobs.csi.digitalocean.com\":\"318312990\"} io.cilium.network.ipv4-cilium-host: 10.244.1.102 io.cilium.network.ipv4-health-ip: 10.244.1.76 io.cilium.network.ipv4-pod-cidr: 10.244.1.0/25 node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Mon, 26 Sep 2022 11:48:39 +0300 Taints: <none> Unschedulable: false Lease: HolderIdentity: basicnp-766qo AcquireTime: <unset> RenewTime: Wed, 28 Sep 2022 11:31:13 +0300 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- NetworkUnavailable False Mon, 26 Sep 2022 11:50:10 +0300 Mon, 26 Sep 2022 11:50:10 +0300 CiliumIsUp Cilium is running on this node MemoryPressure False Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:48:39 +0300 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:48:39 +0300 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:48:39 +0300 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:49:10 +0300 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 10.116.0.4 Hostname: basicnp-766qo ExternalIP: 159.223.163.15 Capacity: cpu: 4 ephemeral-storage: 165089200Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 8149728Ki pods: 110 Allocatable: cpu: 3900m ephemeral-storage: 152146206469 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 6694Mi pods: 110 System Info: Machine ID: a6b0d5360452428382d5b0c516caa546 System UUID: a6b0d536-0452-4283-82d5-b0c516caa546 Boot ID: 3345884c-f9af-4716-af86-17814d28ef96 Kernel Version: 5.10.0-0.bpo.15-amd64 OS Image: Debian GNU/Linux 10 (buster) Operating System: linux Architecture: amd64 Container Runtime Version: containerd://1.4.13 Kubelet Version: v1.24.4 Kube-Proxy Version: v1.24.4 ProviderID: digitalocean://318312990 Non-terminated Pods: (15 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- cert-manager cert-manager-ddd4d6ddf-zmpk4 0 (0%) 0 (0%) 0 (0%) 0 (0%) 47h ingress-nginx ingress-nginx-controller-778667bc4b-7lj4c 100m (2%) 0 (0%) 90Mi (1%) 0 (0%) 47h kube-system cilium-bv8x9 310m (7%) 100m (2%) 310Mi (4%) 75Mi (1%) 47h kube-system cilium-operator-6d485f4f69-fsv7x 100m (2%) 0 (0%) 150M (2%) 150M (2%) 47h kube-system coredns-9c8d9dc8c-9mzcd 100m (2%) 0 (0%) 150M (2%) 150M (2%) 47h kube-system cpc-bridge-proxy-dktvq 100m (2%) 0 (0%) 75Mi (1%) 0 (0%) 47h kube-system csi-do-node-trfdm 0 (0%) 0 (0%) 0 (0%) 0 (0%) 47h kube-system do-node-agent-vt9kn 102m (2%) 102m (2%) 80Mi (1%) 300Mi (4%) 47h kube-system konnectivity-agent-fnvp9 0 (0%) 0 (0%) 0 (0%) 0 (0%) 47h kube-system kube-proxy-9ff7c 0 (0%) 0 (0%) 125Mi (1%) 0 (0%) 47h microservices-demo-dev cartservice-84758f76f-cl9vm 200m (5%) 300m (7%) 128Mi (1%) 256Mi (3%) 45h microservices-demo-dev frontend-6d45d8cc5d-59fmw 100m (2%) 200m (5%) 64Mi (0%) 128Mi (1%) 45h microservices-demo-dev productcatalogservice-556d4f9446-7sqp9 100m (2%) 200m (5%) 64Mi (0%) 128Mi (1%) 45h microservices-demo-dev recommendationservice-59f78c445b-5487v 100m (2%) 200m (5%) 220Mi (3%) 450Mi (6%) 45h microservices-demo-dev redis-cart-596c7658c4-lwf8g 70m (1%) 125m (3%) 200Mi (2%) 256Mi (3%) 45h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1382m (35%) 1227m (31%) memory 1721869056 (24%) 1970381568 (28%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%)","title":"Collect resource metrics from Kubernetes objects"},{"location":"02-development/observability-dev/#installing-the-kubernetes-dashboard","text":"Note The Kubernetes Dashboard is already available as a managed solution for DigitalOcean customers after creating a Kubernetes Cluster. You are installing it separately due to the lack of CPU and memory usage metrics not displayed in the managed solution and those metrics are very important to monitor. Please see this support case for more details. In this section you will install the community maintained Kubernetes Dashboard . Please follow below steps to install it using kubectl: Install the Kubernetes Dashboard using kubectl : kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml Note To check if the installation was successful, run the kubectl get pods -n kubernetes-dashboard command, and confirm that the pods are running. In a new terminal window start the kubectl proxy : kubectl proxy Launch a web browser and open the Kubernetes Dashboard Login page. Then, choose the Kubeconfig option, and provide your cluster's config file to log in. Note To get the config file navigate to your DigitalOcean cloud console, go to Kubernetes , select your cluster and from the Configuration section, click on Download Config File . After successfully logging in, you should be presented with the main dashboard landing page: Next, you can check metric summaries for each pod, node, and namespace in your cluster. Editing Kubernetes objects is also possible, such as scaling up/down deployments, change image version for pods, etc. Going further it is also possible to inspect log streams for pods. From the navigation bar at the top of the Pods view, click on the Logs tab to access a pod log stream directly in your web browser. In case of pods comprised of multiple containers, you have the option to inspect each container logs. Finally, you can Exec into a pod container from the same page. Kuberentes events are also viewable from the Kubernetes Dashboard . From the left menu click on the Events view. Events will be displayed and stored for 1 hour. Next, you will learn how to provision and configure the staging environment for the online boutique sample application. Besides DOKS setup and the sample app deployment, you will also configure a full observability stack comprised of logging, monitoring and alerting via Slack. Usually, a staging environment should be pretty close (if not similar) to a production environment.","title":"Installing the Kubernetes Dashboard"},{"location":"02-development/setup-doks-dev/","text":"Introduction This section will show you how to create a DigitalOcean Kubernetes Cluster ( DOKS ) cluster that can be used for remote development, targeting the online boutique sample application used as a reference in this guide. Prerequisites To complete this section you will need: A container registry already set up as explained in the Set up DOCR section. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Provisioning a Development DOKS Cluster for Microservices In this step, you will create a new Kubernetes cluster running on the DigitalOcean platform, using the doctl utility. Following command will create a DigitalOcean Kubernetes cluster named microservices-demo-dev , with a pool size of 3 nodes , each having 2 vCPUs and 4GB of RAM, in the nyc1 region: doctl k8s cluster create microservices-demo-dev \\ --auto-upgrade = true \\ --maintenance-window \"saturday=21:00\" \\ --node-pool \"name=basicnp;size=s-2vcpu-4gb;count=3;tag=adoption-journey;label=type=basic\" \\ --region nyc1 Notes The example cluster created above is using 3 nodes, each having 2vCPU/4GB size, which amounts to 72$/month . For simplicity and consistency through all the guide, the microservices-demo-dev name was picked for the example cluster. You can choose any name you like, but you need to make sure the naming convention stays consistent. It is recommended to use a region for your cluster that is closest to you for faster interaction. Run the following command - doctl k8s options regions to check available regions. Cluster auto upgrade is enabled ( --auto-upgrade=true ). Kubernetes clusters should be auto-upgraded to ensure that they always contain the latest security patches. Next, you can verify the cluster details. First, fetch your DOKS cluster ID : doctl k8s cluster list Finally, check if the kubectl context was set to point to your DOKS cluster. The doctl utility should do this automatically: kubectl config current-context For more info on this topic please see this Kubernetes Starter Kit DOKS Creation . Configuring DOKS for Private Registries From the command line run the following: doctl registry kubernetes-manifest | kubectl apply -f - This will configure your DOKS cluster to fetch images from your DOCR created in the Set up a DigitalOcean container registry section This step can also be achieved via the DigitalOcean cloud console. Please follow this guide . Next, you will learn how to perform local microservices development using Tilt .","title":"Set up Development DOKS"},{"location":"02-development/setup-doks-dev/#introduction","text":"This section will show you how to create a DigitalOcean Kubernetes Cluster ( DOKS ) cluster that can be used for remote development, targeting the online boutique sample application used as a reference in this guide.","title":"Introduction"},{"location":"02-development/setup-doks-dev/#prerequisites","text":"To complete this section you will need: A container registry already set up as explained in the Set up DOCR section. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section.","title":"Prerequisites"},{"location":"02-development/setup-doks-dev/#provisioning-a-development-doks-cluster-for-microservices","text":"In this step, you will create a new Kubernetes cluster running on the DigitalOcean platform, using the doctl utility. Following command will create a DigitalOcean Kubernetes cluster named microservices-demo-dev , with a pool size of 3 nodes , each having 2 vCPUs and 4GB of RAM, in the nyc1 region: doctl k8s cluster create microservices-demo-dev \\ --auto-upgrade = true \\ --maintenance-window \"saturday=21:00\" \\ --node-pool \"name=basicnp;size=s-2vcpu-4gb;count=3;tag=adoption-journey;label=type=basic\" \\ --region nyc1 Notes The example cluster created above is using 3 nodes, each having 2vCPU/4GB size, which amounts to 72$/month . For simplicity and consistency through all the guide, the microservices-demo-dev name was picked for the example cluster. You can choose any name you like, but you need to make sure the naming convention stays consistent. It is recommended to use a region for your cluster that is closest to you for faster interaction. Run the following command - doctl k8s options regions to check available regions. Cluster auto upgrade is enabled ( --auto-upgrade=true ). Kubernetes clusters should be auto-upgraded to ensure that they always contain the latest security patches. Next, you can verify the cluster details. First, fetch your DOKS cluster ID : doctl k8s cluster list Finally, check if the kubectl context was set to point to your DOKS cluster. The doctl utility should do this automatically: kubectl config current-context For more info on this topic please see this Kubernetes Starter Kit DOKS Creation .","title":"Provisioning a Development DOKS Cluster for Microservices"},{"location":"02-development/setup-doks-dev/#configuring-doks-for-private-registries","text":"From the command line run the following: doctl registry kubernetes-manifest | kubectl apply -f - This will configure your DOKS cluster to fetch images from your DOCR created in the Set up a DigitalOcean container registry section This step can also be achieved via the DigitalOcean cloud console. Please follow this guide . Next, you will learn how to perform local microservices development using Tilt .","title":"Configuring DOKS for Private Registries"},{"location":"02-development/setup-ingress-dev/","text":"Introduction In this section, you will learn how to install and configure the Kubernetes-maintained version of the Nginx Ingress Controller. Then, you're going to discover how to automatically issue TLS certificates for your hosts (thus enabling TLS termination), and route traffic to your backend applications. Prerequisites To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Tilt remote development section. A valid domain available and configured to point to DigitalOcean name servers. More information is available in this article . Digital Ocean is not a domain registrar, so you will need to purchase the domain from a well known vendor, such as GoDaddy. Installing the Nginx Ingress Controller In this section you will install the community maintained version of the Nginx ingress controller. Please follow below steps to install Nginx using Helm: Add the Ingress Nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update ingress-nginx helm search repo ingress-nginx Install the Nginx Ingress Controller using Helm : helm install ingress-nginx ingress-nginx/ingress-nginx --version 4 .1.3 \\ --namespace ingress-nginx \\ --create-namespace Note To check if the installation was successful, run the helm ls -n ingress-nginx command, and confirm the deployment status. Configure DNS for Nginx Ingress Controller: doctl compute domain create <YOUR_DOMAIN_NAME> Info Please note that this domain matches the domain you purchased in the Prerequisites section. You will use this domain to create additional sub-domains to use with the microservices app you will deploy in this section. Create an A record for your host: LOAD_BALANCER_IP = $( doctl compute load-balancer list --format IP --no-header ) doctl compute domain records create <YOUR_DOMAIN_NAME> \\ --record-type \"A\" --record-name <YOUR_RECORD_NAME> \\ --record-data \" $LOAD_BALANCER_IP \" \\ --record-ttl \"30\" Info The upper mentioned command works if you have only one LB in your DO account. If you have multiple LBs you will need to add the its IP in the command. Add the Jetstack Helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install the jetstack/cert-manager chart using Helm : helm install cert-manager jetstack/cert-manager --version 1 .8.0 \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true Note To check if the installation was succesfull you can run the helm ls -n cert-manager and confirm the deployment status. Create a Kubernetes Secret for the DigitalOcean Provider that cert-manager is going to use to perform the DNS-01 challenge using a DigitalOcean API token: DO_API_TOKEN = \"<YOUR_DO_API_TOKEN_HERE>\" kubectl create secret generic \"digitalocean-dns\" \\ --namespace microservices-demo-dev \\ --from-literal = access-token = \" $DO_API_TOKEN \" Note The secret must be created in the same namespace where the Issuer CRD is located - in this case the microservides-demo-dev namespace. Create an issuer resource for cert-manager using kubectl (make sure to replace the <> placeholders first): The issuer manifest file looks like the following: Click to expand issuer manifest file apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-nginx-wcard namespace : microservices-demo-dev spec : # ACME issuer configuration: # `email` - the email address to be associated with the ACME account (make sure it's a valid one). # `server` - the URL used to access the ACME server\u2019s directory endpoint. # `privateKeySecretRef` - Kubernetes Secret to store the automatically generated ACME account private key. acme : email : <YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-nginx-wcard-private # List of challenge solvers that will be used to solve ACME challenges for the matching domains. solvers : # Use the DigitalOcean DNS API to manage DNS01 challenge records. - dns01 : digitalocean : # Kubernetes secret that contains the DO API token . # Must be in the same namespace as the Issuer CRD. tokenSecretRef : name : digitalocean-dns key : access-token Apply via kubectl: kubectl apply -f docs/02-development/assets/manifests/cert-manager-wildcard-issuer.yaml Info Running kubectl get issuer letsencrypt-nginx-wcard -n microservices-demo-dev should result in the True value being displayed under the READY column. Note If the Issuer object displays a Not Ready state you can describe the object to get additional information using: kubectl describe issuer letsencrypt-nginx-wcard -n microservices-demo-dev to get more information. Create the wildcard certificates resource using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The certificate manifest file looks like the following: Click to expland the certificate resource apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : <YOUR_DOMAIN_NAME> # Cert-Manager will put the resulting Secret in the same Kubernetes namespace as the Certificate. namespace : microservices-demo-dev spec : # Secret name to create, where the private key and certificate should be stored. secretName : <YOUR_DOMAIN_NAME> # What Issuer to use for getting the certificate. issuerRef : name : letsencrypt-nginx-wcard kind : Issuer group : cert-manager.io # Common name to be used on the Certificate. commonName : \"*.<YOUR_DOMAIN_NAME>\" # List of DNS subjectAltNames to be set on the Certificate. dnsNames : - \"<YOUR_DOMAIN_NAME>\" - \"*.<YOUR_DOMAIN_NAME>\" Apply via kubectl: kubectl apply -f docs/02-development/assets/manifests/cert-manager-wildcard-certificate.yaml To verify the certificate status run: kubectl get certificate <YOUR_DOMAIN_NAME> -n microservices-demo-dev Info This may take a few minutes to complete. If the Certificate object displays a not ready state you can run: kubectl logs -l app=cert-manager,app.kubernetes.io/component=controller -n cert-manager Add the Ingress Nginx host using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The ingress host manifest file looks like the following: Click to expand the ingress host resource apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-microservices-demo-dev namespace : microservices-demo-dev spec : tls : - hosts : - \"*.<YOUR_DOMAIN_NAME>\" secretName : <YOUR_DOMAIN_NAME> rules : - host : <YOUR_A_RECORD>.<YOUR_DOMAIN_NAME> http : paths : - path : / pathType : Prefix backend : service : name : frontend port : number : 80 ingressClassName : nginx Apply via kubectl: kubectl apply -f docs/02-development/assets/manifests/ingress-host.yaml Open a web browser and point to <YOUR_A_RECORD>.<YOUR_DOMAIN> . You should see the online boutique welcome page. The connection is secure and the certificate is a valid one issued by Let's Encrypt . Next, you will deploy the Kubernetes Dashboard and Kubernetes Metrics Server to your cluster in order to visualize application and cluster related metrics, as well as corresponding logs and events.","title":"Set up ingress"},{"location":"02-development/setup-ingress-dev/#introduction","text":"In this section, you will learn how to install and configure the Kubernetes-maintained version of the Nginx Ingress Controller. Then, you're going to discover how to automatically issue TLS certificates for your hosts (thus enabling TLS termination), and route traffic to your backend applications.","title":"Introduction"},{"location":"02-development/setup-ingress-dev/#prerequisites","text":"To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Tilt remote development section. A valid domain available and configured to point to DigitalOcean name servers. More information is available in this article . Digital Ocean is not a domain registrar, so you will need to purchase the domain from a well known vendor, such as GoDaddy.","title":"Prerequisites"},{"location":"02-development/setup-ingress-dev/#installing-the-nginx-ingress-controller","text":"In this section you will install the community maintained version of the Nginx ingress controller. Please follow below steps to install Nginx using Helm: Add the Ingress Nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update ingress-nginx helm search repo ingress-nginx Install the Nginx Ingress Controller using Helm : helm install ingress-nginx ingress-nginx/ingress-nginx --version 4 .1.3 \\ --namespace ingress-nginx \\ --create-namespace Note To check if the installation was successful, run the helm ls -n ingress-nginx command, and confirm the deployment status. Configure DNS for Nginx Ingress Controller: doctl compute domain create <YOUR_DOMAIN_NAME> Info Please note that this domain matches the domain you purchased in the Prerequisites section. You will use this domain to create additional sub-domains to use with the microservices app you will deploy in this section. Create an A record for your host: LOAD_BALANCER_IP = $( doctl compute load-balancer list --format IP --no-header ) doctl compute domain records create <YOUR_DOMAIN_NAME> \\ --record-type \"A\" --record-name <YOUR_RECORD_NAME> \\ --record-data \" $LOAD_BALANCER_IP \" \\ --record-ttl \"30\" Info The upper mentioned command works if you have only one LB in your DO account. If you have multiple LBs you will need to add the its IP in the command. Add the Jetstack Helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install the jetstack/cert-manager chart using Helm : helm install cert-manager jetstack/cert-manager --version 1 .8.0 \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true Note To check if the installation was succesfull you can run the helm ls -n cert-manager and confirm the deployment status. Create a Kubernetes Secret for the DigitalOcean Provider that cert-manager is going to use to perform the DNS-01 challenge using a DigitalOcean API token: DO_API_TOKEN = \"<YOUR_DO_API_TOKEN_HERE>\" kubectl create secret generic \"digitalocean-dns\" \\ --namespace microservices-demo-dev \\ --from-literal = access-token = \" $DO_API_TOKEN \" Note The secret must be created in the same namespace where the Issuer CRD is located - in this case the microservides-demo-dev namespace. Create an issuer resource for cert-manager using kubectl (make sure to replace the <> placeholders first): The issuer manifest file looks like the following: Click to expand issuer manifest file apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-nginx-wcard namespace : microservices-demo-dev spec : # ACME issuer configuration: # `email` - the email address to be associated with the ACME account (make sure it's a valid one). # `server` - the URL used to access the ACME server\u2019s directory endpoint. # `privateKeySecretRef` - Kubernetes Secret to store the automatically generated ACME account private key. acme : email : <YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-nginx-wcard-private # List of challenge solvers that will be used to solve ACME challenges for the matching domains. solvers : # Use the DigitalOcean DNS API to manage DNS01 challenge records. - dns01 : digitalocean : # Kubernetes secret that contains the DO API token . # Must be in the same namespace as the Issuer CRD. tokenSecretRef : name : digitalocean-dns key : access-token Apply via kubectl: kubectl apply -f docs/02-development/assets/manifests/cert-manager-wildcard-issuer.yaml Info Running kubectl get issuer letsencrypt-nginx-wcard -n microservices-demo-dev should result in the True value being displayed under the READY column. Note If the Issuer object displays a Not Ready state you can describe the object to get additional information using: kubectl describe issuer letsencrypt-nginx-wcard -n microservices-demo-dev to get more information. Create the wildcard certificates resource using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The certificate manifest file looks like the following: Click to expland the certificate resource apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : <YOUR_DOMAIN_NAME> # Cert-Manager will put the resulting Secret in the same Kubernetes namespace as the Certificate. namespace : microservices-demo-dev spec : # Secret name to create, where the private key and certificate should be stored. secretName : <YOUR_DOMAIN_NAME> # What Issuer to use for getting the certificate. issuerRef : name : letsencrypt-nginx-wcard kind : Issuer group : cert-manager.io # Common name to be used on the Certificate. commonName : \"*.<YOUR_DOMAIN_NAME>\" # List of DNS subjectAltNames to be set on the Certificate. dnsNames : - \"<YOUR_DOMAIN_NAME>\" - \"*.<YOUR_DOMAIN_NAME>\" Apply via kubectl: kubectl apply -f docs/02-development/assets/manifests/cert-manager-wildcard-certificate.yaml To verify the certificate status run: kubectl get certificate <YOUR_DOMAIN_NAME> -n microservices-demo-dev Info This may take a few minutes to complete. If the Certificate object displays a not ready state you can run: kubectl logs -l app=cert-manager,app.kubernetes.io/component=controller -n cert-manager Add the Ingress Nginx host using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The ingress host manifest file looks like the following: Click to expand the ingress host resource apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-microservices-demo-dev namespace : microservices-demo-dev spec : tls : - hosts : - \"*.<YOUR_DOMAIN_NAME>\" secretName : <YOUR_DOMAIN_NAME> rules : - host : <YOUR_A_RECORD>.<YOUR_DOMAIN_NAME> http : paths : - path : / pathType : Prefix backend : service : name : frontend port : number : 80 ingressClassName : nginx Apply via kubectl: kubectl apply -f docs/02-development/assets/manifests/ingress-host.yaml Open a web browser and point to <YOUR_A_RECORD>.<YOUR_DOMAIN> . You should see the online boutique welcome page. The connection is secure and the certificate is a valid one issued by Let's Encrypt . Next, you will deploy the Kubernetes Dashboard and Kubernetes Metrics Server to your cluster in order to visualize application and cluster related metrics, as well as corresponding logs and events.","title":"Installing the Nginx Ingress Controller"},{"location":"02-development/tilt-local/","text":"Introduction This section will show you how to do local development using Tilt . Tilt eases local development by taking away the pain of time consuming Docker builds, watching files, and bringing environments up to date. The way Tilt works is based on a single Tiltfile present in your root project directory containing environment setup logic. You will install the online boutique sample application on your local environment using Docker Desktop and Tilt. The way you will use Tilt in this guide is based on configuration profiles . This approach has a major benefit - application logic is decoupled from configuration data. It means, you don't have to modify the base Tiltfile only if really required, or to add new functionality. Tilt automatically loads (if present) the configuration file ( tilt_config.json ), and sets up the environment for you. Based on your current requirements or needs, you just pick the appropriate tilt_config.json file from the tilt-resources directory, and copy it alongside main Tiltfile. Below is an example of such setup: . . . \u251c\u2500\u2500 tilt-resources \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 tilt_config.json \u2502 \u2514\u2500\u2500 local \u2502 \u2514\u2500\u2500 tilt_config.json \u251c\u2500\u2500 Tiltfile \u2514\u2500\u2500 tilt_config.json You need to copy only once the environment specific configuration file, and then run tilt up from the root directory of your project: If you need a local environment setup - cp tilt-resources/local/tilt_config.json <Tiltfile_directory> . Then, run tilt up , and see it in action. If you need a remote development setup - cp tilt-resources/dev/tilt_config.json <Tiltfile_directory> . Then, run tilt up , and see it in action. Prerequisites To complete this section you will need: Tilt already installed and working as explained in the Installing Required Tools -> Tilt section. Docker desktop already installed and running as explained in the Installing Required Tools -> Docker Desktop section. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Local development with Tilt Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to microservices-demo folder: cd microservices-demo Switch your current Kubernetes config to docker-desktop : kubectl config use-context docker-desktop Note This is required for local development as Tilt can be ran against a production Kubernetes cluster for example, and you can accidentally perform unwanted changes. Within the current directory, copy the local profile configuration for Tilt: cp tilt-resources/local/tilt_config.json . Note Tilt configuration files will be excluded from git commits by .gitignore settings. Each developer customizes his profile locally as desired, and should not impact other team members. Bring the microservices-demo local environment up using Tilt: tilt up You should see the following output: Tilt started on http://localhost:10350/ v0.30.7, built 2022-08-12 (space) to open the browser (s) to stream logs (--stream=true) (t) to open legacy terminal mode (--legacy=true) (ctrl-c) to exit Press the Space bar to open Tilt's UI: You should see the following: Note Please note that from the top left you can switch between Table and Detail view. Detail view offers a lot more information on what Tilt is doing such as logs from all Kubernetes resources. Open a web browser and point to localhost:9090 . You should see the online boutique welcome page: Live Updates with Tilt Tilt has the ability to reload and rebuild resources at the right time. Every code change triggers Tilt to automatically rebuild local docker images, and roll out new versions of your application pods. Follow steps below to watch Tilt doing live updates for your application: Change directory to your local clone of the microservices-demo project directory (if not already). Then, open the src/frontend/templates/home.html file using a text editor of your choice (preferably with HTML lint support). For example, you can use VS Code : code src/frontend/templates/home.html Next, change one of the h3 tags to something different, such as: ... < div class = \"col-12\" > < h3 > On Sale Now </ h3 > </ div > }); Navigate to Tilt's detailed view using the web interface. You should see that the frontend resource is being rebuilt. Finally, open a web browser and point to localhost:9090 . You should see the updated online boutique welcome page with your changes: Info Due to browser cache the changes might not appear immediately and for this reason you can hard refresh your browser to see the changes. On modern browsers this can be achieved by pressing Command + Shift + R on macOS, and Ctrl + Shift + R for Linux systems. Next, you will learn how to perform remote development for the same set of microservices, using the Kubernetes environment created in the Set up Development DOKS section.","title":"Tilt for local development"},{"location":"02-development/tilt-local/#introduction","text":"This section will show you how to do local development using Tilt . Tilt eases local development by taking away the pain of time consuming Docker builds, watching files, and bringing environments up to date. The way Tilt works is based on a single Tiltfile present in your root project directory containing environment setup logic. You will install the online boutique sample application on your local environment using Docker Desktop and Tilt. The way you will use Tilt in this guide is based on configuration profiles . This approach has a major benefit - application logic is decoupled from configuration data. It means, you don't have to modify the base Tiltfile only if really required, or to add new functionality. Tilt automatically loads (if present) the configuration file ( tilt_config.json ), and sets up the environment for you. Based on your current requirements or needs, you just pick the appropriate tilt_config.json file from the tilt-resources directory, and copy it alongside main Tiltfile. Below is an example of such setup: . . . \u251c\u2500\u2500 tilt-resources \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 tilt_config.json \u2502 \u2514\u2500\u2500 local \u2502 \u2514\u2500\u2500 tilt_config.json \u251c\u2500\u2500 Tiltfile \u2514\u2500\u2500 tilt_config.json You need to copy only once the environment specific configuration file, and then run tilt up from the root directory of your project: If you need a local environment setup - cp tilt-resources/local/tilt_config.json <Tiltfile_directory> . Then, run tilt up , and see it in action. If you need a remote development setup - cp tilt-resources/dev/tilt_config.json <Tiltfile_directory> . Then, run tilt up , and see it in action.","title":"Introduction"},{"location":"02-development/tilt-local/#prerequisites","text":"To complete this section you will need: Tilt already installed and working as explained in the Installing Required Tools -> Tilt section. Docker desktop already installed and running as explained in the Installing Required Tools -> Docker Desktop section. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section.","title":"Prerequisites"},{"location":"02-development/tilt-local/#local-development-with-tilt","text":"Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to microservices-demo folder: cd microservices-demo Switch your current Kubernetes config to docker-desktop : kubectl config use-context docker-desktop Note This is required for local development as Tilt can be ran against a production Kubernetes cluster for example, and you can accidentally perform unwanted changes. Within the current directory, copy the local profile configuration for Tilt: cp tilt-resources/local/tilt_config.json . Note Tilt configuration files will be excluded from git commits by .gitignore settings. Each developer customizes his profile locally as desired, and should not impact other team members. Bring the microservices-demo local environment up using Tilt: tilt up You should see the following output: Tilt started on http://localhost:10350/ v0.30.7, built 2022-08-12 (space) to open the browser (s) to stream logs (--stream=true) (t) to open legacy terminal mode (--legacy=true) (ctrl-c) to exit Press the Space bar to open Tilt's UI: You should see the following: Note Please note that from the top left you can switch between Table and Detail view. Detail view offers a lot more information on what Tilt is doing such as logs from all Kubernetes resources. Open a web browser and point to localhost:9090 . You should see the online boutique welcome page:","title":"Local development with Tilt"},{"location":"02-development/tilt-local/#live-updates-with-tilt","text":"Tilt has the ability to reload and rebuild resources at the right time. Every code change triggers Tilt to automatically rebuild local docker images, and roll out new versions of your application pods. Follow steps below to watch Tilt doing live updates for your application: Change directory to your local clone of the microservices-demo project directory (if not already). Then, open the src/frontend/templates/home.html file using a text editor of your choice (preferably with HTML lint support). For example, you can use VS Code : code src/frontend/templates/home.html Next, change one of the h3 tags to something different, such as: ... < div class = \"col-12\" > < h3 > On Sale Now </ h3 > </ div > }); Navigate to Tilt's detailed view using the web interface. You should see that the frontend resource is being rebuilt. Finally, open a web browser and point to localhost:9090 . You should see the updated online boutique welcome page with your changes: Info Due to browser cache the changes might not appear immediately and for this reason you can hard refresh your browser to see the changes. On modern browsers this can be achieved by pressing Command + Shift + R on macOS, and Ctrl + Shift + R for Linux systems. Next, you will learn how to perform remote development for the same set of microservices, using the Kubernetes environment created in the Set up Development DOKS section.","title":"Live Updates with Tilt"},{"location":"02-development/tilt-remote/","text":"Introduction This section will show you how to do remote development using Tilt . It is very similar to the local development guide, the only difference being you will work directly on the remote Kubernetes cluster created in the Set up Development DOKS section. Application changes and reloading will happen on the fly on the remote development cluster as well. Next, you will use Tilt to deploy the online boutique sample application to your development DOKS cluster. The same approach based on Tilt configuration profiles is being used here as well. You only need to copy the environment specific configuration file ( tilt_config.json ) to your root project directory, and then run tilt up . Prerequisites To complete this section you will need: Tilt already installed and working as explained in the Installing Required Tools section. A container registry already set up as explained in the Set up DOCR section. A development Kubernetes cluster (DOKS) up and running as explained in the Set up Development DOKS section. You need to have proper permissions to create namespaces and deploy resources. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Remote development with Tilt Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to microservices-demo folder: cd microservices-demo Switch current Kubernetes config to your microservices-demo-dev cluster. Bear in mind that the context is prefixed using the do-<region_id>- string (e.g. do-nyc1- ): kubectl config use-context do -nyc1-microservices-demo-dev All microservices Docker images are built on your local machine, and then pushed to your DOCR registry. A registry login is required first using doctl : doctl registry login Within the current directory, copy the dev profile configuration for Tilt: cp tilt-resources/dev/tilt_config.json . Warn Make sure to give a unique value for the namespace property in the tilt_config.json file to avoid overriding existing applications on the remote development cluster. Tilt configuration files will be excluded from git commits by .gitignore settings. Each developer customizes his profile locally as desired, and should not impact other team members. Bring the microservices-demo dev environment up using Tilt: tilt up You should see the following output: Tilt started on http://localhost:10350/ v0.30.7, built 2022-08-12 (space) to open the browser (s) to stream logs (--stream=true) (t) to open legacy terminal mode (--legacy=true) (ctrl-c) to exit Press the Space bar to open Tilt's UI: Note Please note that from the top left you can switch between Table and Detail view. Detail view offers a lot more information on what Tilt is doing such as logs from all Kubernetes resources. This may take a few minutes. Open a web browser and point to localhost:9090 . You should see the online boutique welcome page: Note Although you open a connection to localhost in your web browser, traffic is forwarded to the remote development cluster by Tilt. Live Updates with Tilt Tilt has the ability to reload and rebuild resources at the right time. Every code change triggers Tilt to automatically rebuild local docker images, and roll out new versions of your application pods. Follow steps below to watch Tilt doing live updates for your application: Change directory to your local clone of the microservices-demo project directory (if not already). Then, open the src/frontend/templates/home.html file using a text editor of your choice (preferably with HTML lint support). For example, you can use VS Code : code src/frontend/templates/home.html Next, change one of the h3 tags to something different, such as: ... <div class=\"col-12\"> <h3>On Sale Now</h3> </div> }); Navigate to Tilt 's detailed view using the web interface. You should see the frontend resource being rebuilt. The updated docker image will be pushed to your DOCR. Finally, open a web browser and point to localhost:9090 . You should see the online boutique welcome page updated with your changes: Info Due to browser cache the changes might not appear immediately and for this reason you can hard refresh your browser to see the changes. On modern browsers this can be achieved by pressing Command + Shift + R on macOS, and Ctrl + Shift + R for Linux systems. Next, you will learn how to deploy and configure the Nginx ingress controller for your development cluster (DOKS) to expose microservices to the outside world. You will also learn how to set up cert-manager to automatically issue valid TLS certificates for your applications.","title":"Tilt for remote development"},{"location":"02-development/tilt-remote/#introduction","text":"This section will show you how to do remote development using Tilt . It is very similar to the local development guide, the only difference being you will work directly on the remote Kubernetes cluster created in the Set up Development DOKS section. Application changes and reloading will happen on the fly on the remote development cluster as well. Next, you will use Tilt to deploy the online boutique sample application to your development DOKS cluster. The same approach based on Tilt configuration profiles is being used here as well. You only need to copy the environment specific configuration file ( tilt_config.json ) to your root project directory, and then run tilt up .","title":"Introduction"},{"location":"02-development/tilt-remote/#prerequisites","text":"To complete this section you will need: Tilt already installed and working as explained in the Installing Required Tools section. A container registry already set up as explained in the Set up DOCR section. A development Kubernetes cluster (DOKS) up and running as explained in the Set up Development DOKS section. You need to have proper permissions to create namespaces and deploy resources. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section.","title":"Prerequisites"},{"location":"02-development/tilt-remote/#remote-development-with-tilt","text":"Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to microservices-demo folder: cd microservices-demo Switch current Kubernetes config to your microservices-demo-dev cluster. Bear in mind that the context is prefixed using the do-<region_id>- string (e.g. do-nyc1- ): kubectl config use-context do -nyc1-microservices-demo-dev All microservices Docker images are built on your local machine, and then pushed to your DOCR registry. A registry login is required first using doctl : doctl registry login Within the current directory, copy the dev profile configuration for Tilt: cp tilt-resources/dev/tilt_config.json . Warn Make sure to give a unique value for the namespace property in the tilt_config.json file to avoid overriding existing applications on the remote development cluster. Tilt configuration files will be excluded from git commits by .gitignore settings. Each developer customizes his profile locally as desired, and should not impact other team members. Bring the microservices-demo dev environment up using Tilt: tilt up You should see the following output: Tilt started on http://localhost:10350/ v0.30.7, built 2022-08-12 (space) to open the browser (s) to stream logs (--stream=true) (t) to open legacy terminal mode (--legacy=true) (ctrl-c) to exit Press the Space bar to open Tilt's UI: Note Please note that from the top left you can switch between Table and Detail view. Detail view offers a lot more information on what Tilt is doing such as logs from all Kubernetes resources. This may take a few minutes. Open a web browser and point to localhost:9090 . You should see the online boutique welcome page: Note Although you open a connection to localhost in your web browser, traffic is forwarded to the remote development cluster by Tilt.","title":"Remote development with Tilt"},{"location":"02-development/tilt-remote/#live-updates-with-tilt","text":"Tilt has the ability to reload and rebuild resources at the right time. Every code change triggers Tilt to automatically rebuild local docker images, and roll out new versions of your application pods. Follow steps below to watch Tilt doing live updates for your application: Change directory to your local clone of the microservices-demo project directory (if not already). Then, open the src/frontend/templates/home.html file using a text editor of your choice (preferably with HTML lint support). For example, you can use VS Code : code src/frontend/templates/home.html Next, change one of the h3 tags to something different, such as: ... <div class=\"col-12\"> <h3>On Sale Now</h3> </div> }); Navigate to Tilt 's detailed view using the web interface. You should see the frontend resource being rebuilt. The updated docker image will be pushed to your DOCR. Finally, open a web browser and point to localhost:9090 . You should see the online boutique welcome page updated with your changes: Info Due to browser cache the changes might not appear immediately and for this reason you can hard refresh your browser to see the changes. On modern browsers this can be achieved by pressing Command + Shift + R on macOS, and Ctrl + Shift + R for Linux systems. Next, you will learn how to deploy and configure the Nginx ingress controller for your development cluster (DOKS) to expose microservices to the outside world. You will also learn how to set up cert-manager to automatically issue valid TLS certificates for your applications.","title":"Live Updates with Tilt"},{"location":"03-staging/deploying-the-online-boutique-sample-application-staging/","text":"Introduction Note On the development cluster you used Tilt to deploy the sample application. On the staging environment, kustomize will be used to initially deploy the application and then ArgoCD will do the heavy lifting for future deployments. In this section you will learn how to deploy the online boutique sample application using Kustomize . It is tool for customizing Kubernetes configurations. It has the following features to manage application configuration files: generating resources from other sources setting cross-cutting fields for resources composing and customizing collections of resources A more common use case of Kustomize is that you\u2019ll need multiple variants of a common set of resources, e.g., a development , staging and production variant. For this purpose, kustomize supports the idea of an overlay and a base . Both are represented by a kustomization file. The base declares things that the variants share in common (both resources and a common customization of those resources), and the overlays declare the differences. This is well represented in the structure of the online boutique sample application repository structure . Info You will be using the kubectl built-in version of Kustomize . Prerequisites To complete this section you will need: A container registry already set up as explained in the Set up DOCR section. The microservices-demo images build and pushed to DOCR as explained in the Set up DOCR --> Building and pushing docker images to DOCR A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Bootstrap the online boutique application using Kustomize Clone your fork of the kubernetes-sample-apps if you haven't already (make sure to replace the <> placeholders). git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/kubernetes-sample-apps.git Info The kubernetes-sample-apps repository was forked initially in the Setup DOCR section. Change directory to the microservices-demo folder: cd kubernetes-sample-apps/microservices-demo Deploy the Kustomization to your cluster using kubectl : kubectl apply -k kustomize/staging Note To verify that the deployment was succesful run the kubectl get all -n microservices-demo-staging command. The application is deployed to the staging environment using the images built and pushed in the Setup DOCR section. Access the web interface by port-forwarding the frontend service: kubectl port-forward service/frontend -n microservices-demo-staging 9090 :80 Open a web browser and point to localhost:9090 . You should see the online boutique welcome page. Note Although you open a connection to localhost in your web browser, traffic is forwarded to the remote staging cluster by kubectl . Next, you will deploy and configure the Nginx ingress controller for your staging cluster (DOKS) to expose microservices to the outside world. You will also set up cert-manager to automatically issue valid TLS certificates for your applications.","title":"Deploying the online boutique sample application"},{"location":"03-staging/deploying-the-online-boutique-sample-application-staging/#introduction","text":"Note On the development cluster you used Tilt to deploy the sample application. On the staging environment, kustomize will be used to initially deploy the application and then ArgoCD will do the heavy lifting for future deployments. In this section you will learn how to deploy the online boutique sample application using Kustomize . It is tool for customizing Kubernetes configurations. It has the following features to manage application configuration files: generating resources from other sources setting cross-cutting fields for resources composing and customizing collections of resources A more common use case of Kustomize is that you\u2019ll need multiple variants of a common set of resources, e.g., a development , staging and production variant. For this purpose, kustomize supports the idea of an overlay and a base . Both are represented by a kustomization file. The base declares things that the variants share in common (both resources and a common customization of those resources), and the overlays declare the differences. This is well represented in the structure of the online boutique sample application repository structure . Info You will be using the kubectl built-in version of Kustomize .","title":"Introduction"},{"location":"03-staging/deploying-the-online-boutique-sample-application-staging/#prerequisites","text":"To complete this section you will need: A container registry already set up as explained in the Set up DOCR section. The microservices-demo images build and pushed to DOCR as explained in the Set up DOCR --> Building and pushing docker images to DOCR A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section.","title":"Prerequisites"},{"location":"03-staging/deploying-the-online-boutique-sample-application-staging/#bootstrap-the-online-boutique-application-using-kustomize","text":"Clone your fork of the kubernetes-sample-apps if you haven't already (make sure to replace the <> placeholders). git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/kubernetes-sample-apps.git Info The kubernetes-sample-apps repository was forked initially in the Setup DOCR section. Change directory to the microservices-demo folder: cd kubernetes-sample-apps/microservices-demo Deploy the Kustomization to your cluster using kubectl : kubectl apply -k kustomize/staging Note To verify that the deployment was succesful run the kubectl get all -n microservices-demo-staging command. The application is deployed to the staging environment using the images built and pushed in the Setup DOCR section. Access the web interface by port-forwarding the frontend service: kubectl port-forward service/frontend -n microservices-demo-staging 9090 :80 Open a web browser and point to localhost:9090 . You should see the online boutique welcome page. Note Although you open a connection to localhost in your web browser, traffic is forwarded to the remote staging cluster by kubectl . Next, you will deploy and configure the Nginx ingress controller for your staging cluster (DOKS) to expose microservices to the outside world. You will also set up cert-manager to automatically issue valid TLS certificates for your applications.","title":"Bootstrap the online boutique application using Kustomize"},{"location":"03-staging/observability-staging/","text":"Introduction The goal of observability is to understand what\u2019s happening across all of your environments and among the technologies, so you can detect and resolve issues to keep your systems efficient and reliable. Observability is a measure of how well the system\u2019s internal states can be inferred from knowledge of its external outputs. It uses the data and insights that monitoring produces to provide a holistic understanding of your system, including its health and performance. The observability of your system, then, depends partly on how well your monitoring metrics can interpret your system's performance indicators. Prerequisites To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A DO Spaces bucket for Loki storage. Please follow the official DigitalOcean tutorial to create one . Make sure that it is set to restrict file listing for security reasons. Installing the Prometheus Monitoring Stack Add the Helm repository and list the available charts: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update prometheus-community Install the kube-prometheus-stack , using Helm : HELM_CHART_VERSION = \"35.5.1\" helm install kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ --create-namespace \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note A specific version for the Helm chart is used. In this case 35.5.1 was picked, which maps to the 0.56.3 version of the application. To check if the installation was successful, run the helm ls -n monitoring command, and confirm the deployment status. Connect to Grafana (using default credentials: admin/prom-operator ) - by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Info You should NOT expose Grafana to public network (eg. you should create an ingress mapping or LB service). Open a web browser and point to localhost:3000 . You should see the Grafna login page. Info Grafana installation comes with a number of dashboards. Open a web browser on localhost:3000 . Once in, you can go to Dashboards -> Browse , and choose different dashboards. As an example, you can open the General / Kubernetes / Compute Resources / Node (Pods) and view the resource metrics for a node and its related pods. Configuring Persistent Storage for Prometheus In this section, you will learn how to enable persistent storage for Prometheus , so that metrics data is persisted across server restarts , or in case of cluster failures . For the staging environment, you will define a 5 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the \"docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: prometheusSpec : storageSpec : volumeClaimTemplate : spec : storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 5Gi Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel. Configuring Persistent Storage for Grafana In this section, you will learn how to enable persistent storage for Grafana , so that metrics data is persisted across server restarts , or in case of cluster failures . For the staging environment, you will define a 5 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: grafana : ... persistence : enabled : true storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] size : 5Gi Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel. Installing the Loki Stack In this section you will learn about Loki , which is a log aggregation system inspired by Prometheus . Loki uses Promtail to fetch logs from all Pods running in your cluster. Then, logs are aggregated and compressed , and sent to the configured storage . Next, you can connect Loki data source to Grafana and view the logs. Add the Grafana Helm repository and list the available charts: helm repo add grafana https://grafana.github.io/helm-charts helm repo update grafana Intall the Loki stack using Helm : HELM_CHART_VERSION = \"2.6.4\" helm install loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ --create-namespace \\ -f \"docs/03-staging/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note The above values file, enables Loki and Promtail for you, so no other input is required. Prometheus and Grafana installation is disabled, because Installing the Prometheus Monitoring Stack took care of it already. The 2.6.4 Helm chart version is picked for loki-stack , which maps to application version 2.4.2 . To check if the installation was successful, run the helm ls -n loki-stack command, and confirm the deployment status. Configuring Grafana with Loki In this section, you will add the Loki data source to Grafana . First, you need to expose the Grafana web interface on your local machine (default credentials: admin/prom-operator ): kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Next, open a web browser on localhost:3000 , and follow below steps: Click the Configuration gear from the left panel. Select Data sources . Click the Add data source blue button. Select Loki from the list and add Loki url http://loki.loki-stack:3100 . Save and test. Info If everything goes well, a green label message will appear, saying Data source connected and labels found. You can access logs from the Explore tab of Grafana . Make sure to select Loki as the data source. Use the Help button for log search cheat sheet. !!! info As an example query, to retrieve all the logs for the microservices-demo-staging namespace you can run: {namespace=\"microservices-demo-staging\"} . Configuring Persistent Storage for Loki In this step, you will learn how to enable persistent storage for Loki . You're going to use the DO Spaces bucket created in the Prerequisites section. Open the docs/03-staging/assets/manifests/loki-stack-values-v35.5.1.yaml file provided and remove the comments surrounding the schema_config and storage_config keys. The definition should look like this: Click to expand the loki config loki : enabled : true config : schema_config : configs : - from : \"2020-10-24\" store : boltdb-shipper object_store : aws schema : v11 index : prefix : index_ period : 24h storage_config : boltdb_shipper : active_index_directory : /data/loki/boltdb-shipper-active cache_location : /data/loki/boltdb-shipper-cache cache_ttl : 24h shared_store : aws aws : bucketnames : <YOUR_DO_SPACES_BUCKET_NAME_HERE> endpoint : <YOUR_DO_SPACES_BUCKET_ENDPOINT_HERE> # in the following format: <region>.digitaloceanspaces.com region : <YOUR_DO_SPACES_BUCKET_REGION_HERE> # short region name (e.g.: fra1) access_key_id : <YOUR_DO_SPACES_ACCESS_KEY_HERE> secret_access_key : <YOURDO_SPACES_SECRET_KEY_HERE> s3forcepathstyle : true Apply the new settings using Helm : HELM_CHART_VERSION = \"2.6.4\" helm upgrade loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ -f \"docs/03-staging/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check if the main Loki application pod is up and running by runnging the following kubectl command: kubectl get pods -n loki-stack -l app=loki . If everything goes well, you should see the DO Spaces bucket containing the index and chunks folders (the chunks folder is called fake , which is a strange name - this is by design, when not running in multi-tenant mode). Setting up a retention policy In this step you will set up a retention policy for your DO Spaces bucket. S3CMD is a good utility to have in order to set up retention policies. Please follow the DigitalOcean guide for installing and setting up s3cmd . Configure the Loki bucket lifecycle, using s3cmd : Click to expand the Loki bucket lifecycle <LifecycleConfiguration xmlns= \"http://s3.amazonaws.com/doc/2006-03-01/\" > <Rule> <ID> Expire old fake data </ID> <Prefix> fake/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> <Rule> <ID> Expire old index data </ID> <Prefix> index/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> </LifecycleConfiguration> s3cmd setlifecycle 04 -setup-observability/assets/manifests/loki_do_spaces_lifecycle.xml s3://<LOKI_STORAGE_BUCKET_NAME> Check that the policy was set (please replace the <> placeholders accordingly): s3cmd getlifecycle s3://<LOKI_STORAGE_BUCKET_NAME> Note The DO Spaces backend implementation will clean the objects for you automatically , based on the expiration date. You can always go back and edit the policy if needed later on, by uploading a new one. Setting up Alert Manager Alertmanager is deployed alongside Prometheus and forms the alerting layer of the kube-prom-stack . It handles alerts generated by Prometheus by deduplicating, grouping, and routing them to various integrations such as email, Slack or PagerDuty. Alerts and notifications are a critical part of your workflow. When things go wrong (e.g. any service is down, or a pod is crashing, etc.), you will want to get notifications in real time to handle critical situations as soon as possible. To create a new alert, you need to add a new definition in the additionalPrometheusRule s section from the kube-prom-stack Helm values file. You will be creating a sample alert that will trigger if the microservices-demo-staging namespace does not have an expected number of instances. The expected number of pods for the online boutique application is 10. Open the docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the additionalPrometheusRulesMap block. The definition should look like this: additionalPrometheusRulesMap : rule-name : groups : - name : online-boutique-instance-down rules : - alert : OnlineBoutiqueInstanceDown expr : sum(kube_pod_owner{namespace=\"microservices-demo-staging\"}) by (namespace) < 10 for : 1m labels : severity : 'critical' annotations : description : ' The Number of pods from the namespace {{ $labels.namespace }} is lower than the expected 10. ' summary : 'Pod {{ $labels.pod }} down' Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Info To check that the alert has been created successfully, first port-forward to your local machine by running this command: kubectl --namespace monitoring port-forward service/kube-prom-stack-kube-prome-prometheus 9090:9090 . Navigate to the Promethes Console click on the Alerts menu item and identify the OnlineBoutiqueInstanceDown alert. It should be visible at the bottom of the list. Configuring Alertmanager to Send Notifications to Slack To complete this section you need to have administrative rights over a workspace. This will enable you to create the incoming webhook you will need in the next steps. You will also need to create a channel where you would like to receive notifications from AlertManager . Steps to follow: Open a web browser and navigate to https://api.slack.com/apps and click on the Create New App button. In the Create an app window select the From scratch option. Then, give your application a name and select the appropriate workspace. From the Basic Information page click on the Incoming Webhooks option, turn it on and click on the Add New Webhook to Workspace button at the bottom. On the next page, use the Search for a channel... drop-down list to select the desired channel where you want to send notifications. When ready, click on the Allow button. Take note of the Webhook URL value displayed on the page. You will be using it in the next section. Next you will tell Alertmanager how to send Slack notifications. Open the docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the alertmanager.config block. Make sure to update the <> placeholders accordingly. The definition should look like: Click to expand the alertmanager config alertmanager : enabled : true config : global : resolve_timeout : 5m slack_api_url : \"<YOUR_SLACK_APP_INCOMING_WEBHOOK_URL_HERE>\" route : receiver : \"null\" repeat_interval : 12h routes : - receiver : \"slack-notifications\" matchers : - alertname=\"OnlineBoutiqueInstanceDown\" continue : false receivers : - name : \"null\" - name : \"slack-notifications\" slack_configs : - channel : \"#<YOUR_SLACK_CHANNEL_NAME_HERE>\" send_resolved : true title : \"{{ range .Alerts }}{{ .Annotations.summary }}\\n{{ end }}\" text : \"{{ range .Alerts }}{{ .Annotations.description }}\\n{{ end }}\" Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" !!! info At this point you should only receieve alerts from the matching OnlinqBoutiqueInstanceDown alertname. Since the continue is set to false Alertmanager will only send notifications from this alert and stop sending for others. Clicking on the notification name in Slack will open a web browser to an unreachable web page with the internal Kubernetes DNS of the Alertmanager pod. This is expected. For more information you can check out this article . For additional information about the configuration parameters for Alertmanager you can check out this doc . You can also at some notification examples in this article . Setting up Event Exporter for events retention A Kubernetes event is an object that shows what\u2019s happening inside a cluster, node, pod, or container. These objects are usually generated in response to changes that occur inside your K8s system. The Kubernetes API Server enables all core components to create these events. Generally, each event is accompanied by a log message as well. Event objects are not regular log events, therefore the Kubernetes logs do not include them. Kubernetes has no builtin support to store or forward these events on the long term, and they are cleaned up after a short retention time defaulting to just 1 hour. In this section you will learn how to configure the Kubernetes Events Exporter and collect and perist those events using Loki . Create the ServiceAccount , ClusterRole and ClusterRoleBinding using kubectl : The manifest file looks like the following: Click to expand the manifest file apiVersion : v1 kind : Namespace metadata : name : event-exporter --- apiVersion : v1 kind : ServiceAccount metadata : namespace : event-exporter name : event-exporter --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : event-exporter roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : view subjects : - kind : ServiceAccount namespace : event-exporter name : event-exporter kubectl apply -f docs/03-staging/assets/manifests/event-exporter-roles.yaml Create the event exporter config using kubectl : The manifest file for the config looks like the following: Click to expand the config manifest file apiVersion : v1 kind : ConfigMap metadata : name : event-exporter-cfg namespace : event-exporter data : config.yaml : | logLevel : error logFormat : json route : routes : - match : - receiver : \"dump\" receivers : - name : \"dump\" stdout : {} kubectl apply -f docs/03-staging/assets/manifests/event-exporter-config.yaml Finally, create the event exporter deployment using kubectl: The manifest file for the deployment looks like the following: Click to expand the deployment manifest file apiVersion : apps/v1 kind : Deployment metadata : name : event-exporter namespace : event-exporter spec : replicas : 1 template : metadata : labels : app : event-exporter version : v1 spec : serviceAccountName : event-exporter containers : - name : event-exporter image : ghcr.io/resmoio/kubernetes-event-exporter:latest imagePullPolicy : IfNotPresent args : - -conf=/data/config.yaml volumeMounts : - mountPath : /data name : cfg volumes : - name : cfg configMap : name : event-exporter-cfg selector : matchLabels : app : event-exporter version : v1 kubectl apply -f docs/03-staging/assets/manifests/event-exporter-deployment.yaml Viewing events in Grafana If the installation went well and no errors were reported you should start seeing event start to flow in Grafana. Connect to Grafana (using default credentials: admin/prom-operator ) by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Open a web browser on localhost:3000 . Once in, you can go to Explore menu and select the Loki as a datasource. In the Log browser input enter the following: { app= \"event-exporter\" } Info You should see events in the Logs section. Any of the fields in the Detected fields section of a log detail view can be used to query. For example you can perform a query using the pod name and view specific logs for a certain pod: {app=\"event-exporter\"} |= \"shippingservice-79bdd5f858-gqm6g\" . For a more in depth explanation on the Obervability topic you can check out the Kubernetes Starter Kit Tutorial .","title":"Observability"},{"location":"03-staging/observability-staging/#introduction","text":"The goal of observability is to understand what\u2019s happening across all of your environments and among the technologies, so you can detect and resolve issues to keep your systems efficient and reliable. Observability is a measure of how well the system\u2019s internal states can be inferred from knowledge of its external outputs. It uses the data and insights that monitoring produces to provide a holistic understanding of your system, including its health and performance. The observability of your system, then, depends partly on how well your monitoring metrics can interpret your system's performance indicators.","title":"Introduction"},{"location":"03-staging/observability-staging/#prerequisites","text":"To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A DO Spaces bucket for Loki storage. Please follow the official DigitalOcean tutorial to create one . Make sure that it is set to restrict file listing for security reasons.","title":"Prerequisites"},{"location":"03-staging/observability-staging/#installing-the-prometheus-monitoring-stack","text":"Add the Helm repository and list the available charts: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update prometheus-community Install the kube-prometheus-stack , using Helm : HELM_CHART_VERSION = \"35.5.1\" helm install kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ --create-namespace \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note A specific version for the Helm chart is used. In this case 35.5.1 was picked, which maps to the 0.56.3 version of the application. To check if the installation was successful, run the helm ls -n monitoring command, and confirm the deployment status. Connect to Grafana (using default credentials: admin/prom-operator ) - by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Info You should NOT expose Grafana to public network (eg. you should create an ingress mapping or LB service). Open a web browser and point to localhost:3000 . You should see the Grafna login page. Info Grafana installation comes with a number of dashboards. Open a web browser on localhost:3000 . Once in, you can go to Dashboards -> Browse , and choose different dashboards. As an example, you can open the General / Kubernetes / Compute Resources / Node (Pods) and view the resource metrics for a node and its related pods.","title":"Installing the Prometheus Monitoring Stack"},{"location":"03-staging/observability-staging/#configuring-persistent-storage-for-prometheus","text":"In this section, you will learn how to enable persistent storage for Prometheus , so that metrics data is persisted across server restarts , or in case of cluster failures . For the staging environment, you will define a 5 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the \"docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: prometheusSpec : storageSpec : volumeClaimTemplate : spec : storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 5Gi Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel.","title":"Configuring Persistent Storage for Prometheus"},{"location":"03-staging/observability-staging/#configuring-persistent-storage-for-grafana","text":"In this section, you will learn how to enable persistent storage for Grafana , so that metrics data is persisted across server restarts , or in case of cluster failures . For the staging environment, you will define a 5 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: grafana : ... persistence : enabled : true storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] size : 5Gi Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel.","title":"Configuring Persistent Storage for Grafana"},{"location":"03-staging/observability-staging/#installing-the-loki-stack","text":"In this section you will learn about Loki , which is a log aggregation system inspired by Prometheus . Loki uses Promtail to fetch logs from all Pods running in your cluster. Then, logs are aggregated and compressed , and sent to the configured storage . Next, you can connect Loki data source to Grafana and view the logs. Add the Grafana Helm repository and list the available charts: helm repo add grafana https://grafana.github.io/helm-charts helm repo update grafana Intall the Loki stack using Helm : HELM_CHART_VERSION = \"2.6.4\" helm install loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ --create-namespace \\ -f \"docs/03-staging/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note The above values file, enables Loki and Promtail for you, so no other input is required. Prometheus and Grafana installation is disabled, because Installing the Prometheus Monitoring Stack took care of it already. The 2.6.4 Helm chart version is picked for loki-stack , which maps to application version 2.4.2 . To check if the installation was successful, run the helm ls -n loki-stack command, and confirm the deployment status.","title":"Installing the Loki Stack"},{"location":"03-staging/observability-staging/#configuring-grafana-with-loki","text":"In this section, you will add the Loki data source to Grafana . First, you need to expose the Grafana web interface on your local machine (default credentials: admin/prom-operator ): kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Next, open a web browser on localhost:3000 , and follow below steps: Click the Configuration gear from the left panel. Select Data sources . Click the Add data source blue button. Select Loki from the list and add Loki url http://loki.loki-stack:3100 . Save and test. Info If everything goes well, a green label message will appear, saying Data source connected and labels found. You can access logs from the Explore tab of Grafana . Make sure to select Loki as the data source. Use the Help button for log search cheat sheet. !!! info As an example query, to retrieve all the logs for the microservices-demo-staging namespace you can run: {namespace=\"microservices-demo-staging\"} .","title":"Configuring Grafana with Loki"},{"location":"03-staging/observability-staging/#configuring-persistent-storage-for-loki","text":"In this step, you will learn how to enable persistent storage for Loki . You're going to use the DO Spaces bucket created in the Prerequisites section. Open the docs/03-staging/assets/manifests/loki-stack-values-v35.5.1.yaml file provided and remove the comments surrounding the schema_config and storage_config keys. The definition should look like this: Click to expand the loki config loki : enabled : true config : schema_config : configs : - from : \"2020-10-24\" store : boltdb-shipper object_store : aws schema : v11 index : prefix : index_ period : 24h storage_config : boltdb_shipper : active_index_directory : /data/loki/boltdb-shipper-active cache_location : /data/loki/boltdb-shipper-cache cache_ttl : 24h shared_store : aws aws : bucketnames : <YOUR_DO_SPACES_BUCKET_NAME_HERE> endpoint : <YOUR_DO_SPACES_BUCKET_ENDPOINT_HERE> # in the following format: <region>.digitaloceanspaces.com region : <YOUR_DO_SPACES_BUCKET_REGION_HERE> # short region name (e.g.: fra1) access_key_id : <YOUR_DO_SPACES_ACCESS_KEY_HERE> secret_access_key : <YOURDO_SPACES_SECRET_KEY_HERE> s3forcepathstyle : true Apply the new settings using Helm : HELM_CHART_VERSION = \"2.6.4\" helm upgrade loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ -f \"docs/03-staging/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check if the main Loki application pod is up and running by runnging the following kubectl command: kubectl get pods -n loki-stack -l app=loki . If everything goes well, you should see the DO Spaces bucket containing the index and chunks folders (the chunks folder is called fake , which is a strange name - this is by design, when not running in multi-tenant mode).","title":"Configuring Persistent Storage for Loki"},{"location":"03-staging/observability-staging/#setting-up-a-retention-policy","text":"In this step you will set up a retention policy for your DO Spaces bucket. S3CMD is a good utility to have in order to set up retention policies. Please follow the DigitalOcean guide for installing and setting up s3cmd . Configure the Loki bucket lifecycle, using s3cmd : Click to expand the Loki bucket lifecycle <LifecycleConfiguration xmlns= \"http://s3.amazonaws.com/doc/2006-03-01/\" > <Rule> <ID> Expire old fake data </ID> <Prefix> fake/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> <Rule> <ID> Expire old index data </ID> <Prefix> index/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> </LifecycleConfiguration> s3cmd setlifecycle 04 -setup-observability/assets/manifests/loki_do_spaces_lifecycle.xml s3://<LOKI_STORAGE_BUCKET_NAME> Check that the policy was set (please replace the <> placeholders accordingly): s3cmd getlifecycle s3://<LOKI_STORAGE_BUCKET_NAME> Note The DO Spaces backend implementation will clean the objects for you automatically , based on the expiration date. You can always go back and edit the policy if needed later on, by uploading a new one.","title":"Setting up a retention policy"},{"location":"03-staging/observability-staging/#setting-up-alert-manager","text":"Alertmanager is deployed alongside Prometheus and forms the alerting layer of the kube-prom-stack . It handles alerts generated by Prometheus by deduplicating, grouping, and routing them to various integrations such as email, Slack or PagerDuty. Alerts and notifications are a critical part of your workflow. When things go wrong (e.g. any service is down, or a pod is crashing, etc.), you will want to get notifications in real time to handle critical situations as soon as possible. To create a new alert, you need to add a new definition in the additionalPrometheusRule s section from the kube-prom-stack Helm values file. You will be creating a sample alert that will trigger if the microservices-demo-staging namespace does not have an expected number of instances. The expected number of pods for the online boutique application is 10. Open the docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the additionalPrometheusRulesMap block. The definition should look like this: additionalPrometheusRulesMap : rule-name : groups : - name : online-boutique-instance-down rules : - alert : OnlineBoutiqueInstanceDown expr : sum(kube_pod_owner{namespace=\"microservices-demo-staging\"}) by (namespace) < 10 for : 1m labels : severity : 'critical' annotations : description : ' The Number of pods from the namespace {{ $labels.namespace }} is lower than the expected 10. ' summary : 'Pod {{ $labels.pod }} down' Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Info To check that the alert has been created successfully, first port-forward to your local machine by running this command: kubectl --namespace monitoring port-forward service/kube-prom-stack-kube-prome-prometheus 9090:9090 . Navigate to the Promethes Console click on the Alerts menu item and identify the OnlineBoutiqueInstanceDown alert. It should be visible at the bottom of the list.","title":"Setting up Alert Manager"},{"location":"03-staging/observability-staging/#configuring-alertmanager-to-send-notifications-to-slack","text":"To complete this section you need to have administrative rights over a workspace. This will enable you to create the incoming webhook you will need in the next steps. You will also need to create a channel where you would like to receive notifications from AlertManager . Steps to follow: Open a web browser and navigate to https://api.slack.com/apps and click on the Create New App button. In the Create an app window select the From scratch option. Then, give your application a name and select the appropriate workspace. From the Basic Information page click on the Incoming Webhooks option, turn it on and click on the Add New Webhook to Workspace button at the bottom. On the next page, use the Search for a channel... drop-down list to select the desired channel where you want to send notifications. When ready, click on the Allow button. Take note of the Webhook URL value displayed on the page. You will be using it in the next section. Next you will tell Alertmanager how to send Slack notifications. Open the docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the alertmanager.config block. Make sure to update the <> placeholders accordingly. The definition should look like: Click to expand the alertmanager config alertmanager : enabled : true config : global : resolve_timeout : 5m slack_api_url : \"<YOUR_SLACK_APP_INCOMING_WEBHOOK_URL_HERE>\" route : receiver : \"null\" repeat_interval : 12h routes : - receiver : \"slack-notifications\" matchers : - alertname=\"OnlineBoutiqueInstanceDown\" continue : false receivers : - name : \"null\" - name : \"slack-notifications\" slack_configs : - channel : \"#<YOUR_SLACK_CHANNEL_NAME_HERE>\" send_resolved : true title : \"{{ range .Alerts }}{{ .Annotations.summary }}\\n{{ end }}\" text : \"{{ range .Alerts }}{{ .Annotations.description }}\\n{{ end }}\" Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" !!! info At this point you should only receieve alerts from the matching OnlinqBoutiqueInstanceDown alertname. Since the continue is set to false Alertmanager will only send notifications from this alert and stop sending for others. Clicking on the notification name in Slack will open a web browser to an unreachable web page with the internal Kubernetes DNS of the Alertmanager pod. This is expected. For more information you can check out this article . For additional information about the configuration parameters for Alertmanager you can check out this doc . You can also at some notification examples in this article .","title":"Configuring Alertmanager to Send Notifications to Slack"},{"location":"03-staging/observability-staging/#setting-up-event-exporter-for-events-retention","text":"A Kubernetes event is an object that shows what\u2019s happening inside a cluster, node, pod, or container. These objects are usually generated in response to changes that occur inside your K8s system. The Kubernetes API Server enables all core components to create these events. Generally, each event is accompanied by a log message as well. Event objects are not regular log events, therefore the Kubernetes logs do not include them. Kubernetes has no builtin support to store or forward these events on the long term, and they are cleaned up after a short retention time defaulting to just 1 hour. In this section you will learn how to configure the Kubernetes Events Exporter and collect and perist those events using Loki . Create the ServiceAccount , ClusterRole and ClusterRoleBinding using kubectl : The manifest file looks like the following: Click to expand the manifest file apiVersion : v1 kind : Namespace metadata : name : event-exporter --- apiVersion : v1 kind : ServiceAccount metadata : namespace : event-exporter name : event-exporter --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : event-exporter roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : view subjects : - kind : ServiceAccount namespace : event-exporter name : event-exporter kubectl apply -f docs/03-staging/assets/manifests/event-exporter-roles.yaml Create the event exporter config using kubectl : The manifest file for the config looks like the following: Click to expand the config manifest file apiVersion : v1 kind : ConfigMap metadata : name : event-exporter-cfg namespace : event-exporter data : config.yaml : | logLevel : error logFormat : json route : routes : - match : - receiver : \"dump\" receivers : - name : \"dump\" stdout : {} kubectl apply -f docs/03-staging/assets/manifests/event-exporter-config.yaml Finally, create the event exporter deployment using kubectl: The manifest file for the deployment looks like the following: Click to expand the deployment manifest file apiVersion : apps/v1 kind : Deployment metadata : name : event-exporter namespace : event-exporter spec : replicas : 1 template : metadata : labels : app : event-exporter version : v1 spec : serviceAccountName : event-exporter containers : - name : event-exporter image : ghcr.io/resmoio/kubernetes-event-exporter:latest imagePullPolicy : IfNotPresent args : - -conf=/data/config.yaml volumeMounts : - mountPath : /data name : cfg volumes : - name : cfg configMap : name : event-exporter-cfg selector : matchLabels : app : event-exporter version : v1 kubectl apply -f docs/03-staging/assets/manifests/event-exporter-deployment.yaml","title":"Setting up Event Exporter for events retention"},{"location":"03-staging/observability-staging/#viewing-events-in-grafana","text":"If the installation went well and no errors were reported you should start seeing event start to flow in Grafana. Connect to Grafana (using default credentials: admin/prom-operator ) by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Open a web browser on localhost:3000 . Once in, you can go to Explore menu and select the Loki as a datasource. In the Log browser input enter the following: { app= \"event-exporter\" } Info You should see events in the Logs section. Any of the fields in the Detected fields section of a log detail view can be used to query. For example you can perform a query using the pod name and view specific logs for a certain pod: {app=\"event-exporter\"} |= \"shippingservice-79bdd5f858-gqm6g\" . For a more in depth explanation on the Obervability topic you can check out the Kubernetes Starter Kit Tutorial .","title":"Viewing events in Grafana"},{"location":"03-staging/setup-doks-staging/","text":"Introduction This section will show you how to create a DigitalOcean Kubernetes Cluster ( DOKS ) cluster which will be used as a staging environment, targeting the online boutique sample application used as a reference in this guide. Note A staging environment should be pretty close (if not similar) to a production environment hence you will be creating a bigger cluster, resource wise, to be able to handle the workload you would normally have on your production environment. Prerequisites To complete this section you will need: Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Make sure that you're authenticated with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API section. Provisioning a Staging DOKS Cluster for Microservices In this step, you will create a new Kubernetes cluster running on the DigitalOcean platform, using the doctl utility. Following command will create a DigitalOcean Kubernetes cluster named microservices-demo-staging , with a pool size of 3 nodes , auto-scale to 2-4 each having 2 vCPUs and 4gbGB of RAM, in the nyc1 region: doctl k8s cluster create microservices-demo-staging \\ --auto-upgrade = true \\ --maintenance-window \"saturday=21:00\" \\ --node-pool \"name=basicnp;size=s-2vcpu-4gb-amd;count=3;tag=cluster2;label=type=basic;auto-scale=true;min-nodes=2;max-nodes=4\" \\ --region nyc1 Notes The example cluster created above is using 3 nodes, each having 2vCPU/4GB size, which amounts to 84$/month . For simplicity and consistency through all the guide, the microservices-demo-staging name was picked for the example cluster. You can choose any name you like, but you need to make sure the naming convention stays consistent. It is recommended to use a region for your cluster that is closest to you for faster interaction. Run the following command - doctl k8s options regions to check available regions. Cluster auto upgrade is enabled ( --auto-upgrade=true ). Kubernetes clusters should be auto-upgraded to ensure that they always contain the latest security patches. Next, you can verify the cluster details. First, fetch your DOKS cluster ID : doctl k8s cluster list Finally, check if the kubectl context was set to point to your DOKS cluster. The doctl utility should do this automatically: kubectl config current-context For more info on this topic please see this Kubernetes Starter Kit DOKS Creation . Configuring DOKS for Private Registries From the command line run the following: doctl registry kubernetes-manifest | kubectl apply -f - This will configure your DOKS cluster to fetch images from your DOCR created in the Set up a DigitalOcean container registry section This step can also be achieved via the DigitalOcean cloud console. Please follow this guide . Next, you will learn how to deploy the online boutique sample application to your staging cluster using Kustomize .","title":"Set up Staging DOKS"},{"location":"03-staging/setup-doks-staging/#introduction","text":"This section will show you how to create a DigitalOcean Kubernetes Cluster ( DOKS ) cluster which will be used as a staging environment, targeting the online boutique sample application used as a reference in this guide. Note A staging environment should be pretty close (if not similar) to a production environment hence you will be creating a bigger cluster, resource wise, to be able to handle the workload you would normally have on your production environment.","title":"Introduction"},{"location":"03-staging/setup-doks-staging/#prerequisites","text":"To complete this section you will need: Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Make sure that you're authenticated with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API section.","title":"Prerequisites"},{"location":"03-staging/setup-doks-staging/#provisioning-a-staging-doks-cluster-for-microservices","text":"In this step, you will create a new Kubernetes cluster running on the DigitalOcean platform, using the doctl utility. Following command will create a DigitalOcean Kubernetes cluster named microservices-demo-staging , with a pool size of 3 nodes , auto-scale to 2-4 each having 2 vCPUs and 4gbGB of RAM, in the nyc1 region: doctl k8s cluster create microservices-demo-staging \\ --auto-upgrade = true \\ --maintenance-window \"saturday=21:00\" \\ --node-pool \"name=basicnp;size=s-2vcpu-4gb-amd;count=3;tag=cluster2;label=type=basic;auto-scale=true;min-nodes=2;max-nodes=4\" \\ --region nyc1 Notes The example cluster created above is using 3 nodes, each having 2vCPU/4GB size, which amounts to 84$/month . For simplicity and consistency through all the guide, the microservices-demo-staging name was picked for the example cluster. You can choose any name you like, but you need to make sure the naming convention stays consistent. It is recommended to use a region for your cluster that is closest to you for faster interaction. Run the following command - doctl k8s options regions to check available regions. Cluster auto upgrade is enabled ( --auto-upgrade=true ). Kubernetes clusters should be auto-upgraded to ensure that they always contain the latest security patches. Next, you can verify the cluster details. First, fetch your DOKS cluster ID : doctl k8s cluster list Finally, check if the kubectl context was set to point to your DOKS cluster. The doctl utility should do this automatically: kubectl config current-context For more info on this topic please see this Kubernetes Starter Kit DOKS Creation .","title":"Provisioning a Staging DOKS Cluster for Microservices"},{"location":"03-staging/setup-doks-staging/#configuring-doks-for-private-registries","text":"From the command line run the following: doctl registry kubernetes-manifest | kubectl apply -f - This will configure your DOKS cluster to fetch images from your DOCR created in the Set up a DigitalOcean container registry section This step can also be achieved via the DigitalOcean cloud console. Please follow this guide . Next, you will learn how to deploy the online boutique sample application to your staging cluster using Kustomize .","title":"Configuring DOKS for Private Registries"},{"location":"03-staging/setup-ingress-staging/","text":"Introduction In this section, you will install and configure the Kubernetes-maintained version of the Nginx Ingress Controller. Then, you're going to issue TLS certificates for your hosts (thus enabling TLS termination), and route traffic to your backend applications. Prerequisites To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A valid domain available and configured to point to DigitalOcean name servers. More information is available in this article . Digital Ocean is not a domain registrar, so you will need to purchase the domain from a well known vendor, such as GoDaddy. Installing the Nginx Ingress Controller In this section you will install the community maintained version of the Nginx ingress controller. Please follow below steps to install Nginx using Helm: Add the Ingress Nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update ingress-nginx helm search repo ingress-nginx Install the Nginx Ingress Controller using Helm : helm install ingress-nginx ingress-nginx/ingress-nginx --version 4 .1.3 \\ --namespace ingress-nginx \\ --create-namespace Note To check if the installation was successful, run the helm ls -n ingress-nginx command, and confirm the deployment status. Configure DNS for Nginx Ingress Controller: doctl compute domain create <YOUR_DOMAIN_NAME> Info Please note that this domain matches the domain you purchased in the Prerequisites section. You will use this domain to create additional sub-domains to use with the microservices app you will deploy in this section. You can also use the domain you already created in the Development ingress setup section. Create an A record for your host (make sure to replace the <> placeholders first): doctl compute domain records create <YOUR_DOMAIN_NAME> \\ --record-type \"A\" --record-name <YOUR_RECORD_NAME> \\ --record-data \"<YOUR_STAGING_LB_IP_ADDRESS>\" \\ --record-ttl \"30\" Add the Jetstack Helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install the jetstack/cert-manager chart using Helm : helm install cert-manager jetstack/cert-manager --version 1 .8.0 \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true Note To check if the installation was succesfull you can run the helm ls -n cert-manager and confirm the deployment status. Create a Kubernetes Secret for the DigitalOcean Provider that cert-manager is going to use to perform the DNS-01 challenge using a DigitalOcean API token: DO_API_TOKEN = \"<YOUR_DO_API_TOKEN_HERE>\" kubectl create secret generic \"digitalocean-dns\" \\ --namespace microservices-demo-staging \\ --from-literal = access-token = \" $DO_API_TOKEN \" Note The secret must be created in the same namespace where the Issuer CRD is located - in this case the microservides-demo-staging namespace. Create an issuer resource for cert-manager using kubectl (make sure to replace the <> placeholders first): The issuer manifest file looks like the following: Click to expand issuer manifest file apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-nginx-wcard namespace : microservices-demo-staging spec : # ACME issuer configuration: # `email` - the email address to be associated with the ACME account (make sure it's a valid one). # `server` - the URL used to access the ACME server\u2019s directory endpoint. # `privateKeySecretRef` - Kubernetes Secret to store the automatically generated ACME account private key. acme : email : <YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-nginx-wcard-private # List of challenge solvers that will be used to solve ACME challenges for the matching domains. solvers : # Use the DigitalOcean DNS API to manage DNS01 challenge records. - dns01 : digitalocean : # Kubernetes secret that contains the DO API token . # Must be in the same namespace as the Issuer CRD. tokenSecretRef : name : digitalocean-dns key : access-token Apply via kubectl: kubectl apply -f docs/03-staging/assets/manifests/cert-manager-wildcard-issuer.yaml Info Running kubectl get issuer letsencrypt-nginx-wcard -n microservices-demo-staging should result in the True value being displayed under the READY column. Note If the Issuer object displays a Not Ready state you can describe the object to get additional information using: kubectl describe issuer letsencrypt-nginx-wcard -n microservices-demo-staging to get more information. Create the wildcard certificates resource using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The certificate manifest file looks like the following: Click to expland the certificate resource apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : <YOUR_DOMAIN_NAME> # Cert-Manager will put the resulting Secret in the same Kubernetes namespace as the Certificate. namespace : microservices-demo-staging spec : # Secret name to create, where the private key and certificate should be stored. secretName : <YOUR_DOMAIN_NAME> # What Issuer to use for getting the certificate. issuerRef : name : letsencrypt-nginx-wcard kind : Issuer group : cert-manager.io # Common name to be used on the Certificate. commonName : \"*.<YOUR_DOMAIN_NAME>\" # List of DNS subjectAltNames to be set on the Certificate. dnsNames : - \"<YOUR_DOMAIN_NAME>\" - \"*.<YOUR_DOMAIN_NAME>\" Apply via kubectl: kubectl apply -f docs/03-staging/assets/manifests/cert-manager-wildcard-certificate.yaml To verify the certificate status run: kubectl get certificate <YOUR_DOMAIN_NAME> -n microservices-demo-staging Info This may take a few minutes to complete. If the Certificate object displays a not ready state you can run: kubectl logs -l app=cert-manager,app.kubernetes.io/component=controller -n cert-manager Add the Ingress Nginx host using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The ingress host manifest file looks like the following: Click to expand the ingress host resource apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-microservices-demo-staging namespace : microservices-demo-staging spec : tls : - hosts : - \"*.<YOUR_DOMAIN_NAME>\" secretName : <YOUR_DOMAIN_NAME> rules : - host : <YOUR_A_RECORD>.<YOUR_DOMAIN_NAME> http : paths : - path : / pathType : Prefix backend : service : name : frontend port : number : 80 ingressClassName : nginx Apply via kubectl: kubectl apply -f docs/03-staging/assets/manifests/ingress-host.yaml Open a web browser and point to <YOUR_A_RECORD>.<YOUR_DOMAIN> . You should see the online boutique welcome page. The connection is secure and the certificate is a valid one issued by Let's Encrypt . Next you will install and configure the Prometheus stack for monitoring your DOKS cluster, Loki to fetch and aggregate logs from your cluster's resources and view them in Grafana and configure AlertManager to alert and notify when there is a critical issue in your cluster. You will also configure the events exporter tool to grab Kubernetes events and send and store them in Loki as they are a great way to monitor the health and activity of your K8s clusters.","title":"Set up ingress"},{"location":"03-staging/setup-ingress-staging/#introduction","text":"In this section, you will install and configure the Kubernetes-maintained version of the Nginx Ingress Controller. Then, you're going to issue TLS certificates for your hosts (thus enabling TLS termination), and route traffic to your backend applications.","title":"Introduction"},{"location":"03-staging/setup-ingress-staging/#prerequisites","text":"To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A valid domain available and configured to point to DigitalOcean name servers. More information is available in this article . Digital Ocean is not a domain registrar, so you will need to purchase the domain from a well known vendor, such as GoDaddy.","title":"Prerequisites"},{"location":"03-staging/setup-ingress-staging/#installing-the-nginx-ingress-controller","text":"In this section you will install the community maintained version of the Nginx ingress controller. Please follow below steps to install Nginx using Helm: Add the Ingress Nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update ingress-nginx helm search repo ingress-nginx Install the Nginx Ingress Controller using Helm : helm install ingress-nginx ingress-nginx/ingress-nginx --version 4 .1.3 \\ --namespace ingress-nginx \\ --create-namespace Note To check if the installation was successful, run the helm ls -n ingress-nginx command, and confirm the deployment status. Configure DNS for Nginx Ingress Controller: doctl compute domain create <YOUR_DOMAIN_NAME> Info Please note that this domain matches the domain you purchased in the Prerequisites section. You will use this domain to create additional sub-domains to use with the microservices app you will deploy in this section. You can also use the domain you already created in the Development ingress setup section. Create an A record for your host (make sure to replace the <> placeholders first): doctl compute domain records create <YOUR_DOMAIN_NAME> \\ --record-type \"A\" --record-name <YOUR_RECORD_NAME> \\ --record-data \"<YOUR_STAGING_LB_IP_ADDRESS>\" \\ --record-ttl \"30\" Add the Jetstack Helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install the jetstack/cert-manager chart using Helm : helm install cert-manager jetstack/cert-manager --version 1 .8.0 \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true Note To check if the installation was succesfull you can run the helm ls -n cert-manager and confirm the deployment status. Create a Kubernetes Secret for the DigitalOcean Provider that cert-manager is going to use to perform the DNS-01 challenge using a DigitalOcean API token: DO_API_TOKEN = \"<YOUR_DO_API_TOKEN_HERE>\" kubectl create secret generic \"digitalocean-dns\" \\ --namespace microservices-demo-staging \\ --from-literal = access-token = \" $DO_API_TOKEN \" Note The secret must be created in the same namespace where the Issuer CRD is located - in this case the microservides-demo-staging namespace. Create an issuer resource for cert-manager using kubectl (make sure to replace the <> placeholders first): The issuer manifest file looks like the following: Click to expand issuer manifest file apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-nginx-wcard namespace : microservices-demo-staging spec : # ACME issuer configuration: # `email` - the email address to be associated with the ACME account (make sure it's a valid one). # `server` - the URL used to access the ACME server\u2019s directory endpoint. # `privateKeySecretRef` - Kubernetes Secret to store the automatically generated ACME account private key. acme : email : <YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-nginx-wcard-private # List of challenge solvers that will be used to solve ACME challenges for the matching domains. solvers : # Use the DigitalOcean DNS API to manage DNS01 challenge records. - dns01 : digitalocean : # Kubernetes secret that contains the DO API token . # Must be in the same namespace as the Issuer CRD. tokenSecretRef : name : digitalocean-dns key : access-token Apply via kubectl: kubectl apply -f docs/03-staging/assets/manifests/cert-manager-wildcard-issuer.yaml Info Running kubectl get issuer letsencrypt-nginx-wcard -n microservices-demo-staging should result in the True value being displayed under the READY column. Note If the Issuer object displays a Not Ready state you can describe the object to get additional information using: kubectl describe issuer letsencrypt-nginx-wcard -n microservices-demo-staging to get more information. Create the wildcard certificates resource using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The certificate manifest file looks like the following: Click to expland the certificate resource apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : <YOUR_DOMAIN_NAME> # Cert-Manager will put the resulting Secret in the same Kubernetes namespace as the Certificate. namespace : microservices-demo-staging spec : # Secret name to create, where the private key and certificate should be stored. secretName : <YOUR_DOMAIN_NAME> # What Issuer to use for getting the certificate. issuerRef : name : letsencrypt-nginx-wcard kind : Issuer group : cert-manager.io # Common name to be used on the Certificate. commonName : \"*.<YOUR_DOMAIN_NAME>\" # List of DNS subjectAltNames to be set on the Certificate. dnsNames : - \"<YOUR_DOMAIN_NAME>\" - \"*.<YOUR_DOMAIN_NAME>\" Apply via kubectl: kubectl apply -f docs/03-staging/assets/manifests/cert-manager-wildcard-certificate.yaml To verify the certificate status run: kubectl get certificate <YOUR_DOMAIN_NAME> -n microservices-demo-staging Info This may take a few minutes to complete. If the Certificate object displays a not ready state you can run: kubectl logs -l app=cert-manager,app.kubernetes.io/component=controller -n cert-manager Add the Ingress Nginx host using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The ingress host manifest file looks like the following: Click to expand the ingress host resource apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-microservices-demo-staging namespace : microservices-demo-staging spec : tls : - hosts : - \"*.<YOUR_DOMAIN_NAME>\" secretName : <YOUR_DOMAIN_NAME> rules : - host : <YOUR_A_RECORD>.<YOUR_DOMAIN_NAME> http : paths : - path : / pathType : Prefix backend : service : name : frontend port : number : 80 ingressClassName : nginx Apply via kubectl: kubectl apply -f docs/03-staging/assets/manifests/ingress-host.yaml Open a web browser and point to <YOUR_A_RECORD>.<YOUR_DOMAIN> . You should see the online boutique welcome page. The connection is secure and the certificate is a valid one issued by Let's Encrypt . Next you will install and configure the Prometheus stack for monitoring your DOKS cluster, Loki to fetch and aggregate logs from your cluster's resources and view them in Grafana and configure AlertManager to alert and notify when there is a critical issue in your cluster. You will also configure the events exporter tool to grab Kubernetes events and send and store them in Loki as they are a great way to monitor the health and activity of your K8s clusters.","title":"Installing the Nginx Ingress Controller"},{"location":"04-production/deploying-the-online-boutique-sample-application-production/","text":"Introduction In this section you will learn how to deploy the online boutique sample application using Kustomize . It is tool for customizing Kubernetes configurations. It has the following features to manage application configuration files: generating resources from other sources setting cross-cutting fields for resources composing and customizing collections of resources A more common use case of Kustomize is that you\u2019ll need multiple variants of a common set of resources, e.g., a development , staging and production variant. For this purpose, kustomize supports the idea of an overlay and a base . Both are represented by a kustomization file. The base declares things that the variants share in common (both resources and a common customization of those resources), and the overlays declare the differences. This is well represented in the structure of the online boutique sample application repository structure . Info You will be using the kubectl built-in version of Kustomize . Prerequisites To complete this section you will need: A container registry already set up as explained in the Set up DOCR section. The microservices-demo images build and pushed to DOCR as explained in the Set up DOCR --> Building and pushing docker images to DOCR A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Bootstrap the online boutique application using Kustomize Clone your fork of the kubernetes-sample-apps if you haven't already (make sure to replace the <> placeholders). git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/kubernetes-sample-apps.git Info The kubernetes-sample-apps repository was forked initially in the Setup DOCR section. Change directory to the microservices-demo folder: cd kubernetes-sample-apps/microservices-demo Deploy the Kustomization to your cluster using kubectl : kubectl apply -k kustomize/prod Note To verify that the deployment was succesful run the kubectl get all -n microservices-demo-prod command. The application is deployed to the production environment using the images built and pushed in the Setup DOCR section. Access the web interface by port-forwarding the frontend service: kubectl port-forward service/frontend -n microservices-demo-prod 9090 :80 Open a web browser and point to localhost:9090 . You should see the online boutique welcome page. Note Although you open a connection to localhost in your web browser, traffic is forwarded to the remote production cluster by kubectl . Next, you will deploy and configure the Nginx ingress controller for your production cluster (DOKS) to expose microservices to the outside world. You will also set up cert-manager to automatically issue valid TLS certificates for your applications.","title":"Deploying the online boutique sample application"},{"location":"04-production/deploying-the-online-boutique-sample-application-production/#introduction","text":"In this section you will learn how to deploy the online boutique sample application using Kustomize . It is tool for customizing Kubernetes configurations. It has the following features to manage application configuration files: generating resources from other sources setting cross-cutting fields for resources composing and customizing collections of resources A more common use case of Kustomize is that you\u2019ll need multiple variants of a common set of resources, e.g., a development , staging and production variant. For this purpose, kustomize supports the idea of an overlay and a base . Both are represented by a kustomization file. The base declares things that the variants share in common (both resources and a common customization of those resources), and the overlays declare the differences. This is well represented in the structure of the online boutique sample application repository structure . Info You will be using the kubectl built-in version of Kustomize .","title":"Introduction"},{"location":"04-production/deploying-the-online-boutique-sample-application-production/#prerequisites","text":"To complete this section you will need: A container registry already set up as explained in the Set up DOCR section. The microservices-demo images build and pushed to DOCR as explained in the Set up DOCR --> Building and pushing docker images to DOCR A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section.","title":"Prerequisites"},{"location":"04-production/deploying-the-online-boutique-sample-application-production/#bootstrap-the-online-boutique-application-using-kustomize","text":"Clone your fork of the kubernetes-sample-apps if you haven't already (make sure to replace the <> placeholders). git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/kubernetes-sample-apps.git Info The kubernetes-sample-apps repository was forked initially in the Setup DOCR section. Change directory to the microservices-demo folder: cd kubernetes-sample-apps/microservices-demo Deploy the Kustomization to your cluster using kubectl : kubectl apply -k kustomize/prod Note To verify that the deployment was succesful run the kubectl get all -n microservices-demo-prod command. The application is deployed to the production environment using the images built and pushed in the Setup DOCR section. Access the web interface by port-forwarding the frontend service: kubectl port-forward service/frontend -n microservices-demo-prod 9090 :80 Open a web browser and point to localhost:9090 . You should see the online boutique welcome page. Note Although you open a connection to localhost in your web browser, traffic is forwarded to the remote production cluster by kubectl . Next, you will deploy and configure the Nginx ingress controller for your production cluster (DOKS) to expose microservices to the outside world. You will also set up cert-manager to automatically issue valid TLS certificates for your applications.","title":"Bootstrap the online boutique application using Kustomize"},{"location":"04-production/observability-production/","text":"Introduction The goal of observability is to understand what\u2019s happening across all of your environments and among the technologies, so you can detect and resolve issues to keep your systems efficient and reliable. Observability is a measure of how well the system\u2019s internal states can be inferred from knowledge of its external outputs. It uses the data and insights that monitoring produces to provide a holistic understanding of your system, including its health and performance. The observability of your system, then, depends partly on how well your monitoring metrics can interpret your system's performance indicators. Prerequisites To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A DO Spaces bucket for Loki storage. Please follow the official DigitalOcean tutorial to create one . Make sure that it is set to restrict file listing for security reasons. Installing the Prometheus Monitoring Stack Add the Helm repository and list the available charts: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update prometheus-community Install the kube-prometheus-stack , using Helm : HELM_CHART_VERSION = \"35.5.1\" helm install kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ --create-namespace \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note A specific version for the Helm chart is used. In this case 35.5.1 was picked, which maps to the 0.56.3 version of the application. To check if the installation was successful, run the helm ls -n monitoring command, and confirm the deployment status. Connect to Grafana (using default credentials: admin/prom-operator ) - by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Info You should NOT expose Grafana to public network (eg. you should create an ingress mapping or LB service). Open a web browser and point to localhost:3000 . You should see the Grafna login page. Info Grafana installation comes with a number of dashboards. Open a web browser on localhost:3000 . Once in, you can go to Dashboards -> Browse , and choose different dashboards. As an example, you can open the General / Kubernetes / Compute Resources / Node (Pods) and view the resource metrics for a node and its related pods. Configuring Persistent Storage for Prometheus In this section, you will learn how to enable persistent storage for Prometheus , so that metrics data is persisted across server restarts , or in case of cluster failures . For the production environment, you will define a 10 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the \"docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: prometheusSpec : replicas : 2 retention : 20 storageSpec : volumeClaimTemplate : spec : storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 10Gi Note The default retention time for metrics is set to 10d by default in the kube-prometheus-stack helm chart. In production the retention time will be set to 20d . After 20 days the metrics will be deleted from the Volume . Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel. Configuring Persistent Storage for Grafana In this section, you will learn how to enable persistent storage for Grafana , so that metrics data is persisted across server restarts , or in case of cluster failures . For the production environment, you will define a 10 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: grafana : ... persistence : enabled : true storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] size : 10Gi Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel. Installing the Loki Stack In this section you will learn about Loki , which is a log aggregation system inspired by Prometheus . Loki uses Promtail to fetch logs from all Pods running in your cluster. Then, logs are aggregated and compressed , and sent to the configured storage . Next, you can connect Loki data source to Grafana and view the logs. Add the Grafana Helm repository and list the available charts: helm repo add grafana https://grafana.github.io/helm-charts helm repo update grafana Intall the Loki stack using Helm : HELM_CHART_VERSION = \"2.6.4\" helm install loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ --create-namespace \\ -f \"docs/04-production/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note The above values file, enables Loki and Promtail for you, so no other input is required. Prometheus and Grafana installation is disabled, because Installing the Prometheus Monitoring Stack took care of it already. The 2.6.4 Helm chart version is picked for loki-stack , which maps to application version 2.4.2 . To check if the installation was successful, run the helm ls -n loki-stack command, and confirm the deployment status. Configuring Grafana with Loki In this section, you will add the Loki data source to Grafana . First, you need to expose the Grafana web interface on your local machine (default credentials: admin/prom-operator ): kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Next, open a web browser on localhost:3000 , and follow below steps: Click the Configuration gear from the left panel. Select Data sources . Click the Add data source blue button. Select Loki from the list and add Loki url http://loki.loki-stack:3100 . Save and test. Info If everything goes well, a green label message will appear, saying Data source connected and labels found. You can access logs from the Explore tab of Grafana . Make sure to select Loki as the data source. Use the Help button for log search cheat sheet. !!! info As an example query, to retrieve all the logs for the microservices-demo-prod namespace you can run: {namespace=\"microservices-demo-prod\"} . Configuring Persistent Storage for Loki In this step, you will learn how to enable persistent storage for Loki . You're going to use the DO Spaces bucket created in the Prerequisites section. Open the docs/04-production/assets/manifests/loki-stack-values-v35.5.1.yaml file provided and remove the comments surrounding the schema_config and storage_config keys. The definition should look like this: Click to expand the loki config loki : enabled : true config : schema_config : configs : - from : \"2020-10-24\" store : boltdb-shipper object_store : aws schema : v11 index : prefix : index_ period : 24h storage_config : boltdb_shipper : active_index_directory : /data/loki/boltdb-shipper-active cache_location : /data/loki/boltdb-shipper-cache cache_ttl : 24h shared_store : aws aws : bucketnames : <YOUR_DO_SPACES_BUCKET_NAME_HERE> endpoint : <YOUR_DO_SPACES_BUCKET_ENDPOINT_HERE> # in the following format: <region>.digitaloceanspaces.com region : <YOUR_DO_SPACES_BUCKET_REGION_HERE> # short region name (e.g.: fra1) access_key_id : <YOUR_DO_SPACES_ACCESS_KEY_HERE> secret_access_key : <YOURDO_SPACES_SECRET_KEY_HERE> s3forcepathstyle : true Apply the new settings using Helm : HELM_CHART_VERSION = \"2.6.4\" helm upgrade loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ -f \"docs/04-production/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check if the main Loki application pod is up and running by runnging the following kubectl command: kubectl get pods -n loki-stack -l app=loki . If everything goes well, you should see the DO Spaces bucket containing the index and chunks folders (the chunks folder is called fake , which is a strange name - this is by design, when not running in multi-tenant mode). Setting up a retention policy In this step you will set up a retention policy for your DO Spaces bucket. S3CMD is a good utility to have in order to set up retention policies. Please follow the DigitalOcean guide for installing and setting up s3cmd . Configure the Loki bucket lifecycle, using s3cmd : Click to expand the Loki bucket lifecycle <LifecycleConfiguration xmlns= \"http://s3.amazonaws.com/doc/2006-03-01/\" > <Rule> <ID> Expire old fake data </ID> <Prefix> fake/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> <Rule> <ID> Expire old index data </ID> <Prefix> index/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> </LifecycleConfiguration> s3cmd setlifecycle 04 -setup-observability/assets/manifests/loki_do_spaces_lifecycle.xml s3://<LOKI_STORAGE_BUCKET_NAME> Check that the policy was set (please replace the <> placeholders accordingly): s3cmd getlifecycle s3://<LOKI_STORAGE_BUCKET_NAME> Note The DO Spaces backend implementation will clean the objects for you automatically , based on the expiration date. You can always go back and edit the policy if needed later on, by uploading a new one. Setting up Alert Manager Alertmanager is deployed alongside Prometheus and forms the alerting layer of the kube-prom-stack . It handles alerts generated by Prometheus by deduplicating, grouping, and routing them to various integrations such as email, Slack or PagerDuty. Alerts and notifications are a critical part of your workflow. When things go wrong (e.g. any service is down, or a pod is crashing, etc.), you will want to get notifications in real time to handle critical situations as soon as possible. To create a new alert, you need to add a new definition in the additionalPrometheusRule s section from the kube-prom-stack Helm values file. You will be creating a sample alert that will trigger if the microservices-demo-prod namespace does not have an expected number of instances. The expected number of pods for the online boutique application is 10. Open the docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the additionalPrometheusRulesMap block. The definition should look like this: additionalPrometheusRulesMap : rule-name : groups : - name : online-boutique-instance-down rules : - alert : OnlineBoutiqueInstanceDown expr : sum(kube_pod_owner{namespace=\"microservices-demo-prod\"}) by (namespace) < 10 for : 1m labels : severity : 'critical' annotations : description : ' The Number of pods from the namespace {{ $labels.namespace }} is lower than the expected 10. ' summary : 'Pod {{ $labels.pod }} down' Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Info To check that the alert has been created successfully, first port-forward to your local machine by running this command: kubectl --namespace monitoring port-forward service/kube-prom-stack-kube-prome-prometheus 9090:9090 . Navigate to the Promethes Console click on the Alerts menu item and identify the OnlineBoutiqueInstanceDown alert. It should be visible at the bottom of the list. Configuring Alertmanager to Send Notifications to Slack To complete this section you need to have administrative rights over a workspace. This will enable you to create the incoming webhook you will need in the next steps. You will also need to create a channel where you would like to receive notifications from AlertManager . Steps to follow: Open a web browser and navigate to https://api.slack.com/apps and click on the Create New App button. In the Create an app window select the From scratch option. Then, give your application a name and select the appropriate workspace. From the Basic Information page click on the Incoming Webhooks option, turn it on and click on the Add New Webhook to Workspace button at the bottom. On the next page, use the Search for a channel... drop-down list to select the desired channel where you want to send notifications. When ready, click on the Allow button. Take note of the Webhook URL value displayed on the page. You will be using it in the next section. Next you will tell Alertmanager how to send Slack notifications. Open the docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the alertmaanager.config block. Make sure to update the <> placeholders accordingly. The definition should look like: Click to expand the alertmanager config alertmanager : enabled : true config : global : resolve_timeout : 5m slack_api_url : \"<YOUR_SLACK_APP_INCOMING_WEBHOOK_URL_HERE>\" route : receiver : \"null\" repeat_interval : 12h routes : - receiver : \"slack-notifications\" matchers : - alertname=\"OnlineBoutiqueInstanceDown\" continue : false receivers : - name : \"null\" - name : \"slack-notifications\" slack_configs : - channel : \"#<YOUR_SLACK_CHANNEL_NAME_HERE>\" send_resolved : true title : \"{{ range .Alerts }}{{ .Annotations.summary }}\\n{{ end }}\" text : \"{{ range .Alerts }}{{ .Annotations.description }}\\n{{ end }}\" Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" !!! info At this point you should only receieve alerts from the matching OnlinqBoutiqueInstanceDown alertname. Since the continue is set to false Alertmanager will only send notifications from this alert and stop sending for others. Clicking on the notification name in Slack will open a web browser to an unreachable web page with the internal Kubernetes DNS of the Alertmanager pod. This is expected. For more information you can check out this article . For additional information about the configuration parameters for Alertmanager you can check out this doc . You can also at some notification examples in this article . Setting up Event Exporter for events retention A Kubernetes event is an object that shows what\u2019s happening inside a cluster, node, pod, or container. These objects are usually generated in response to changes that occur inside your K8s system. The Kubernetes API Server enables all core components to create these events. Generally, each event is accompanied by a log message as well. Event objects are not regular log events, therefore the Kubernetes logs do not include them. Kubernetes has no builtin support to store or forward these events on the long term, and they are cleaned up after a short retention time defaulting to just 1 hour. In this section you will learn how to configure the Kubernetes Events Exporter and collect and perist those events using Loki . Create the ServiceAccount , ClusterRole and ClusterRoleBinding using kubectl : The manifest file looks like the following: Click to expand the manifest file apiVersion : v1 kind : Namespace metadata : name : event-exporter --- apiVersion : v1 kind : ServiceAccount metadata : namespace : event-exporter name : event-exporter --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : event-exporter roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : view subjects : - kind : ServiceAccount namespace : event-exporter name : event-exporter kubectl apply -f docs/04-production/assets/manifests/event-exporter-roles.yaml Create the event exporter config using kubectl : The manifest file for the config looks like the following: Click to expand the config manifest file apiVersion : v1 kind : ConfigMap metadata : name : event-exporter-cfg namespace : event-exporter data : config.yaml : | logLevel : error logFormat : json route : routes : - match : - receiver : \"dump\" receivers : - name : \"dump\" stdout : {} kubectl apply -f docs/04-production/assets/manifests/event-exporter-config.yaml Finally, create the event exporter deployment using kubectl: The manifest file for the deployment looks like the following: Click to expand the deployment manifest file apiVersion : apps/v1 kind : Deployment metadata : name : event-exporter namespace : event-exporter spec : replicas : 1 template : metadata : labels : app : event-exporter version : v1 spec : serviceAccountName : event-exporter containers : - name : event-exporter image : ghcr.io/resmoio/kubernetes-event-exporter:latest imagePullPolicy : IfNotPresent args : - -conf=/data/config.yaml volumeMounts : - mountPath : /data name : cfg volumes : - name : cfg configMap : name : event-exporter-cfg selector : matchLabels : app : event-exporter version : v1 kubectl apply -f docs/04-production/assets/manifests/event-exporter-deployment.yaml Viewing events in Grafana If the installation went well and no errors were reported you should start seeing event start to flow in Grafana. Connect to Grafana (using default credentials: admin/prom-operator ) by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Open a web browser on localhost:3000 . Once in, you can go to Explore menu and select the Loki as a datasource. In the Log browser input enter the following: { app= \"event-exporter\" } Info You should see events in the Logs section. Any of the fields in the Detected fields section of a log detail view can be used to query. For example you can perform a query using the pod name and view specific logs for a certain pod: {app=\"event-exporter\"} |= \"shippingservice-79bdd5f858-gqm6g\" . For a more in depth explanation on the Obervability topic you can check out the Kubernetes Starter Kit Tutorial . Next, you will learn how to configure the CI/CD process and associated GitHub workflows for all project components used in this guide.","title":"Observability"},{"location":"04-production/observability-production/#introduction","text":"The goal of observability is to understand what\u2019s happening across all of your environments and among the technologies, so you can detect and resolve issues to keep your systems efficient and reliable. Observability is a measure of how well the system\u2019s internal states can be inferred from knowledge of its external outputs. It uses the data and insights that monitoring produces to provide a holistic understanding of your system, including its health and performance. The observability of your system, then, depends partly on how well your monitoring metrics can interpret your system's performance indicators.","title":"Introduction"},{"location":"04-production/observability-production/#prerequisites","text":"To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A DO Spaces bucket for Loki storage. Please follow the official DigitalOcean tutorial to create one . Make sure that it is set to restrict file listing for security reasons.","title":"Prerequisites"},{"location":"04-production/observability-production/#installing-the-prometheus-monitoring-stack","text":"Add the Helm repository and list the available charts: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update prometheus-community Install the kube-prometheus-stack , using Helm : HELM_CHART_VERSION = \"35.5.1\" helm install kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ --create-namespace \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note A specific version for the Helm chart is used. In this case 35.5.1 was picked, which maps to the 0.56.3 version of the application. To check if the installation was successful, run the helm ls -n monitoring command, and confirm the deployment status. Connect to Grafana (using default credentials: admin/prom-operator ) - by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Info You should NOT expose Grafana to public network (eg. you should create an ingress mapping or LB service). Open a web browser and point to localhost:3000 . You should see the Grafna login page. Info Grafana installation comes with a number of dashboards. Open a web browser on localhost:3000 . Once in, you can go to Dashboards -> Browse , and choose different dashboards. As an example, you can open the General / Kubernetes / Compute Resources / Node (Pods) and view the resource metrics for a node and its related pods.","title":"Installing the Prometheus Monitoring Stack"},{"location":"04-production/observability-production/#configuring-persistent-storage-for-prometheus","text":"In this section, you will learn how to enable persistent storage for Prometheus , so that metrics data is persisted across server restarts , or in case of cluster failures . For the production environment, you will define a 10 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the \"docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: prometheusSpec : replicas : 2 retention : 20 storageSpec : volumeClaimTemplate : spec : storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 10Gi Note The default retention time for metrics is set to 10d by default in the kube-prometheus-stack helm chart. In production the retention time will be set to 20d . After 20 days the metrics will be deleted from the Volume . Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel.","title":"Configuring Persistent Storage for Prometheus"},{"location":"04-production/observability-production/#configuring-persistent-storage-for-grafana","text":"In this section, you will learn how to enable persistent storage for Grafana , so that metrics data is persisted across server restarts , or in case of cluster failures . For the production environment, you will define a 10 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: grafana : ... persistence : enabled : true storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] size : 10Gi Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel.","title":"Configuring Persistent Storage for Grafana"},{"location":"04-production/observability-production/#installing-the-loki-stack","text":"In this section you will learn about Loki , which is a log aggregation system inspired by Prometheus . Loki uses Promtail to fetch logs from all Pods running in your cluster. Then, logs are aggregated and compressed , and sent to the configured storage . Next, you can connect Loki data source to Grafana and view the logs. Add the Grafana Helm repository and list the available charts: helm repo add grafana https://grafana.github.io/helm-charts helm repo update grafana Intall the Loki stack using Helm : HELM_CHART_VERSION = \"2.6.4\" helm install loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ --create-namespace \\ -f \"docs/04-production/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note The above values file, enables Loki and Promtail for you, so no other input is required. Prometheus and Grafana installation is disabled, because Installing the Prometheus Monitoring Stack took care of it already. The 2.6.4 Helm chart version is picked for loki-stack , which maps to application version 2.4.2 . To check if the installation was successful, run the helm ls -n loki-stack command, and confirm the deployment status.","title":"Installing the Loki Stack"},{"location":"04-production/observability-production/#configuring-grafana-with-loki","text":"In this section, you will add the Loki data source to Grafana . First, you need to expose the Grafana web interface on your local machine (default credentials: admin/prom-operator ): kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Next, open a web browser on localhost:3000 , and follow below steps: Click the Configuration gear from the left panel. Select Data sources . Click the Add data source blue button. Select Loki from the list and add Loki url http://loki.loki-stack:3100 . Save and test. Info If everything goes well, a green label message will appear, saying Data source connected and labels found. You can access logs from the Explore tab of Grafana . Make sure to select Loki as the data source. Use the Help button for log search cheat sheet. !!! info As an example query, to retrieve all the logs for the microservices-demo-prod namespace you can run: {namespace=\"microservices-demo-prod\"} .","title":"Configuring Grafana with Loki"},{"location":"04-production/observability-production/#configuring-persistent-storage-for-loki","text":"In this step, you will learn how to enable persistent storage for Loki . You're going to use the DO Spaces bucket created in the Prerequisites section. Open the docs/04-production/assets/manifests/loki-stack-values-v35.5.1.yaml file provided and remove the comments surrounding the schema_config and storage_config keys. The definition should look like this: Click to expand the loki config loki : enabled : true config : schema_config : configs : - from : \"2020-10-24\" store : boltdb-shipper object_store : aws schema : v11 index : prefix : index_ period : 24h storage_config : boltdb_shipper : active_index_directory : /data/loki/boltdb-shipper-active cache_location : /data/loki/boltdb-shipper-cache cache_ttl : 24h shared_store : aws aws : bucketnames : <YOUR_DO_SPACES_BUCKET_NAME_HERE> endpoint : <YOUR_DO_SPACES_BUCKET_ENDPOINT_HERE> # in the following format: <region>.digitaloceanspaces.com region : <YOUR_DO_SPACES_BUCKET_REGION_HERE> # short region name (e.g.: fra1) access_key_id : <YOUR_DO_SPACES_ACCESS_KEY_HERE> secret_access_key : <YOURDO_SPACES_SECRET_KEY_HERE> s3forcepathstyle : true Apply the new settings using Helm : HELM_CHART_VERSION = \"2.6.4\" helm upgrade loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ -f \"docs/04-production/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check if the main Loki application pod is up and running by runnging the following kubectl command: kubectl get pods -n loki-stack -l app=loki . If everything goes well, you should see the DO Spaces bucket containing the index and chunks folders (the chunks folder is called fake , which is a strange name - this is by design, when not running in multi-tenant mode).","title":"Configuring Persistent Storage for Loki"},{"location":"04-production/observability-production/#setting-up-a-retention-policy","text":"In this step you will set up a retention policy for your DO Spaces bucket. S3CMD is a good utility to have in order to set up retention policies. Please follow the DigitalOcean guide for installing and setting up s3cmd . Configure the Loki bucket lifecycle, using s3cmd : Click to expand the Loki bucket lifecycle <LifecycleConfiguration xmlns= \"http://s3.amazonaws.com/doc/2006-03-01/\" > <Rule> <ID> Expire old fake data </ID> <Prefix> fake/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> <Rule> <ID> Expire old index data </ID> <Prefix> index/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> </LifecycleConfiguration> s3cmd setlifecycle 04 -setup-observability/assets/manifests/loki_do_spaces_lifecycle.xml s3://<LOKI_STORAGE_BUCKET_NAME> Check that the policy was set (please replace the <> placeholders accordingly): s3cmd getlifecycle s3://<LOKI_STORAGE_BUCKET_NAME> Note The DO Spaces backend implementation will clean the objects for you automatically , based on the expiration date. You can always go back and edit the policy if needed later on, by uploading a new one.","title":"Setting up a retention policy"},{"location":"04-production/observability-production/#setting-up-alert-manager","text":"Alertmanager is deployed alongside Prometheus and forms the alerting layer of the kube-prom-stack . It handles alerts generated by Prometheus by deduplicating, grouping, and routing them to various integrations such as email, Slack or PagerDuty. Alerts and notifications are a critical part of your workflow. When things go wrong (e.g. any service is down, or a pod is crashing, etc.), you will want to get notifications in real time to handle critical situations as soon as possible. To create a new alert, you need to add a new definition in the additionalPrometheusRule s section from the kube-prom-stack Helm values file. You will be creating a sample alert that will trigger if the microservices-demo-prod namespace does not have an expected number of instances. The expected number of pods for the online boutique application is 10. Open the docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the additionalPrometheusRulesMap block. The definition should look like this: additionalPrometheusRulesMap : rule-name : groups : - name : online-boutique-instance-down rules : - alert : OnlineBoutiqueInstanceDown expr : sum(kube_pod_owner{namespace=\"microservices-demo-prod\"}) by (namespace) < 10 for : 1m labels : severity : 'critical' annotations : description : ' The Number of pods from the namespace {{ $labels.namespace }} is lower than the expected 10. ' summary : 'Pod {{ $labels.pod }} down' Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Info To check that the alert has been created successfully, first port-forward to your local machine by running this command: kubectl --namespace monitoring port-forward service/kube-prom-stack-kube-prome-prometheus 9090:9090 . Navigate to the Promethes Console click on the Alerts menu item and identify the OnlineBoutiqueInstanceDown alert. It should be visible at the bottom of the list.","title":"Setting up Alert Manager"},{"location":"04-production/observability-production/#configuring-alertmanager-to-send-notifications-to-slack","text":"To complete this section you need to have administrative rights over a workspace. This will enable you to create the incoming webhook you will need in the next steps. You will also need to create a channel where you would like to receive notifications from AlertManager . Steps to follow: Open a web browser and navigate to https://api.slack.com/apps and click on the Create New App button. In the Create an app window select the From scratch option. Then, give your application a name and select the appropriate workspace. From the Basic Information page click on the Incoming Webhooks option, turn it on and click on the Add New Webhook to Workspace button at the bottom. On the next page, use the Search for a channel... drop-down list to select the desired channel where you want to send notifications. When ready, click on the Allow button. Take note of the Webhook URL value displayed on the page. You will be using it in the next section. Next you will tell Alertmanager how to send Slack notifications. Open the docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the alertmaanager.config block. Make sure to update the <> placeholders accordingly. The definition should look like: Click to expand the alertmanager config alertmanager : enabled : true config : global : resolve_timeout : 5m slack_api_url : \"<YOUR_SLACK_APP_INCOMING_WEBHOOK_URL_HERE>\" route : receiver : \"null\" repeat_interval : 12h routes : - receiver : \"slack-notifications\" matchers : - alertname=\"OnlineBoutiqueInstanceDown\" continue : false receivers : - name : \"null\" - name : \"slack-notifications\" slack_configs : - channel : \"#<YOUR_SLACK_CHANNEL_NAME_HERE>\" send_resolved : true title : \"{{ range .Alerts }}{{ .Annotations.summary }}\\n{{ end }}\" text : \"{{ range .Alerts }}{{ .Annotations.description }}\\n{{ end }}\" Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" !!! info At this point you should only receieve alerts from the matching OnlinqBoutiqueInstanceDown alertname. Since the continue is set to false Alertmanager will only send notifications from this alert and stop sending for others. Clicking on the notification name in Slack will open a web browser to an unreachable web page with the internal Kubernetes DNS of the Alertmanager pod. This is expected. For more information you can check out this article . For additional information about the configuration parameters for Alertmanager you can check out this doc . You can also at some notification examples in this article .","title":"Configuring Alertmanager to Send Notifications to Slack"},{"location":"04-production/observability-production/#setting-up-event-exporter-for-events-retention","text":"A Kubernetes event is an object that shows what\u2019s happening inside a cluster, node, pod, or container. These objects are usually generated in response to changes that occur inside your K8s system. The Kubernetes API Server enables all core components to create these events. Generally, each event is accompanied by a log message as well. Event objects are not regular log events, therefore the Kubernetes logs do not include them. Kubernetes has no builtin support to store or forward these events on the long term, and they are cleaned up after a short retention time defaulting to just 1 hour. In this section you will learn how to configure the Kubernetes Events Exporter and collect and perist those events using Loki . Create the ServiceAccount , ClusterRole and ClusterRoleBinding using kubectl : The manifest file looks like the following: Click to expand the manifest file apiVersion : v1 kind : Namespace metadata : name : event-exporter --- apiVersion : v1 kind : ServiceAccount metadata : namespace : event-exporter name : event-exporter --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : event-exporter roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : view subjects : - kind : ServiceAccount namespace : event-exporter name : event-exporter kubectl apply -f docs/04-production/assets/manifests/event-exporter-roles.yaml Create the event exporter config using kubectl : The manifest file for the config looks like the following: Click to expand the config manifest file apiVersion : v1 kind : ConfigMap metadata : name : event-exporter-cfg namespace : event-exporter data : config.yaml : | logLevel : error logFormat : json route : routes : - match : - receiver : \"dump\" receivers : - name : \"dump\" stdout : {} kubectl apply -f docs/04-production/assets/manifests/event-exporter-config.yaml Finally, create the event exporter deployment using kubectl: The manifest file for the deployment looks like the following: Click to expand the deployment manifest file apiVersion : apps/v1 kind : Deployment metadata : name : event-exporter namespace : event-exporter spec : replicas : 1 template : metadata : labels : app : event-exporter version : v1 spec : serviceAccountName : event-exporter containers : - name : event-exporter image : ghcr.io/resmoio/kubernetes-event-exporter:latest imagePullPolicy : IfNotPresent args : - -conf=/data/config.yaml volumeMounts : - mountPath : /data name : cfg volumes : - name : cfg configMap : name : event-exporter-cfg selector : matchLabels : app : event-exporter version : v1 kubectl apply -f docs/04-production/assets/manifests/event-exporter-deployment.yaml","title":"Setting up Event Exporter for events retention"},{"location":"04-production/observability-production/#viewing-events-in-grafana","text":"If the installation went well and no errors were reported you should start seeing event start to flow in Grafana. Connect to Grafana (using default credentials: admin/prom-operator ) by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Open a web browser on localhost:3000 . Once in, you can go to Explore menu and select the Loki as a datasource. In the Log browser input enter the following: { app= \"event-exporter\" } Info You should see events in the Logs section. Any of the fields in the Detected fields section of a log detail view can be used to query. For example you can perform a query using the pod name and view specific logs for a certain pod: {app=\"event-exporter\"} |= \"shippingservice-79bdd5f858-gqm6g\" . For a more in depth explanation on the Obervability topic you can check out the Kubernetes Starter Kit Tutorial . Next, you will learn how to configure the CI/CD process and associated GitHub workflows for all project components used in this guide.","title":"Viewing events in Grafana"},{"location":"04-production/setup-doks-production/","text":"Introduction This section will show you how to create a DigitalOcean Kubernetes Cluster ( DOKS ) cluster which will be used as the production environment, targeting the online boutique sample application used as a reference in this guide. A Kubernetes environment is referred to as production-ready when it has everything needed to serve traffic to real end users and has the resources to adapt to changing demands. A production environment should be secure, scalable, highly available and reliable, and must provide logging and monitoring capabilities that meet organizational requirements. Prerequisites To complete this section you will need: Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Make sure that you're authenticated with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API section. Provisioning a Production DOKS Cluster for Microservices In this step, you will create a new Kubernetes cluster running on the DigitalOcean platform, using the doctl utility. Following command will create a DigitalOcean Kubernetes cluster named microservices-demo-production , with a pool size of 4 nodes , auto-scale to 3-5 each having 2 vCPUs and 4gbGB of RAM, in the nyc1 region: doctl k8s cluster create microservices-demo-production \\ --auto-upgrade = true \\ --maintenance-window \"saturday=21:00\" \\ --node-pool \"name=basicnp;size=s-2vcpu-4gb-amd;count=4;tag=cluster2;label=type=basic;auto-scale=true;min-nodes=3;max-nodes=5\" \\ --region nyc1 Notes The example cluster created above is using 4 nodes, each having 2vCPU/4GB size, which amounts to 94$/month . For simplicity and consistency through all the guide, the microservices-demo-production name was picked for the example cluster. You can choose any name you like, but you need to make sure the naming convention stays consistent. It is recommended to use a region for your cluster that is closest to you for faster interaction. Run the following command - doctl k8s options regions to check available regions. Cluster auto upgrade is enabled ( --auto-upgrade=true ). Kubernetes clusters should be auto-upgraded to ensure that they always contain the latest security patches. Next, you can verify the cluster details. First, fetch your DOKS cluster ID : doctl k8s cluster list Finally, check if the kubectl context was set to point to your DOKS cluster. The doctl utility should do this automatically: kubectl config current-context For more info on this topic please see this Kubernetes Starter Kit DOKS Creation . Configuring DOKS for Private Registries From the command line run the following: doctl registry kubernetes-manifest | kubectl apply -f - This will configure your DOKS cluster to fetch images from your DOCR created in the Set up a DigitalOcean container registry section This step can also be achieved via the DigitalOcean cloud console. Please follow this guide . Next, you will learn how to deploy the online boutique sample application to your production cluster using Kustomize .","title":"Set up Production DOKS"},{"location":"04-production/setup-doks-production/#introduction","text":"This section will show you how to create a DigitalOcean Kubernetes Cluster ( DOKS ) cluster which will be used as the production environment, targeting the online boutique sample application used as a reference in this guide. A Kubernetes environment is referred to as production-ready when it has everything needed to serve traffic to real end users and has the resources to adapt to changing demands. A production environment should be secure, scalable, highly available and reliable, and must provide logging and monitoring capabilities that meet organizational requirements.","title":"Introduction"},{"location":"04-production/setup-doks-production/#prerequisites","text":"To complete this section you will need: Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Make sure that you're authenticated with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API section.","title":"Prerequisites"},{"location":"04-production/setup-doks-production/#provisioning-a-production-doks-cluster-for-microservices","text":"In this step, you will create a new Kubernetes cluster running on the DigitalOcean platform, using the doctl utility. Following command will create a DigitalOcean Kubernetes cluster named microservices-demo-production , with a pool size of 4 nodes , auto-scale to 3-5 each having 2 vCPUs and 4gbGB of RAM, in the nyc1 region: doctl k8s cluster create microservices-demo-production \\ --auto-upgrade = true \\ --maintenance-window \"saturday=21:00\" \\ --node-pool \"name=basicnp;size=s-2vcpu-4gb-amd;count=4;tag=cluster2;label=type=basic;auto-scale=true;min-nodes=3;max-nodes=5\" \\ --region nyc1 Notes The example cluster created above is using 4 nodes, each having 2vCPU/4GB size, which amounts to 94$/month . For simplicity and consistency through all the guide, the microservices-demo-production name was picked for the example cluster. You can choose any name you like, but you need to make sure the naming convention stays consistent. It is recommended to use a region for your cluster that is closest to you for faster interaction. Run the following command - doctl k8s options regions to check available regions. Cluster auto upgrade is enabled ( --auto-upgrade=true ). Kubernetes clusters should be auto-upgraded to ensure that they always contain the latest security patches. Next, you can verify the cluster details. First, fetch your DOKS cluster ID : doctl k8s cluster list Finally, check if the kubectl context was set to point to your DOKS cluster. The doctl utility should do this automatically: kubectl config current-context For more info on this topic please see this Kubernetes Starter Kit DOKS Creation .","title":"Provisioning a Production DOKS Cluster for Microservices"},{"location":"04-production/setup-doks-production/#configuring-doks-for-private-registries","text":"From the command line run the following: doctl registry kubernetes-manifest | kubectl apply -f - This will configure your DOKS cluster to fetch images from your DOCR created in the Set up a DigitalOcean container registry section This step can also be achieved via the DigitalOcean cloud console. Please follow this guide . Next, you will learn how to deploy the online boutique sample application to your production cluster using Kustomize .","title":"Configuring DOKS for Private Registries"},{"location":"04-production/setup-ingress-production/","text":"Introduction In this section, you will install and configure the Kubernetes-maintained version of the Nginx Ingress Controller. Then, you're going to issue TLS certificates for your hosts (thus enabling TLS termination), and route traffic to your backend applications. Prerequisites To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A valid domain available and configured to point to DigitalOcean name servers. More information is available in this article . Digital Ocean is not a domain registrar, so you will need to purchase the domain from a well known vendor, such as GoDaddy. Installing the Nginx Ingress Controller In this section you will install the community maintained version of the Nginx ingress controller. Please follow below steps to install Nginx using Helm: Add the Ingress Nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update ingress-nginx helm search repo ingress-nginx Install the Nginx Ingress Controller using Helm : helm install ingress-nginx ingress-nginx/ingress-nginx --version 4 .1.3 \\ --namespace ingress-nginx \\ --create-namespace Note To check if the installation was successful, run the helm ls -n ingress-nginx command, and confirm the deployment status. Configure DNS for Nginx Ingress Controller: doctl compute domain create <YOUR_DOMAIN_NAME> Info Please note that this domain matches the domain you purchased in the Prerequisites section. You will use this domain to create additional sub-domains to use with the microservices app you will deploy in this section. You can also use the domain you already created in the Development ingress setup section. Create an A record for your host (make sure to replace the <> placeholders first): doctl compute domain records create <YOUR_DOMAIN_NAME> \\ --record-type \"A\" --record-name <YOUR_RECORD_NAME> \\ --record-data \"<YOUR_PRODUCTION_LB_IP_ADDRESS>\" \\ --record-ttl \"30\" Add the Jetstack Helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install the jetstack/cert-manager chart using Helm : helm install cert-manager jetstack/cert-manager --version 1 .8.0 \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true Note To check if the installation was succesfull you can run the helm ls -n cert-manager and confirm the deployment status. Create a Kubernetes Secret for the DigitalOcean Provider that cert-manager is going to use to perform the DNS-01 challenge using a DigitalOcean API token: DO_API_TOKEN = \"<YOUR_DO_API_TOKEN_HERE>\" kubectl create secret generic \"digitalocean-dns\" \\ --namespace microservices-demo-prod \\ --from-literal = access-token = \" $DO_API_TOKEN \" Note The secret must be created in the same namespace where the Issuer CRD is located - in this case the microservides-demo-prod namespace. Create an issuer resource for cert-manager using kubectl (make sure to replace the <> placeholders first): The issuer manifest file looks like the following: Click to expand issuer manifest file apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-nginx-wcard namespace : microservices-demo-prod spec : # ACME issuer configuration: # `email` - the email address to be associated with the ACME account (make sure it's a valid one). # `server` - the URL used to access the ACME server\u2019s directory endpoint. # `privateKeySecretRef` - Kubernetes Secret to store the automatically generated ACME account private key. acme : email : <YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-nginx-wcard-private # List of challenge solvers that will be used to solve ACME challenges for the matching domains. solvers : # Use the DigitalOcean DNS API to manage DNS01 challenge records. - dns01 : digitalocean : # Kubernetes secret that contains the DO API token . # Must be in the same namespace as the Issuer CRD. tokenSecretRef : name : digitalocean-dns key : access-token Apply via kubectl: kubectl apply -f docs/04-production/assets/manifests/cert-manager-wildcard-issuer.yaml Info Running kubectl get issuer letsencrypt-nginx-wcard -n microservices-demo-prod should result in the True value being displayed under the READY column. Note If the Issuer object displays a Not Ready state you can describe the object to get additional information using: kubectl describe issuer letsencrypt-nginx-wcard -n microservices-demo-prod to get more information. Create the wildcard certificates resource using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The certificate manifest file looks like the following: Click to expland the certificate resource apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : <YOUR_DOMAIN_NAME> # Cert-Manager will put the resulting Secret in the same Kubernetes namespace as the Certificate. namespace : microservices-demo-prod spec : # Secret name to create, where the private key and certificate should be stored. secretName : <YOUR_DOMAIN_NAME> # What Issuer to use for getting the certificate. issuerRef : name : letsencrypt-nginx-wcard kind : Issuer group : cert-manager.io # Common name to be used on the Certificate. commonName : \"*.<YOUR_DOMAIN_NAME>\" # List of DNS subjectAltNames to be set on the Certificate. dnsNames : - \"<YOUR_DOMAIN_NAME>\" - \"*.<YOUR_DOMAIN_NAME>\" Apply via kubectl: kubectl apply -f docs/04-production/assets/manifests/cert-manager-wildcard-certificate.yaml To verify the certificate status run: kubectl get certificate <YOUR_DOMAIN_NAME> -n microservices-demo-prod Info This may take a few minutes to complete. If the Certificate object displays a not ready state you can run: kubectl logs -l app=cert-manager,app.kubernetes.io/component=controller -n cert-manager Add the Ingress Nginx host using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The ingress host manifest file looks like the following: Click to expand the ingress host resource apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-microservices-demo-prod namespace : microservices-demo-prod spec : tls : - hosts : - \"*.<YOUR_DOMAIN_NAME>\" secretName : <YOUR_DOMAIN_NAME> rules : - host : <YOUR_A_RECORD>.<YOUR_DOMAIN_NAME> http : paths : - path : / pathType : Prefix backend : service : name : frontend port : number : 80 ingressClassName : nginx Apply via kubectl: kubectl apply -f docs/04-production/assets/manifests/ingress-host.yaml Open a web browser and point to <YOUR_A_RECORD>.<YOUR_DOMAIN> . You should see the online boutique welcome page. The connection is secure and the certificate is a valid one issued by Let's Encrypt . Next you will install and configure the Prometheus stack for monitoring your DOKS cluster, Loki to fetch and aggregate logs from your cluster's resources and view them in Grafana and configure AlertManager to alert and notify when there is a critical issue in your cluster. You will also configure the events exporter tool to grab Kubernetes events and send and store them in Loki as they are a great way to monitor the health and activity of your K8s clusters.","title":"Set up ingress"},{"location":"04-production/setup-ingress-production/#introduction","text":"In this section, you will install and configure the Kubernetes-maintained version of the Nginx Ingress Controller. Then, you're going to issue TLS certificates for your hosts (thus enabling TLS termination), and route traffic to your backend applications.","title":"Introduction"},{"location":"04-production/setup-ingress-production/#prerequisites","text":"To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A valid domain available and configured to point to DigitalOcean name servers. More information is available in this article . Digital Ocean is not a domain registrar, so you will need to purchase the domain from a well known vendor, such as GoDaddy.","title":"Prerequisites"},{"location":"04-production/setup-ingress-production/#installing-the-nginx-ingress-controller","text":"In this section you will install the community maintained version of the Nginx ingress controller. Please follow below steps to install Nginx using Helm: Add the Ingress Nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update ingress-nginx helm search repo ingress-nginx Install the Nginx Ingress Controller using Helm : helm install ingress-nginx ingress-nginx/ingress-nginx --version 4 .1.3 \\ --namespace ingress-nginx \\ --create-namespace Note To check if the installation was successful, run the helm ls -n ingress-nginx command, and confirm the deployment status. Configure DNS for Nginx Ingress Controller: doctl compute domain create <YOUR_DOMAIN_NAME> Info Please note that this domain matches the domain you purchased in the Prerequisites section. You will use this domain to create additional sub-domains to use with the microservices app you will deploy in this section. You can also use the domain you already created in the Development ingress setup section. Create an A record for your host (make sure to replace the <> placeholders first): doctl compute domain records create <YOUR_DOMAIN_NAME> \\ --record-type \"A\" --record-name <YOUR_RECORD_NAME> \\ --record-data \"<YOUR_PRODUCTION_LB_IP_ADDRESS>\" \\ --record-ttl \"30\" Add the Jetstack Helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install the jetstack/cert-manager chart using Helm : helm install cert-manager jetstack/cert-manager --version 1 .8.0 \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true Note To check if the installation was succesfull you can run the helm ls -n cert-manager and confirm the deployment status. Create a Kubernetes Secret for the DigitalOcean Provider that cert-manager is going to use to perform the DNS-01 challenge using a DigitalOcean API token: DO_API_TOKEN = \"<YOUR_DO_API_TOKEN_HERE>\" kubectl create secret generic \"digitalocean-dns\" \\ --namespace microservices-demo-prod \\ --from-literal = access-token = \" $DO_API_TOKEN \" Note The secret must be created in the same namespace where the Issuer CRD is located - in this case the microservides-demo-prod namespace. Create an issuer resource for cert-manager using kubectl (make sure to replace the <> placeholders first): The issuer manifest file looks like the following: Click to expand issuer manifest file apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-nginx-wcard namespace : microservices-demo-prod spec : # ACME issuer configuration: # `email` - the email address to be associated with the ACME account (make sure it's a valid one). # `server` - the URL used to access the ACME server\u2019s directory endpoint. # `privateKeySecretRef` - Kubernetes Secret to store the automatically generated ACME account private key. acme : email : <YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-nginx-wcard-private # List of challenge solvers that will be used to solve ACME challenges for the matching domains. solvers : # Use the DigitalOcean DNS API to manage DNS01 challenge records. - dns01 : digitalocean : # Kubernetes secret that contains the DO API token . # Must be in the same namespace as the Issuer CRD. tokenSecretRef : name : digitalocean-dns key : access-token Apply via kubectl: kubectl apply -f docs/04-production/assets/manifests/cert-manager-wildcard-issuer.yaml Info Running kubectl get issuer letsencrypt-nginx-wcard -n microservices-demo-prod should result in the True value being displayed under the READY column. Note If the Issuer object displays a Not Ready state you can describe the object to get additional information using: kubectl describe issuer letsencrypt-nginx-wcard -n microservices-demo-prod to get more information. Create the wildcard certificates resource using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The certificate manifest file looks like the following: Click to expland the certificate resource apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : <YOUR_DOMAIN_NAME> # Cert-Manager will put the resulting Secret in the same Kubernetes namespace as the Certificate. namespace : microservices-demo-prod spec : # Secret name to create, where the private key and certificate should be stored. secretName : <YOUR_DOMAIN_NAME> # What Issuer to use for getting the certificate. issuerRef : name : letsencrypt-nginx-wcard kind : Issuer group : cert-manager.io # Common name to be used on the Certificate. commonName : \"*.<YOUR_DOMAIN_NAME>\" # List of DNS subjectAltNames to be set on the Certificate. dnsNames : - \"<YOUR_DOMAIN_NAME>\" - \"*.<YOUR_DOMAIN_NAME>\" Apply via kubectl: kubectl apply -f docs/04-production/assets/manifests/cert-manager-wildcard-certificate.yaml To verify the certificate status run: kubectl get certificate <YOUR_DOMAIN_NAME> -n microservices-demo-prod Info This may take a few minutes to complete. If the Certificate object displays a not ready state you can run: kubectl logs -l app=cert-manager,app.kubernetes.io/component=controller -n cert-manager Add the Ingress Nginx host using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The ingress host manifest file looks like the following: Click to expand the ingress host resource apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-microservices-demo-prod namespace : microservices-demo-prod spec : tls : - hosts : - \"*.<YOUR_DOMAIN_NAME>\" secretName : <YOUR_DOMAIN_NAME> rules : - host : <YOUR_A_RECORD>.<YOUR_DOMAIN_NAME> http : paths : - path : / pathType : Prefix backend : service : name : frontend port : number : 80 ingressClassName : nginx Apply via kubectl: kubectl apply -f docs/04-production/assets/manifests/ingress-host.yaml Open a web browser and point to <YOUR_A_RECORD>.<YOUR_DOMAIN> . You should see the online boutique welcome page. The connection is secure and the certificate is a valid one issued by Let's Encrypt . Next you will install and configure the Prometheus stack for monitoring your DOKS cluster, Loki to fetch and aggregate logs from your cluster's resources and view them in Grafana and configure AlertManager to alert and notify when there is a critical issue in your cluster. You will also configure the events exporter tool to grab Kubernetes events and send and store them in Loki as they are a great way to monitor the health and activity of your K8s clusters.","title":"Installing the Nginx Ingress Controller"},{"location":"05-ci-cd/setup-continuous-deployments/","text":"Introduction After setting up the CI process, the next step is to configure automated (or continuous) deployments for your environments. In some setups, continuous deployments is not desired. In the end, it all drills down to how often you want to deliver release for your project. If the release cycle and cadence of your project is more agile and you want to deliver more releases in a short period of time, then it makes sense to have such an automated setup. In practice, this is not the only reason. You will want some environments such as the development environment to continuously reflect latest code changes of your main application repository branch. Here is where continuous deployments play an important role. Another important aspect is - how do you track each change and what is deployed where? To answer all above questions, a new concept is introduced called GitOps . GitOps is yet another set of methodologies and accompanying practices, allowing automated deployments and easy track of changes for all your application deployments to various environments. It relies on Git as the single source of truth. It means, you rely on the Git history feature to track all changes, and revert the system to a previous working state in case something goes bad. One popular solution used to implement GitOps principles is Argo CD , a free and open source project very well supported by the community. Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. Where does Argo CD fit in? Argo CD sits at the very end of your CI process. It waits for changes to happen in your Git repository over it is watching. No need to create and maintain separate GitHub workflows for deploying application components to each environment. Just tell Argo about your GitHub repository, and what Kubernetes cluster to sync with. Then, let Argo CD do the heavy lifting. Do I need to create separate GitHub repositories and/or branches to sync each Kubernetes environment? The short answer is no. No, you don't have to do this. Because you already use Kustomize overlays in your project, you have full control over the deployment process to each environment. No need to create separate branches to target each environment, which is hard to maintain and not recommended. For small projects it is often enough to use a monorepo structure where you have both application code and Kubernetes configuration manifests. In case of bigger projects, it's best to split application code and Kubernetes stuff into separate repositories. It's easier to track application vs Kubernetes changes this way. This guides relies on a monorepo approach, but it should be relatively easy to migrate to a split repo setup because all Kubernetes manifests are kept in the kustomize subfolder. Do I need a separate Argo CD instance per environment or just one connecting all? You can go either one or the other. This guide is using a separate ArgoCD instance per environment. This kind of setup doesn't affect one environment or the other if one ArgoCD instance goes down, or even if one of the clusters is in a degraded state. Only the current environment where Argo operates is affected. This is called a decentralized setup. The only drawback of this configuration is that application specific resources and system specific resources operate in the same DOKS cluster which leads to additional CPU and/or memory usage. Another kind of setup (not covered in this guide) is where you have one dedicated DOKS instance (or a dedicated node pool) to serve this purpose. This is called a centralized setup where one Argo CD instance manages all environments from a single place. Main advantage is user application is now decoupled from system apps, and more resources become available for the app you're developing. Main disadvantage is additional costs for operating a dedicated cluster. Another drawback is possible Argo CD downtime for all environments if HA is not properly configured, or if the cluster is not properly sized to handle multiple Argo projects and applications. Following diagram depicts the Argo CD setup used in this guide for each environment (decentralized setup): It's important to understand Argo CD concepts, so please follow the official getting started guide . To keep it short, you need to know how to operate Argo applications and projects . The application CRD is the most important bit of configuration dealing with connecting various sources such as Git repositories and Kubernetes clusters. On the other hand, Argo CD projects represent a logical way of grouping multiple applications related to each other. Next, you will learn how to bootstrap Argo CD for each environment. The procedure is basically the same, so once you learn how to do it for one environment, it should be pretty straightforward to perform the same steps for the remaining ones. Prerequisites To complete this section you will need: A container registry already set up as explained in the Getting Started -> Set up DOCR section. Also, make sure the initial version ( v1.0.0 ) for the demo application is already pushed to your DOCR as explained in the same chapter. A DOKS cluster set up and running for each environment: Development Environment -> Set up DOKS Staging Environment -> Set up DOKS Production Environment -> Set up DOKS The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Argo CD Autopilot CLI installed for your distribution as explained in the official docs. A GitHub Personal Access Token (or PAT for short) with the repo permissions set. It is required only once by the autopilot CLI to bootstrap Argo CD to your cluster for each environment. Bootstrapping Argo CD for the Development Environment In this section you will deploy Argo CD to your development DOKS cluster using the autopilot CLI . Then, you will configure Argo to sync changes using the dev overlay folder from your microservices-demo GitHub repository. The Argo CD autopilot project aims to ease the initial bootstrapping process of your Argo instance for each environment. What is nice about this approach is that the Argo installation itself is also synced with your GitHub repo in a GitOps fashion. The autopilot CLI will first deploy Argo CD in your cluster, and then push all required manifests to your GitHub repository. Please follow below steps to bootstrap and deploy Argo CD to your development DOKS cluster: Export the GitHub personal access token via the GIT_TOKEN environment variable (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your microservices-demo-dev cluster (notice that cluster name is prefixed using the do-<region>- identifier): kubectl config set-context do -nyc1-microservices-demo-dev Bootstrap Argo CD for the development DOKS cluster using the autopilot CLI. You need to do this only once on a fresh cluster (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/dev\" Note Once the process finishes successfully you will receive instructions how to access the Argo CD web interface. Most important, you will get the admin user password - put it safe for later use. Also, a new folder should be present in your Git repository - argocd/dev , containing all manifests for the Argo CD dev environment instance. The same folder is synced by Argo to keep itself up to date, thus following GitOps principles. Check if the Argo CD dev instance is up and running: kubectl get deployments -n argocd The output looks similar to: NAME READY UP-TO-DATE AVAILABLE AGE argocd-applicationset-controller 1/1 1 1 15m argocd-dex-server 1/1 1 1 15m argocd-notifications-controller 1/1 1 1 15m argocd-redis 1/1 1 1 15m argocd-repo-server 1/1 1 1 15m argocd-server 1/1 1 1 15m All Argo CD deployments must be healthy and running. Create an Argo project for the dev environment (make sure to replace the <> placeholders first): argocd-autopilot project create dev \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/dev\" Tip It's best practice to organize your Argo applications using projects. For example, you can use Argo projects to define environments, such as in the above example. Finally, create an Argo application for the microservices-demo project using the Kustomize dev overlay (make sure to replace the <> placeholders first): argocd-autopilot app create microservices-demo \\ --app \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/dev\" \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/dev\" \\ --project dev \\ --type kustomize Now, check if Argo created the microservices-demo application. First, port-forward the Argo web interface: kubectl port-forward -n argocd svc/argocd-server 8080 :80 Open the Argo CD dashboard in your web browser using this link - localhost:8080 : If everything went well, you should see the dev-microservices-demo app created and synced successfully. Note Argo CD is using this naming convention - <project_name>-<app_name> , hence the dev-microservices-demo name for the application. Please bear in mind that Argo CD is using the Git polling method by default which takes around 3 minutes to trigger. So, changes are not propagated in an instant. Next, click on the dev-microservices-demo app tile - you should see the online boutique application composition (microservices): Finally, port-forward the frontend service to check the online boutique application status: kubectl port-forward -n microservices-demo-dev svc/frontend 9090 :80 Open a web browser pointing to localhost:9090 - you should see the online boutique application landing page. Tip You should see the previous changes that you made in the Set up continuous integration -> Testing the Online Boutique Application GitHub Workflows chapter applied as well. So, please go ahead and check that as well. Next, you will perform the same steps to bootstrap Argo CD for the staging environment. Bootstrapping Argo CD for the Staging Environment In this section you will deploy Argo CD to your staging DOKS cluster using the autopilot CLI . Then, you will configure Argo to sync changes using the staging overlay folder from your microservices-demo GitHub repository. Please follow below steps to bootstrap and deploy Argo CD to your staging DOKS cluster: Export the GitHub personal access token via the GIT_TOKEN environment variable, if not already (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your microservices-demo-staging cluster (notice that cluster name is prefixed using the do-<region>- identifier): kubectl config set-context do -nyc1-microservices-demo-staging Bootstrap Argo CD for the staging DOKS cluster using the autopilot CLI. You need to do this only once on a fresh cluster (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" Note Once the process finishes successfully you will receive instructions how to access the Argo CD web interface. Most important, you will get the admin user password - put it safe for later use. Also, a new folder should be present in your Git repository - argocd/staging , containing all manifests for the Argo CD staging environment instance. The same folder is synced by Argo to keep itself up to date, thus following GitOps principles. Check if the Argo CD staging instance is up and running: kubectl get deployments -n argocd The output looks similar to: NAME READY UP-TO-DATE AVAILABLE AGE argocd-applicationset-controller 1/1 1 1 10m argocd-dex-server 1/1 1 1 10m argocd-notifications-controller 1/1 1 1 10m argocd-redis 1/1 1 1 10m argocd-repo-server 1/1 1 1 10m argocd-server 1/1 1 1 10m All Argo CD deployments must be healthy and running. Create an Argo project for the staging environment (make sure to replace the <> placeholders first): argocd-autopilot project create staging \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" Tip It's best practice to organize your Argo applications using projects. For example, you can use Argo projects to define environments, such as in the above example. Finally, create an Argo application for the microservices-demo project using the Kustomize staging overlay (make sure to replace the <> placeholders first): argocd-autopilot app create microservices-demo \\ --app \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/staging\" \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" \\ --project staging \\ --type kustomize To verify the whole setup works, please use the same procedure as you already learned in the Bootstrapping Argo CD for the Development Environment section of this chapter. Note Please bear in mind that Argo CD is using the Git polling method by default which takes around 3 minutes to trigger. So, changes are not propagated in an instant. Next, you will perform the same steps to bootstrap Argo CD for the production environment. Bootstrapping Argo CD for the Production Environment In this section you will deploy Argo CD to your production DOKS cluster using the autopilot CLI . Then, you will configure Argo to sync changes using the prod overlay folder from your microservices-demo GitHub repository. Please follow below steps to bootstrap and deploy Argo CD to your production DOKS cluster: Export the GitHub personal access token via the GIT_TOKEN environment variable, if not already (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your microservices-demo-production cluster (notice that cluster name is prefixed using the do-<region>- identifier): kubectl config set-context do -nyc1-microservices-demo-production Bootstrap Argo CD for the production DOKS cluster using the autopilot CLI. You need to do this only once on a fresh cluster (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/prod\" Note Once the process finishes successfully you will receive instructions how to access the Argo CD web interface. Most important, you will get the admin user password - put it safe for later use. Also, a new folder should be present in your Git repository - argocd/prod , containing all manifests for the Argo CD production environment instance. The same folder is synced by Argo to keep itself up to date, thus following GitOps principles. Check if the Argo CD production instance is up and running: kubectl get deployments -n argocd The output looks similar to: NAME READY UP-TO-DATE AVAILABLE AGE argocd-applicationset-controller 1/1 1 1 5m argocd-dex-server 1/1 1 1 5m argocd-notifications-controller 1/1 1 1 5m argocd-redis 1/1 1 1 5m argocd-repo-server 1/1 1 1 5m argocd-server 1/1 1 1 5m All Argo CD deployments must be healthy and running. Create an Argo project for the production environment (make sure to replace the <> placeholders first): argocd-autopilot project create prod \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/prod\" Tip It's best practice to organize your Argo applications using projects. For example, you can use Argo projects to define environments, such as in the above example. Finally, create an Argo application for the microservices-demo project using the Kustomize prod overlay (make sure to replace the <> placeholders first): argocd-autopilot app create microservices-demo \\ --app \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/prod\" \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/prod\" \\ --project prod \\ --type kustomize To verify the whole setup works, please use the same procedure as you already learned in the Bootstrapping Argo CD for the Development Environment section of this chapter. Note Please bear in mind that Argo CD is using the Git polling method by default which takes around 3 minutes to trigger. So, changes are not propagated in an instant. Testing the Final Setup As an exercise, go ahead and make a change to one or even more microservices. Then, open a PR and check the associated workflow. Next, approve the PR and merge change into main branch. Check the main branch GitHub workflow as it progresses. When finished, check if Argo propagated the latest changes to your development DOKS cluster. Next, you will learn how to create GitHub releases for your application and propagate (or promote) changes to upper environments. First to staging, and then after QA approval (may imply project manager decision as well) deploy to production environment as well.","title":"Set up continuous deployments"},{"location":"05-ci-cd/setup-continuous-deployments/#introduction","text":"After setting up the CI process, the next step is to configure automated (or continuous) deployments for your environments. In some setups, continuous deployments is not desired. In the end, it all drills down to how often you want to deliver release for your project. If the release cycle and cadence of your project is more agile and you want to deliver more releases in a short period of time, then it makes sense to have such an automated setup. In practice, this is not the only reason. You will want some environments such as the development environment to continuously reflect latest code changes of your main application repository branch. Here is where continuous deployments play an important role. Another important aspect is - how do you track each change and what is deployed where? To answer all above questions, a new concept is introduced called GitOps . GitOps is yet another set of methodologies and accompanying practices, allowing automated deployments and easy track of changes for all your application deployments to various environments. It relies on Git as the single source of truth. It means, you rely on the Git history feature to track all changes, and revert the system to a previous working state in case something goes bad. One popular solution used to implement GitOps principles is Argo CD , a free and open source project very well supported by the community. Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. Where does Argo CD fit in? Argo CD sits at the very end of your CI process. It waits for changes to happen in your Git repository over it is watching. No need to create and maintain separate GitHub workflows for deploying application components to each environment. Just tell Argo about your GitHub repository, and what Kubernetes cluster to sync with. Then, let Argo CD do the heavy lifting. Do I need to create separate GitHub repositories and/or branches to sync each Kubernetes environment? The short answer is no. No, you don't have to do this. Because you already use Kustomize overlays in your project, you have full control over the deployment process to each environment. No need to create separate branches to target each environment, which is hard to maintain and not recommended. For small projects it is often enough to use a monorepo structure where you have both application code and Kubernetes configuration manifests. In case of bigger projects, it's best to split application code and Kubernetes stuff into separate repositories. It's easier to track application vs Kubernetes changes this way. This guides relies on a monorepo approach, but it should be relatively easy to migrate to a split repo setup because all Kubernetes manifests are kept in the kustomize subfolder. Do I need a separate Argo CD instance per environment or just one connecting all? You can go either one or the other. This guide is using a separate ArgoCD instance per environment. This kind of setup doesn't affect one environment or the other if one ArgoCD instance goes down, or even if one of the clusters is in a degraded state. Only the current environment where Argo operates is affected. This is called a decentralized setup. The only drawback of this configuration is that application specific resources and system specific resources operate in the same DOKS cluster which leads to additional CPU and/or memory usage. Another kind of setup (not covered in this guide) is where you have one dedicated DOKS instance (or a dedicated node pool) to serve this purpose. This is called a centralized setup where one Argo CD instance manages all environments from a single place. Main advantage is user application is now decoupled from system apps, and more resources become available for the app you're developing. Main disadvantage is additional costs for operating a dedicated cluster. Another drawback is possible Argo CD downtime for all environments if HA is not properly configured, or if the cluster is not properly sized to handle multiple Argo projects and applications. Following diagram depicts the Argo CD setup used in this guide for each environment (decentralized setup): It's important to understand Argo CD concepts, so please follow the official getting started guide . To keep it short, you need to know how to operate Argo applications and projects . The application CRD is the most important bit of configuration dealing with connecting various sources such as Git repositories and Kubernetes clusters. On the other hand, Argo CD projects represent a logical way of grouping multiple applications related to each other. Next, you will learn how to bootstrap Argo CD for each environment. The procedure is basically the same, so once you learn how to do it for one environment, it should be pretty straightforward to perform the same steps for the remaining ones.","title":"Introduction"},{"location":"05-ci-cd/setup-continuous-deployments/#prerequisites","text":"To complete this section you will need: A container registry already set up as explained in the Getting Started -> Set up DOCR section. Also, make sure the initial version ( v1.0.0 ) for the demo application is already pushed to your DOCR as explained in the same chapter. A DOKS cluster set up and running for each environment: Development Environment -> Set up DOKS Staging Environment -> Set up DOKS Production Environment -> Set up DOKS The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Argo CD Autopilot CLI installed for your distribution as explained in the official docs. A GitHub Personal Access Token (or PAT for short) with the repo permissions set. It is required only once by the autopilot CLI to bootstrap Argo CD to your cluster for each environment.","title":"Prerequisites"},{"location":"05-ci-cd/setup-continuous-deployments/#bootstrapping-argo-cd-for-the-development-environment","text":"In this section you will deploy Argo CD to your development DOKS cluster using the autopilot CLI . Then, you will configure Argo to sync changes using the dev overlay folder from your microservices-demo GitHub repository. The Argo CD autopilot project aims to ease the initial bootstrapping process of your Argo instance for each environment. What is nice about this approach is that the Argo installation itself is also synced with your GitHub repo in a GitOps fashion. The autopilot CLI will first deploy Argo CD in your cluster, and then push all required manifests to your GitHub repository. Please follow below steps to bootstrap and deploy Argo CD to your development DOKS cluster: Export the GitHub personal access token via the GIT_TOKEN environment variable (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your microservices-demo-dev cluster (notice that cluster name is prefixed using the do-<region>- identifier): kubectl config set-context do -nyc1-microservices-demo-dev Bootstrap Argo CD for the development DOKS cluster using the autopilot CLI. You need to do this only once on a fresh cluster (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/dev\" Note Once the process finishes successfully you will receive instructions how to access the Argo CD web interface. Most important, you will get the admin user password - put it safe for later use. Also, a new folder should be present in your Git repository - argocd/dev , containing all manifests for the Argo CD dev environment instance. The same folder is synced by Argo to keep itself up to date, thus following GitOps principles. Check if the Argo CD dev instance is up and running: kubectl get deployments -n argocd The output looks similar to: NAME READY UP-TO-DATE AVAILABLE AGE argocd-applicationset-controller 1/1 1 1 15m argocd-dex-server 1/1 1 1 15m argocd-notifications-controller 1/1 1 1 15m argocd-redis 1/1 1 1 15m argocd-repo-server 1/1 1 1 15m argocd-server 1/1 1 1 15m All Argo CD deployments must be healthy and running. Create an Argo project for the dev environment (make sure to replace the <> placeholders first): argocd-autopilot project create dev \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/dev\" Tip It's best practice to organize your Argo applications using projects. For example, you can use Argo projects to define environments, such as in the above example. Finally, create an Argo application for the microservices-demo project using the Kustomize dev overlay (make sure to replace the <> placeholders first): argocd-autopilot app create microservices-demo \\ --app \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/dev\" \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/dev\" \\ --project dev \\ --type kustomize Now, check if Argo created the microservices-demo application. First, port-forward the Argo web interface: kubectl port-forward -n argocd svc/argocd-server 8080 :80 Open the Argo CD dashboard in your web browser using this link - localhost:8080 : If everything went well, you should see the dev-microservices-demo app created and synced successfully. Note Argo CD is using this naming convention - <project_name>-<app_name> , hence the dev-microservices-demo name for the application. Please bear in mind that Argo CD is using the Git polling method by default which takes around 3 minutes to trigger. So, changes are not propagated in an instant. Next, click on the dev-microservices-demo app tile - you should see the online boutique application composition (microservices): Finally, port-forward the frontend service to check the online boutique application status: kubectl port-forward -n microservices-demo-dev svc/frontend 9090 :80 Open a web browser pointing to localhost:9090 - you should see the online boutique application landing page. Tip You should see the previous changes that you made in the Set up continuous integration -> Testing the Online Boutique Application GitHub Workflows chapter applied as well. So, please go ahead and check that as well. Next, you will perform the same steps to bootstrap Argo CD for the staging environment.","title":"Bootstrapping Argo CD for the Development Environment"},{"location":"05-ci-cd/setup-continuous-deployments/#bootstrapping-argo-cd-for-the-staging-environment","text":"In this section you will deploy Argo CD to your staging DOKS cluster using the autopilot CLI . Then, you will configure Argo to sync changes using the staging overlay folder from your microservices-demo GitHub repository. Please follow below steps to bootstrap and deploy Argo CD to your staging DOKS cluster: Export the GitHub personal access token via the GIT_TOKEN environment variable, if not already (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your microservices-demo-staging cluster (notice that cluster name is prefixed using the do-<region>- identifier): kubectl config set-context do -nyc1-microservices-demo-staging Bootstrap Argo CD for the staging DOKS cluster using the autopilot CLI. You need to do this only once on a fresh cluster (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" Note Once the process finishes successfully you will receive instructions how to access the Argo CD web interface. Most important, you will get the admin user password - put it safe for later use. Also, a new folder should be present in your Git repository - argocd/staging , containing all manifests for the Argo CD staging environment instance. The same folder is synced by Argo to keep itself up to date, thus following GitOps principles. Check if the Argo CD staging instance is up and running: kubectl get deployments -n argocd The output looks similar to: NAME READY UP-TO-DATE AVAILABLE AGE argocd-applicationset-controller 1/1 1 1 10m argocd-dex-server 1/1 1 1 10m argocd-notifications-controller 1/1 1 1 10m argocd-redis 1/1 1 1 10m argocd-repo-server 1/1 1 1 10m argocd-server 1/1 1 1 10m All Argo CD deployments must be healthy and running. Create an Argo project for the staging environment (make sure to replace the <> placeholders first): argocd-autopilot project create staging \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" Tip It's best practice to organize your Argo applications using projects. For example, you can use Argo projects to define environments, such as in the above example. Finally, create an Argo application for the microservices-demo project using the Kustomize staging overlay (make sure to replace the <> placeholders first): argocd-autopilot app create microservices-demo \\ --app \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/staging\" \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" \\ --project staging \\ --type kustomize To verify the whole setup works, please use the same procedure as you already learned in the Bootstrapping Argo CD for the Development Environment section of this chapter. Note Please bear in mind that Argo CD is using the Git polling method by default which takes around 3 minutes to trigger. So, changes are not propagated in an instant. Next, you will perform the same steps to bootstrap Argo CD for the production environment.","title":"Bootstrapping Argo CD for the Staging Environment"},{"location":"05-ci-cd/setup-continuous-deployments/#bootstrapping-argo-cd-for-the-production-environment","text":"In this section you will deploy Argo CD to your production DOKS cluster using the autopilot CLI . Then, you will configure Argo to sync changes using the prod overlay folder from your microservices-demo GitHub repository. Please follow below steps to bootstrap and deploy Argo CD to your production DOKS cluster: Export the GitHub personal access token via the GIT_TOKEN environment variable, if not already (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your microservices-demo-production cluster (notice that cluster name is prefixed using the do-<region>- identifier): kubectl config set-context do -nyc1-microservices-demo-production Bootstrap Argo CD for the production DOKS cluster using the autopilot CLI. You need to do this only once on a fresh cluster (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/prod\" Note Once the process finishes successfully you will receive instructions how to access the Argo CD web interface. Most important, you will get the admin user password - put it safe for later use. Also, a new folder should be present in your Git repository - argocd/prod , containing all manifests for the Argo CD production environment instance. The same folder is synced by Argo to keep itself up to date, thus following GitOps principles. Check if the Argo CD production instance is up and running: kubectl get deployments -n argocd The output looks similar to: NAME READY UP-TO-DATE AVAILABLE AGE argocd-applicationset-controller 1/1 1 1 5m argocd-dex-server 1/1 1 1 5m argocd-notifications-controller 1/1 1 1 5m argocd-redis 1/1 1 1 5m argocd-repo-server 1/1 1 1 5m argocd-server 1/1 1 1 5m All Argo CD deployments must be healthy and running. Create an Argo project for the production environment (make sure to replace the <> placeholders first): argocd-autopilot project create prod \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/prod\" Tip It's best practice to organize your Argo applications using projects. For example, you can use Argo projects to define environments, such as in the above example. Finally, create an Argo application for the microservices-demo project using the Kustomize prod overlay (make sure to replace the <> placeholders first): argocd-autopilot app create microservices-demo \\ --app \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/prod\" \\ --repo \"github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/prod\" \\ --project prod \\ --type kustomize To verify the whole setup works, please use the same procedure as you already learned in the Bootstrapping Argo CD for the Development Environment section of this chapter. Note Please bear in mind that Argo CD is using the Git polling method by default which takes around 3 minutes to trigger. So, changes are not propagated in an instant.","title":"Bootstrapping Argo CD for the Production Environment"},{"location":"05-ci-cd/setup-continuous-deployments/#testing-the-final-setup","text":"As an exercise, go ahead and make a change to one or even more microservices. Then, open a PR and check the associated workflow. Next, approve the PR and merge change into main branch. Check the main branch GitHub workflow as it progresses. When finished, check if Argo propagated the latest changes to your development DOKS cluster. Next, you will learn how to create GitHub releases for your application and propagate (or promote) changes to upper environments. First to staging, and then after QA approval (may imply project manager decision as well) deploy to production environment as well.","title":"Testing the Final Setup"},{"location":"05-ci-cd/setup-continuous-integration/","text":"Introduction Continuous integration, or CI for short, is the ongoing process of continuously integrating code into the main or active development branch of a Git repository. Usually, each developer works on a piece of code dealing with a specific functionality. In the end, all pieces merge together via the CI process, thus creating the final product. One typical Git flow is depicted below: gitGraph commit commit branch feature1 checkout feature1 commit commit checkout main branch feature2 checkout feature2 commit commit commit checkout main merge feature1 id: \"PR merge feature1\" tag: \"NEW TAG - v1.0.0\" type: HIGHLIGHT commit checkout feature2 merge main checkout main merge feature2 id: \"PR merge feature2\" tag: \"NEW TAG - v1.0.1\" type: HIGHLIGHT commit The Git flow used in this guide is a simple one - no release branches, no feature branches, etc. It's just the main branch and release tags. Of course, each developer will work in the end on a feature, fixing a bug, etc, so a dedicated branch gets created. But, in the end everything is merged into the main branch via pull requests. There is a list of reviewers which is automatically assigned, as well as automated workflows to automatically validate or invalidate PRs. Then, when a certain amount of bug fixes and/or proposed features are merged in, the owner (or the project manager) of the application repository creates a release. This is a separate process and requires manual intervention from designated team members. In the release process, a new version and tag is created (using semantic versioning), as well as the accompanying changelog (or release notes). GitHub is the most popular solution for collaborative work, sharing open source ideas, and storing application code. It is more than just a frontend for the most reliable source code management tool - Git . It also empowers issue tracking, project management, and most important of all - CI automation. GitHub offers actions and workflows to help you achieve this goal, hence it's a very good candidate. In this section, you will learn how to leverage the power of GitHub actions to implement the CI process required by the online boutique sample application. Prerequisites To complete this section you will need: A container registry already set up as explained in the Getting Started -> Set up DOCR section. A development DOKS cluster set up and running as explained in the Development Environment -> Set up DOKS section. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Main branch protection is enabled and configured for your repository. Also number of reviewers should be set to at least one. Main branch changes should be allowed only via pull requests . Your DigitalOcean authentication token stored as a GitHub secret named DIGITALOCEAN_ACCESS_TOKEN in the microservices-demo repository. Follow this guide to learn more about creating GitHub secrets. Online Boutique Application CI Flows Configuration The CI process used in this guide is comprised of two phases, each with a workflow associated: Developers open a new pull request (or PR for short). Then, automated tests run via a dedicated GitHub workflow, and validate or invalidate the PR. Also, an additional manual review step is required by setting the number of approvals in the main branch protection rules . A typical pull request flow is depicted below: graph TD A(New PR) --> B(Run Automated Tests) B --> C{Tests Passing?} C -- No --> D(Reject PR) C -- Yes --> E{PR Review Passed?} E -- No --> D E -- Yes ---> F(Merge PR) If the PR gets approved and code is merged to main branch, a second workflow runs which builds project assets, pushes them to a registry, and deploys the application to the development environment. Note In practice, it's safe to have automated deployments for the development environment. This way, developers or other team members can see immediate results, and test application behavior on the real environment (besides remote development with Tilt ). A typical main branch flow is depicted below: graph TD A(Close PR) --> B(Merge to Main Branch) B --> C(Run Integration Tests) C --> D{Tests Passing?} D -- No --> E(Reject Build) D -- Yes --> F(Build & Tag DEV Images) F --> G(Push DEV Images to Registry) G --> H(Deploy to DEV Environment) Next, you will learn how to implement the CI workflows used in this guide for the online boutique sample application . Configuring Pull Request Workflow On each pull request a dedicated workflow is triggered responsible with running automated tests, part of PR validation step. Next, manual review is requested by the developer opening the PR. Reviewers can be assigned also automatically . Follow below steps to configure and enable main branch PR validation workflow provided in this guide: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Create workflows directory (required only once): mkdir -p .github/workflows/ Fetch the CI workflow file from the kubernetes-sample-apps repo: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-pr-ci.yaml \\ -o .github/workflows/online-boutique-pr-ci.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-pr-ci.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-pr-ci.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand Online Boutique PR CI workflow file name : Online Boutique PR CI on : # Used for testing only (can be disabled afterwards) workflow_dispatch : # Uncomment below lines to enable this workflow on main branch PR events pull_request : branches : - main paths-ignore : - \"**/README.md\" - \"kustomize/**\" - \"argocd/**\" - \".github/workflows/*\" # Do not allow concurrent workflows for PRs # If disabled, leads to memory exhaustion on the DOKS dev cluster concurrency : pr-ci-dev # Global environment variables env : DOCR_ENDPOINT : \"registry.digitalocean.com/microservices-demo\" CLUSTER_NAME : \"microservices-demo-dev\" REGION : \"nyc1\" K8S_NAMESPACE : \"microservices-demo-${{ github.event.pull_request.number }}\" PROJECT_NAME : \"online-boutique\" jobs : # Run unit tests in parallel using below matrix to cut down time # Unit tests are standalone and should not raise conflicts # Each microservice is written in a specific language, hence it's added to the matrix unit-tests : runs-on : ubuntu-latest strategy : matrix : include : - project_name : cartservice project_language : csharp - project_name : checkoutservice project_language : golang - project_name : currencyservice project_language : javascript - project_name : emailservice project_language : python - project_name : frontend project_language : golang - project_name : paymentservice project_language : javascript - project_name : productcatalogservice project_language : golang - project_name : recommendationservice project_language : python - project_name : shippingservice project_language : golang steps : - name : Checkout code uses : actions/checkout@v3 with : ref : ${{github.event.pull_request.head.ref}} repository : ${{github.event.pull_request.head.repo.full_name}} - name : Set up Go env if : ${{ matrix.project_language == 'golang' }} uses : actions/setup-go@v3 with : go-version : \"1.19\" - name : Go Unit Tests if : ${{ matrix.project_language == 'golang' }} timeout-minutes : 5 run : | ( cd src/${{ matrix.project_name }} go test ) - name : Set up C# env if : ${{ matrix.project_language == 'csharp' }} uses : actions/setup-dotnet@v2 with : dotnet-version : \"6.0\" include-prerelease : true - name : C# Unit Tests if : ${{ matrix.project_language == 'csharp' }} timeout-minutes : 5 run : dotnet test src/${{ matrix.project_name }}/ - name : Set up NodeJS env if : ${{ matrix.project_language == 'javascript' }} uses : actions/setup-node@v3 with : node-version : 18 - name : Javascript Unit Tests if : ${{ matrix.project_language == 'javascript' }} timeout-minutes : 5 run : | ( cd src/${{ matrix.project_name }} npm install npm run test ) - name : Set up Python env if : ${{ matrix.project_language == 'python' }} uses : actions/setup-python@v3 with : python-version : \"3.7\" - name : Python Unit Tests if : ${{ matrix.project_language == 'python' }} timeout-minutes : 5 run : | ( cd src/${{ matrix.project_name }} pip install -r requirements.txt pytest ) # Run deployment tests (smoke tests) # You can add integration tests as well # Please bear in mind that more tests means increased workflow run time # With workflow concurrency disabled, means more waiting time for other PRs in the queue deployment-tests : needs : unit-tests runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 with : ref : ${{github.event.pull_request.head.ref}} repository : ${{github.event.pull_request.head.repo.full_name}} - name : Install doctl uses : digitalocean/action-doctl@v2 with : token : ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }} - name : Log in to DOCR with short-lived credentials run : doctl registry login --expiry-seconds 1200 - name : Set up K8S tools uses : yokawasa/action-setup-kube-tools@v0.8.2 with : kubectl : \"1.25.2\" kustomize : \"4.5.7\" tilt : \"0.30.9\" - name : Configure kubectl for DOKS with short-lived credentials run : doctl kubernetes cluster kubeconfig save ${{ env.CLUSTER_NAME }} --expiry-seconds 1200 - name : Deploy microservices to DOKS timeout-minutes : 10 run : | # Bring all microservices up using Tilt and wait for all deployments cp tilt-resources/dev/tilt_config.json . tilt ci -- \\ --allowed_contexts \"do-${{ env.REGION }}-${{ env.CLUSTER_NAME }}\" \\ --default_registry \"${{ env.DOCR_ENDPOINT }}\" \\ --environment \"dev\" \\ --namespace \"${{ env.K8S_NAMESPACE }}\" - name : Build loadgenerator image uses : docker/build-push-action@v3 with : context : \"src/loadgenerator\" push : true tags : \"${{ env.DOCR_ENDPOINT }}/loadgenerator\" - name : Smoke tests timeout-minutes : 10 run : | # Prepare load generator # Inject workflow custom docker image sed -i \"s#<LOAD_GENERATOR_IMAGE>#${{ env.DOCR_ENDPOINT }}/loadgenerator#g\" loadgenerator.yaml # Deploy load generator kubectl apply -f loadgenerator.yaml -n ${{ env.K8S_NAMESPACE }} # Wait for load generator deployment to be ready kubectl wait --for=condition=available --timeout=60s deployment/loadgenerator -n ${{ env.K8S_NAMESPACE }} || { # Show why load generator failed to start echo \"[INFO] Load generator pod events\" kubectl describe pod -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | tail -10 exit 1 } # Run smoke tests REQUEST_COUNT=\"0\" while [[ \"$REQUEST_COUNT\" -lt \"50\" ]]; do sleep 5 REQUEST_COUNT=$(kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | grep Aggregated | awk '{print $2}') done # ensure there are no errors hitting endpoints ERROR_COUNT=$(kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | grep Aggregated | awk '{print $3}' | sed \"s/[(][^)]*[)]//g\") if [[ \"$ERROR_COUNT\" -gt \"0\" ]]; then # Print final results echo \"[INFO] Load generator results\" kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | tail -20 exit 1 fi # Print final results echo \"[INFO] Load generator results\" kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | tail -20 working-directory : \"src/loadgenerator\" - name : Clean up Tilt microservices environment if : ${{ always() }} run : | # Remove all microservices and the namespace created by Tilt tilt down --delete-namespaces -- --namespace \"${{ env.K8S_NAMESPACE }}\" - name : Clean up Tilt docker images from registry if : ${{ always() }} run : | # Remove Tilt docker images from registry for tilt_repo in $(docker images --format \"{{.Repository}}:{{.Tag}}\" | grep \"tilt-.*[a-z|0-9]\"); do repo_and_tag=\"${tilt_repo##*/}\" repo=\"${repo_and_tag%%:*}\" tag=\"${repo_and_tag##*:}\" echo \"[INFO] Deleting tag $tag from repo $repo ...\" doctl registry repository delete-tag \"$repo\" \"$tag\" -f done echo \"[INFO] Remember to run the DOCR garbage collector from time to time!\" Save the file and commit changes to your git repository main branch (you may need to disable main branch protection temporarily to perform this step). Explanation for the above configuration: on.pull_request - triggers the Online Boutique PR CI workflow on pull request events only. on.pull_request.branches - triggers the Online Boutique PR CI workflow whenever a pull request event is detected for the specified list of branches. In this case only main branch is desired. on.pull_requests.paths-ignore - list of repository paths used for filtering. The Online Boutique PR CI workflow is triggered only on paths not present in the paths-ignore list. Basically, everything is included (microservices as well), except for Kustomize logic, README files, and workflows logic. Kustomize configuration is dealt in a separate workflow, because it's not related to microservices application logic. env - sets environment variables to use for the whole pipeline. Usually, environment variables control CI workflow behavior (such as telling Tilt how to load environment specific profiles). jobs - defines list of job to run inside the pipeline such as unit tests, deployment tests, etc. strategy.matrix - use a matrix build type. Runs the unit tests in parallel for each project type combination. This approach is a perfect match for projects using multiple components, such as microservices. It also cuts down the time required to build and test each component. Each element from the matrix sets the project name, and the language being used. steps - list of steps to execute as part of the workflow jobs. For each project component (or microservice), the following list of actions is executed: Code checkout, via actions/checkout@v3 . Specific tools are installed based on project language ( actions/setup-dotnet@v2 , actions/setup-go@v3 , etc). Unit tests are executed for each project. For deployment tests, the whole suite of microservices is deployed to the DEV environment in a dedicated namespace (unique across PRs) using Tilt. The dev profile is being used. Then, load tests are performed via the smoke tests step. Finally, clean up is performed. The microservices setup allocated for the PR is destroyed, including associated Docker images pushed to the registry. Following diagram shows the Online Boutique PR CI workflow composition: graph LR A(PR Workflow Start) --> B(Unit Tests Matrix) B --> C(Cart Service <br> Tests) B --> D(Checkout Service <br> Tests) B --> E(Currency Service <br> Tests) B --> F(Email Service <br> Tests) B --> G(Frontend Tests) B --> H(Payment Service <br> Tests) B --> I(Product Catalog Service <br> Tests) B --> J(Recommendation Service <br> Tests) B --> K(Shipping Service <br> Tests) C & D & E & F & G & H & I & J & K --> L(Deployment Tests) L --> M(Env Clean Up) Note In order to run deployment tests, the example CI workflow provided in this guide creates a dedicated Kubernetes namespace to deploy microservices on your target DOKS development cluster. It is suffixed using the PR number which is unique across PRs. When it finishes, it will try to clean up all associated resources such as the provisioned Kubernetes namespace, and docker images from the registry. For docker registry, you still need to run garbage collection (not triggered automatically in the workflow because it puts the DOCR in read-only mode, hence other PR workflows will fail at the docker clean up step). Next, you learn how to configure and enable the main branch workflow that gets triggered whenever changes are pushed to the main branch after each PR merge. Configuring Main Branch Workflow Whenever a PR is closed and code is merged into the main branch, a dedicated GitHub workflow is triggered. Main purpose of the main branch workflow is to test the whole application as a whole after merging various features via integration tests. Then, it will build and tag images using latest commit id, push them to registry, and finally deploy everything to the development environment. Usually you want the development environment to continuously reflect the latest changes from the development (or main) branch. Notes There's no point to run unit tests again after each PR merge. Unit testing deals with changes for the affected components only. This part it is already taken care in the PR workflow. What makes more sense is to run integration tests as part of the main branch CI flow. This way, you check if application functionality is impacted after merging features in. Adding a GitHub badge in the main project README is another great benefit, giving visual feedback for the latest build state - passing or failing. Follow below steps to configure and enable the main branch workflow provided in this guide: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the CI workflow file from the kubernetes-sample-apps repo: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-main-ci.yaml \\ -o .github/workflows/online-boutique-main-ci.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-main-ci.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-main-ci.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand Online Boutique CI Main workflow file name : Online Boutique CI Main on : # Used for testing only (can be disabled afterwards) workflow_dispatch : # Uncomment below lines to enable this workflow on push to main events push : branches : - main paths-ignore : - \"**/README.md\" - \"kustomize/**\" - \"argocd/**\" - \".github/workflows/*\" # Do not allow concurrent workflows # Changes should be delivered one a time as code is merged into the main branch concurrency : main-ci-dev # Global environment variables env : CI_COMMIT_AUTHOR : \"CI GitHub Actions\" CI_COMMIT_AUTHOR_EMAIL : \"gh-actions@noreply.github.com\" DOCR_ENDPOINT : \"registry.digitalocean.com/microservices-demo\" PROJECT_NAME : \"online-boutique\" jobs : # There's no point to run unit tests again after PR merge # Unit testing deals with changes for the affected components, # and that it is already taken care in the PR workflow # What it makes sense, is to run integration tests, # to see if the whole application is impacted after merging the changes # # Run deployment tests (integration tests) # Please bear in mind that more tests means increased workflow run time # With workflow concurrency disabled, means more waiting time for other PRs in the queue deployment-tests : runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 - name : Run integration tests run : | echo \"[INFO] Not implemented yet!\" # Build and push project images in parallel using a matrix strategy # Cuts down build time build-and-push-images : needs : deployment-tests runs-on : ubuntu-latest strategy : matrix : project : - cartservice - checkoutservice - currencyservice - emailservice - frontend - paymentservice - productcatalogservice - recommendationservice - shippingservice steps : - name : Checkout code uses : actions/checkout@v3 - name : Install doctl uses : digitalocean/action-doctl@v2 with : token : ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }} - name : Log in to DOCR with short-lived credentials run : doctl registry login --expiry-seconds 600 - name : Build and push image uses : docker/build-push-action@v3 with : # cartservice is an exception - Dockerfile is placed in src/cartservice/src subfolder context : \"src/${{ matrix.project }}/${{ matrix.project == 'cartservice' && 'src' || ''}}\" push : true tags : \"${{ env.DOCR_ENDPOINT }}/${{ matrix.project }}:${{ github.sha }}\" # Kustomize image field for each microservice present in the `src/` dir # Finally, commit changes to main branch and let ArgoCD take over afterwards apply-kustomize-changes : needs : build-and-push-images runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 - name : Set up K8S tools uses : yokawasa/action-setup-kube-tools@v0.8.2 with : kustomize : \"4.5.7\" - name : Kustomize dev environment images run : | for microservice in src/*/; do microservice=\"$(basename $microservice)\" if [[ \"$microservice\" == \"loadgenerator\" ]]; then continue fi ( cd kustomize/dev kustomize edit set image $microservice=${{ env.DOCR_ENDPOINT }}/${microservice}:${{ github.sha }} ) done - name : Commit Kustomize manifests for dev env run : | git config --global user.name \"${{ env.CI_COMMIT_AUTHOR }}\" git config --global user.email \"${{ env.CI_COMMIT_AUTHOR_EMAIL }}\" git add kustomize/dev/ git commit -m \"Bump docker images tag\" - name : Push changes uses : ad-m/github-push-action@master with : github_token : ${{ secrets.GITHUB_TOKEN }} Save the file and commit changes to your git repository main branch (you may need to disable main branch protection temporarily to perform this step). Explanation for the above configuration: on.push - triggers the Online Boutique Main CI workflow on push events only. on.push.branches - triggers the Online Boutique Main CI workflow whenever a push event is detected for the specified list of branches. In this case only main branch is desired. on.push.paths-ignore - list of repository paths used for filtering. The Online Boutique Main CI workflow is triggered only on paths not present in the paths-ignore list. Basically, everything is included (microservices as well), except for Kustomize logic, README files, and workflows logic. Kustomize configuration is dealt in a separate workflow, because it's not related to microservices application logic. env - sets environment variables to use for the whole pipeline. Usually, environment variables control CI workflow behavior (such as telling ArgoCD how to sync only for the specific environment). jobs - defines list of job to run inside the pipeline such as integration tests, build and push docker images, apply Kustomize changes, etc. steps - list of steps implementing workflow jobs logic. Following diagram shows the Online Boutique Main CI workflow composition: graph TD A(Main Branch Workflow Start) --> B(Deployment Tests) B --> C{Tests Passing?} C -- No --> D(Reject build) C -- Yes --> E(Build and Push Images) E --> F(Kustomize DEV Images Tags) F --> G(Commit Kustomize Changes) G --> H(Set Build Passing Status Badge) H -.-> I(ArgoCD DEV Sync) style I stroke-dasharray: 5 5 Notes The deployment tests step contains only a placeholder for integration tests. This part is implementation specific and it is left for the user to configure. The final ArgoCD sync step from above diagram is marked as optional (dotted representation) because Argo will automatically trigger a deployment for the DEV environment anyways. Depending how Argo is configured, it can happen immediately via GitHub webhooks, or via polling (3 minutes interval by default). You can also force a sync and avoid waiting time via the CI workflow if desired. Next, you will test each workflow to check if it meets the required functionality. Testing the Online Boutique Application GitHub Workflows In this section you will test each workflow functionality for each event - pull requests and main branch code merge. Testing the Pull Request Workflow Follow below steps to test the PR workflow for the online boutique application: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Checkout to a new branch to add a new feature: git checkout -b features/musical_instruments Open the header template file for the frontend microservice using a text editor of your choice, preferably with HTML lint support. For example, you can use VS Code : code src/frontend/templates/header.html Change the Cymbal Shops string from title section to something different (e.g. Musical Instruments ): < title > {{ if $.is_cymbal_brand }} Musical Instruments {{ else }} Online Boutique {{ end }} </ title > Tip You can quickly spin up a local environment using Tilt to test the changes first, as explained in the local development using Tilt section. You will get live updates for each change which helps in the iterative development process. Save changes, commit and push new branch to remote: git commit -am \"New feature - musical instruments.\" git push origin features/musical_instruments Navigate to your GitHub repository page, and open a new pull request against main branch using the features/musical_instruments branch as the base. At this point, the Online Boutique PR CI should kick in. Branch merging should be blocked until the workflow finishes successfully, and at least one approval is present (explained in the prerequisites section): Next, navigate to the Actions tab of your GitHub repository and inspect PR workflow progress: If everything goes as planned, the workflow should pass and the PR can be merged. This is the happy flow. You should also check what happens if introducing a change that breaks things - change one of the unit tests so that it doesn't compile successfully. Next, you will check the main branch workflow and see if applies required kustomize changes for the development environment. Testing the Main Branch Workflow The second workflow should automatically start after merging code into main branch via the pull request created in the previous step. Navigate to the actions tab of your GitHub repository to see it in action: Note Automated deployments to the development environment cluster are disabled for now until configuring continuous deployments via ArgoCD in the next chapter. So, nothing gets deployed yet to the dev environment. You should also see a new commit added in the Git history of your repo. It shows kustomize changes for each image that was built and tagged using latest PR merge commit id: Going further, you should be able to enhance the CI process even more by following some additional best practices, such as: Run tests only for affected microservices. Should improve workflow run time even more. Strategy matrix used to speed up actions inside the workflow such as running unit tests and building docker images is static. It is possible to create a strategy matrix with dynamic values. For example, the tilt_config.json file from the dev profile already contains the list of project microservices, so the matrix can read the values directly from the list. Use artifacts caching inside workflows whenever possible. Decouple application code and Kubernetes configuration by using a dedicated Git repository. Application code should not be dependent on Kubernetes configuration, hence it make sense to stay in a different repository. Also, whenever changes are required on the Kubernetes side, you don't need to open PRs and merge Kubernetes specific commits into the application repository. Next, ArgoCD should pickup the changes and deploy the application automatically to your development environment DOKS cluster. In order for this part to work you need to set it up in the following section.","title":"Set up continuous integration"},{"location":"05-ci-cd/setup-continuous-integration/#introduction","text":"Continuous integration, or CI for short, is the ongoing process of continuously integrating code into the main or active development branch of a Git repository. Usually, each developer works on a piece of code dealing with a specific functionality. In the end, all pieces merge together via the CI process, thus creating the final product. One typical Git flow is depicted below: gitGraph commit commit branch feature1 checkout feature1 commit commit checkout main branch feature2 checkout feature2 commit commit commit checkout main merge feature1 id: \"PR merge feature1\" tag: \"NEW TAG - v1.0.0\" type: HIGHLIGHT commit checkout feature2 merge main checkout main merge feature2 id: \"PR merge feature2\" tag: \"NEW TAG - v1.0.1\" type: HIGHLIGHT commit The Git flow used in this guide is a simple one - no release branches, no feature branches, etc. It's just the main branch and release tags. Of course, each developer will work in the end on a feature, fixing a bug, etc, so a dedicated branch gets created. But, in the end everything is merged into the main branch via pull requests. There is a list of reviewers which is automatically assigned, as well as automated workflows to automatically validate or invalidate PRs. Then, when a certain amount of bug fixes and/or proposed features are merged in, the owner (or the project manager) of the application repository creates a release. This is a separate process and requires manual intervention from designated team members. In the release process, a new version and tag is created (using semantic versioning), as well as the accompanying changelog (or release notes). GitHub is the most popular solution for collaborative work, sharing open source ideas, and storing application code. It is more than just a frontend for the most reliable source code management tool - Git . It also empowers issue tracking, project management, and most important of all - CI automation. GitHub offers actions and workflows to help you achieve this goal, hence it's a very good candidate. In this section, you will learn how to leverage the power of GitHub actions to implement the CI process required by the online boutique sample application.","title":"Introduction"},{"location":"05-ci-cd/setup-continuous-integration/#prerequisites","text":"To complete this section you will need: A container registry already set up as explained in the Getting Started -> Set up DOCR section. A development DOKS cluster set up and running as explained in the Development Environment -> Set up DOKS section. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Main branch protection is enabled and configured for your repository. Also number of reviewers should be set to at least one. Main branch changes should be allowed only via pull requests . Your DigitalOcean authentication token stored as a GitHub secret named DIGITALOCEAN_ACCESS_TOKEN in the microservices-demo repository. Follow this guide to learn more about creating GitHub secrets.","title":"Prerequisites"},{"location":"05-ci-cd/setup-continuous-integration/#online-boutique-application-ci-flows-configuration","text":"The CI process used in this guide is comprised of two phases, each with a workflow associated: Developers open a new pull request (or PR for short). Then, automated tests run via a dedicated GitHub workflow, and validate or invalidate the PR. Also, an additional manual review step is required by setting the number of approvals in the main branch protection rules . A typical pull request flow is depicted below: graph TD A(New PR) --> B(Run Automated Tests) B --> C{Tests Passing?} C -- No --> D(Reject PR) C -- Yes --> E{PR Review Passed?} E -- No --> D E -- Yes ---> F(Merge PR) If the PR gets approved and code is merged to main branch, a second workflow runs which builds project assets, pushes them to a registry, and deploys the application to the development environment. Note In practice, it's safe to have automated deployments for the development environment. This way, developers or other team members can see immediate results, and test application behavior on the real environment (besides remote development with Tilt ). A typical main branch flow is depicted below: graph TD A(Close PR) --> B(Merge to Main Branch) B --> C(Run Integration Tests) C --> D{Tests Passing?} D -- No --> E(Reject Build) D -- Yes --> F(Build & Tag DEV Images) F --> G(Push DEV Images to Registry) G --> H(Deploy to DEV Environment) Next, you will learn how to implement the CI workflows used in this guide for the online boutique sample application .","title":"Online Boutique Application CI Flows Configuration"},{"location":"05-ci-cd/setup-continuous-integration/#configuring-pull-request-workflow","text":"On each pull request a dedicated workflow is triggered responsible with running automated tests, part of PR validation step. Next, manual review is requested by the developer opening the PR. Reviewers can be assigned also automatically . Follow below steps to configure and enable main branch PR validation workflow provided in this guide: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Create workflows directory (required only once): mkdir -p .github/workflows/ Fetch the CI workflow file from the kubernetes-sample-apps repo: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-pr-ci.yaml \\ -o .github/workflows/online-boutique-pr-ci.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-pr-ci.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-pr-ci.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand Online Boutique PR CI workflow file name : Online Boutique PR CI on : # Used for testing only (can be disabled afterwards) workflow_dispatch : # Uncomment below lines to enable this workflow on main branch PR events pull_request : branches : - main paths-ignore : - \"**/README.md\" - \"kustomize/**\" - \"argocd/**\" - \".github/workflows/*\" # Do not allow concurrent workflows for PRs # If disabled, leads to memory exhaustion on the DOKS dev cluster concurrency : pr-ci-dev # Global environment variables env : DOCR_ENDPOINT : \"registry.digitalocean.com/microservices-demo\" CLUSTER_NAME : \"microservices-demo-dev\" REGION : \"nyc1\" K8S_NAMESPACE : \"microservices-demo-${{ github.event.pull_request.number }}\" PROJECT_NAME : \"online-boutique\" jobs : # Run unit tests in parallel using below matrix to cut down time # Unit tests are standalone and should not raise conflicts # Each microservice is written in a specific language, hence it's added to the matrix unit-tests : runs-on : ubuntu-latest strategy : matrix : include : - project_name : cartservice project_language : csharp - project_name : checkoutservice project_language : golang - project_name : currencyservice project_language : javascript - project_name : emailservice project_language : python - project_name : frontend project_language : golang - project_name : paymentservice project_language : javascript - project_name : productcatalogservice project_language : golang - project_name : recommendationservice project_language : python - project_name : shippingservice project_language : golang steps : - name : Checkout code uses : actions/checkout@v3 with : ref : ${{github.event.pull_request.head.ref}} repository : ${{github.event.pull_request.head.repo.full_name}} - name : Set up Go env if : ${{ matrix.project_language == 'golang' }} uses : actions/setup-go@v3 with : go-version : \"1.19\" - name : Go Unit Tests if : ${{ matrix.project_language == 'golang' }} timeout-minutes : 5 run : | ( cd src/${{ matrix.project_name }} go test ) - name : Set up C# env if : ${{ matrix.project_language == 'csharp' }} uses : actions/setup-dotnet@v2 with : dotnet-version : \"6.0\" include-prerelease : true - name : C# Unit Tests if : ${{ matrix.project_language == 'csharp' }} timeout-minutes : 5 run : dotnet test src/${{ matrix.project_name }}/ - name : Set up NodeJS env if : ${{ matrix.project_language == 'javascript' }} uses : actions/setup-node@v3 with : node-version : 18 - name : Javascript Unit Tests if : ${{ matrix.project_language == 'javascript' }} timeout-minutes : 5 run : | ( cd src/${{ matrix.project_name }} npm install npm run test ) - name : Set up Python env if : ${{ matrix.project_language == 'python' }} uses : actions/setup-python@v3 with : python-version : \"3.7\" - name : Python Unit Tests if : ${{ matrix.project_language == 'python' }} timeout-minutes : 5 run : | ( cd src/${{ matrix.project_name }} pip install -r requirements.txt pytest ) # Run deployment tests (smoke tests) # You can add integration tests as well # Please bear in mind that more tests means increased workflow run time # With workflow concurrency disabled, means more waiting time for other PRs in the queue deployment-tests : needs : unit-tests runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 with : ref : ${{github.event.pull_request.head.ref}} repository : ${{github.event.pull_request.head.repo.full_name}} - name : Install doctl uses : digitalocean/action-doctl@v2 with : token : ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }} - name : Log in to DOCR with short-lived credentials run : doctl registry login --expiry-seconds 1200 - name : Set up K8S tools uses : yokawasa/action-setup-kube-tools@v0.8.2 with : kubectl : \"1.25.2\" kustomize : \"4.5.7\" tilt : \"0.30.9\" - name : Configure kubectl for DOKS with short-lived credentials run : doctl kubernetes cluster kubeconfig save ${{ env.CLUSTER_NAME }} --expiry-seconds 1200 - name : Deploy microservices to DOKS timeout-minutes : 10 run : | # Bring all microservices up using Tilt and wait for all deployments cp tilt-resources/dev/tilt_config.json . tilt ci -- \\ --allowed_contexts \"do-${{ env.REGION }}-${{ env.CLUSTER_NAME }}\" \\ --default_registry \"${{ env.DOCR_ENDPOINT }}\" \\ --environment \"dev\" \\ --namespace \"${{ env.K8S_NAMESPACE }}\" - name : Build loadgenerator image uses : docker/build-push-action@v3 with : context : \"src/loadgenerator\" push : true tags : \"${{ env.DOCR_ENDPOINT }}/loadgenerator\" - name : Smoke tests timeout-minutes : 10 run : | # Prepare load generator # Inject workflow custom docker image sed -i \"s#<LOAD_GENERATOR_IMAGE>#${{ env.DOCR_ENDPOINT }}/loadgenerator#g\" loadgenerator.yaml # Deploy load generator kubectl apply -f loadgenerator.yaml -n ${{ env.K8S_NAMESPACE }} # Wait for load generator deployment to be ready kubectl wait --for=condition=available --timeout=60s deployment/loadgenerator -n ${{ env.K8S_NAMESPACE }} || { # Show why load generator failed to start echo \"[INFO] Load generator pod events\" kubectl describe pod -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | tail -10 exit 1 } # Run smoke tests REQUEST_COUNT=\"0\" while [[ \"$REQUEST_COUNT\" -lt \"50\" ]]; do sleep 5 REQUEST_COUNT=$(kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | grep Aggregated | awk '{print $2}') done # ensure there are no errors hitting endpoints ERROR_COUNT=$(kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | grep Aggregated | awk '{print $3}' | sed \"s/[(][^)]*[)]//g\") if [[ \"$ERROR_COUNT\" -gt \"0\" ]]; then # Print final results echo \"[INFO] Load generator results\" kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | tail -20 exit 1 fi # Print final results echo \"[INFO] Load generator results\" kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | tail -20 working-directory : \"src/loadgenerator\" - name : Clean up Tilt microservices environment if : ${{ always() }} run : | # Remove all microservices and the namespace created by Tilt tilt down --delete-namespaces -- --namespace \"${{ env.K8S_NAMESPACE }}\" - name : Clean up Tilt docker images from registry if : ${{ always() }} run : | # Remove Tilt docker images from registry for tilt_repo in $(docker images --format \"{{.Repository}}:{{.Tag}}\" | grep \"tilt-.*[a-z|0-9]\"); do repo_and_tag=\"${tilt_repo##*/}\" repo=\"${repo_and_tag%%:*}\" tag=\"${repo_and_tag##*:}\" echo \"[INFO] Deleting tag $tag from repo $repo ...\" doctl registry repository delete-tag \"$repo\" \"$tag\" -f done echo \"[INFO] Remember to run the DOCR garbage collector from time to time!\" Save the file and commit changes to your git repository main branch (you may need to disable main branch protection temporarily to perform this step). Explanation for the above configuration: on.pull_request - triggers the Online Boutique PR CI workflow on pull request events only. on.pull_request.branches - triggers the Online Boutique PR CI workflow whenever a pull request event is detected for the specified list of branches. In this case only main branch is desired. on.pull_requests.paths-ignore - list of repository paths used for filtering. The Online Boutique PR CI workflow is triggered only on paths not present in the paths-ignore list. Basically, everything is included (microservices as well), except for Kustomize logic, README files, and workflows logic. Kustomize configuration is dealt in a separate workflow, because it's not related to microservices application logic. env - sets environment variables to use for the whole pipeline. Usually, environment variables control CI workflow behavior (such as telling Tilt how to load environment specific profiles). jobs - defines list of job to run inside the pipeline such as unit tests, deployment tests, etc. strategy.matrix - use a matrix build type. Runs the unit tests in parallel for each project type combination. This approach is a perfect match for projects using multiple components, such as microservices. It also cuts down the time required to build and test each component. Each element from the matrix sets the project name, and the language being used. steps - list of steps to execute as part of the workflow jobs. For each project component (or microservice), the following list of actions is executed: Code checkout, via actions/checkout@v3 . Specific tools are installed based on project language ( actions/setup-dotnet@v2 , actions/setup-go@v3 , etc). Unit tests are executed for each project. For deployment tests, the whole suite of microservices is deployed to the DEV environment in a dedicated namespace (unique across PRs) using Tilt. The dev profile is being used. Then, load tests are performed via the smoke tests step. Finally, clean up is performed. The microservices setup allocated for the PR is destroyed, including associated Docker images pushed to the registry. Following diagram shows the Online Boutique PR CI workflow composition: graph LR A(PR Workflow Start) --> B(Unit Tests Matrix) B --> C(Cart Service <br> Tests) B --> D(Checkout Service <br> Tests) B --> E(Currency Service <br> Tests) B --> F(Email Service <br> Tests) B --> G(Frontend Tests) B --> H(Payment Service <br> Tests) B --> I(Product Catalog Service <br> Tests) B --> J(Recommendation Service <br> Tests) B --> K(Shipping Service <br> Tests) C & D & E & F & G & H & I & J & K --> L(Deployment Tests) L --> M(Env Clean Up) Note In order to run deployment tests, the example CI workflow provided in this guide creates a dedicated Kubernetes namespace to deploy microservices on your target DOKS development cluster. It is suffixed using the PR number which is unique across PRs. When it finishes, it will try to clean up all associated resources such as the provisioned Kubernetes namespace, and docker images from the registry. For docker registry, you still need to run garbage collection (not triggered automatically in the workflow because it puts the DOCR in read-only mode, hence other PR workflows will fail at the docker clean up step). Next, you learn how to configure and enable the main branch workflow that gets triggered whenever changes are pushed to the main branch after each PR merge.","title":"Configuring Pull Request Workflow"},{"location":"05-ci-cd/setup-continuous-integration/#configuring-main-branch-workflow","text":"Whenever a PR is closed and code is merged into the main branch, a dedicated GitHub workflow is triggered. Main purpose of the main branch workflow is to test the whole application as a whole after merging various features via integration tests. Then, it will build and tag images using latest commit id, push them to registry, and finally deploy everything to the development environment. Usually you want the development environment to continuously reflect the latest changes from the development (or main) branch. Notes There's no point to run unit tests again after each PR merge. Unit testing deals with changes for the affected components only. This part it is already taken care in the PR workflow. What makes more sense is to run integration tests as part of the main branch CI flow. This way, you check if application functionality is impacted after merging features in. Adding a GitHub badge in the main project README is another great benefit, giving visual feedback for the latest build state - passing or failing. Follow below steps to configure and enable the main branch workflow provided in this guide: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the CI workflow file from the kubernetes-sample-apps repo: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-main-ci.yaml \\ -o .github/workflows/online-boutique-main-ci.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-main-ci.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-main-ci.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand Online Boutique CI Main workflow file name : Online Boutique CI Main on : # Used for testing only (can be disabled afterwards) workflow_dispatch : # Uncomment below lines to enable this workflow on push to main events push : branches : - main paths-ignore : - \"**/README.md\" - \"kustomize/**\" - \"argocd/**\" - \".github/workflows/*\" # Do not allow concurrent workflows # Changes should be delivered one a time as code is merged into the main branch concurrency : main-ci-dev # Global environment variables env : CI_COMMIT_AUTHOR : \"CI GitHub Actions\" CI_COMMIT_AUTHOR_EMAIL : \"gh-actions@noreply.github.com\" DOCR_ENDPOINT : \"registry.digitalocean.com/microservices-demo\" PROJECT_NAME : \"online-boutique\" jobs : # There's no point to run unit tests again after PR merge # Unit testing deals with changes for the affected components, # and that it is already taken care in the PR workflow # What it makes sense, is to run integration tests, # to see if the whole application is impacted after merging the changes # # Run deployment tests (integration tests) # Please bear in mind that more tests means increased workflow run time # With workflow concurrency disabled, means more waiting time for other PRs in the queue deployment-tests : runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 - name : Run integration tests run : | echo \"[INFO] Not implemented yet!\" # Build and push project images in parallel using a matrix strategy # Cuts down build time build-and-push-images : needs : deployment-tests runs-on : ubuntu-latest strategy : matrix : project : - cartservice - checkoutservice - currencyservice - emailservice - frontend - paymentservice - productcatalogservice - recommendationservice - shippingservice steps : - name : Checkout code uses : actions/checkout@v3 - name : Install doctl uses : digitalocean/action-doctl@v2 with : token : ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }} - name : Log in to DOCR with short-lived credentials run : doctl registry login --expiry-seconds 600 - name : Build and push image uses : docker/build-push-action@v3 with : # cartservice is an exception - Dockerfile is placed in src/cartservice/src subfolder context : \"src/${{ matrix.project }}/${{ matrix.project == 'cartservice' && 'src' || ''}}\" push : true tags : \"${{ env.DOCR_ENDPOINT }}/${{ matrix.project }}:${{ github.sha }}\" # Kustomize image field for each microservice present in the `src/` dir # Finally, commit changes to main branch and let ArgoCD take over afterwards apply-kustomize-changes : needs : build-and-push-images runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 - name : Set up K8S tools uses : yokawasa/action-setup-kube-tools@v0.8.2 with : kustomize : \"4.5.7\" - name : Kustomize dev environment images run : | for microservice in src/*/; do microservice=\"$(basename $microservice)\" if [[ \"$microservice\" == \"loadgenerator\" ]]; then continue fi ( cd kustomize/dev kustomize edit set image $microservice=${{ env.DOCR_ENDPOINT }}/${microservice}:${{ github.sha }} ) done - name : Commit Kustomize manifests for dev env run : | git config --global user.name \"${{ env.CI_COMMIT_AUTHOR }}\" git config --global user.email \"${{ env.CI_COMMIT_AUTHOR_EMAIL }}\" git add kustomize/dev/ git commit -m \"Bump docker images tag\" - name : Push changes uses : ad-m/github-push-action@master with : github_token : ${{ secrets.GITHUB_TOKEN }} Save the file and commit changes to your git repository main branch (you may need to disable main branch protection temporarily to perform this step). Explanation for the above configuration: on.push - triggers the Online Boutique Main CI workflow on push events only. on.push.branches - triggers the Online Boutique Main CI workflow whenever a push event is detected for the specified list of branches. In this case only main branch is desired. on.push.paths-ignore - list of repository paths used for filtering. The Online Boutique Main CI workflow is triggered only on paths not present in the paths-ignore list. Basically, everything is included (microservices as well), except for Kustomize logic, README files, and workflows logic. Kustomize configuration is dealt in a separate workflow, because it's not related to microservices application logic. env - sets environment variables to use for the whole pipeline. Usually, environment variables control CI workflow behavior (such as telling ArgoCD how to sync only for the specific environment). jobs - defines list of job to run inside the pipeline such as integration tests, build and push docker images, apply Kustomize changes, etc. steps - list of steps implementing workflow jobs logic. Following diagram shows the Online Boutique Main CI workflow composition: graph TD A(Main Branch Workflow Start) --> B(Deployment Tests) B --> C{Tests Passing?} C -- No --> D(Reject build) C -- Yes --> E(Build and Push Images) E --> F(Kustomize DEV Images Tags) F --> G(Commit Kustomize Changes) G --> H(Set Build Passing Status Badge) H -.-> I(ArgoCD DEV Sync) style I stroke-dasharray: 5 5 Notes The deployment tests step contains only a placeholder for integration tests. This part is implementation specific and it is left for the user to configure. The final ArgoCD sync step from above diagram is marked as optional (dotted representation) because Argo will automatically trigger a deployment for the DEV environment anyways. Depending how Argo is configured, it can happen immediately via GitHub webhooks, or via polling (3 minutes interval by default). You can also force a sync and avoid waiting time via the CI workflow if desired. Next, you will test each workflow to check if it meets the required functionality.","title":"Configuring Main Branch Workflow"},{"location":"05-ci-cd/setup-continuous-integration/#testing-the-online-boutique-application-github-workflows","text":"In this section you will test each workflow functionality for each event - pull requests and main branch code merge.","title":"Testing the Online Boutique Application GitHub Workflows"},{"location":"05-ci-cd/setup-continuous-integration/#testing-the-pull-request-workflow","text":"Follow below steps to test the PR workflow for the online boutique application: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Checkout to a new branch to add a new feature: git checkout -b features/musical_instruments Open the header template file for the frontend microservice using a text editor of your choice, preferably with HTML lint support. For example, you can use VS Code : code src/frontend/templates/header.html Change the Cymbal Shops string from title section to something different (e.g. Musical Instruments ): < title > {{ if $.is_cymbal_brand }} Musical Instruments {{ else }} Online Boutique {{ end }} </ title > Tip You can quickly spin up a local environment using Tilt to test the changes first, as explained in the local development using Tilt section. You will get live updates for each change which helps in the iterative development process. Save changes, commit and push new branch to remote: git commit -am \"New feature - musical instruments.\" git push origin features/musical_instruments Navigate to your GitHub repository page, and open a new pull request against main branch using the features/musical_instruments branch as the base. At this point, the Online Boutique PR CI should kick in. Branch merging should be blocked until the workflow finishes successfully, and at least one approval is present (explained in the prerequisites section): Next, navigate to the Actions tab of your GitHub repository and inspect PR workflow progress: If everything goes as planned, the workflow should pass and the PR can be merged. This is the happy flow. You should also check what happens if introducing a change that breaks things - change one of the unit tests so that it doesn't compile successfully. Next, you will check the main branch workflow and see if applies required kustomize changes for the development environment.","title":"Testing the Pull Request Workflow"},{"location":"05-ci-cd/setup-continuous-integration/#testing-the-main-branch-workflow","text":"The second workflow should automatically start after merging code into main branch via the pull request created in the previous step. Navigate to the actions tab of your GitHub repository to see it in action: Note Automated deployments to the development environment cluster are disabled for now until configuring continuous deployments via ArgoCD in the next chapter. So, nothing gets deployed yet to the dev environment. You should also see a new commit added in the Git history of your repo. It shows kustomize changes for each image that was built and tagged using latest PR merge commit id: Going further, you should be able to enhance the CI process even more by following some additional best practices, such as: Run tests only for affected microservices. Should improve workflow run time even more. Strategy matrix used to speed up actions inside the workflow such as running unit tests and building docker images is static. It is possible to create a strategy matrix with dynamic values. For example, the tilt_config.json file from the dev profile already contains the list of project microservices, so the matrix can read the values directly from the list. Use artifacts caching inside workflows whenever possible. Decouple application code and Kubernetes configuration by using a dedicated Git repository. Application code should not be dependent on Kubernetes configuration, hence it make sense to stay in a different repository. Also, whenever changes are required on the Kubernetes side, you don't need to open PRs and merge Kubernetes specific commits into the application repository. Next, ArgoCD should pickup the changes and deploy the application automatically to your development environment DOKS cluster. In order for this part to work you need to set it up in the following section.","title":"Testing the Main Branch Workflow"},{"location":"05-ci-cd/setup-release-process/","text":"Warning NOT FINISHED YET - WORK IN PROGRESS!!! Managing Application Releases The release process is pretty straightforward, and it's based on semantic versioning: Semantic versioning is used to distinguish between each release. First version starts at v1.0.0 . For each release the version is increased by a single value . For each release a corresponding git tag is created. Using tags helps verify changes introduced by a specific version in time. Also, in case something goes bad you can rebuild the assets at that point in time. A dedicated workflow runs for tag events which builds and pushes the project docker images using release tag version. Releasing a new version for a project is a separate process, and it is usually triggered manually. GitHub offers the possibility to create and manage releases via the web interface in a very straightforward manner. Releasing a new version for the online boutique sample application consists of: Creating a new GitHub release: Use tagging based on semantic versioning (e.g. v1.0.0) Generate release changelog Automatically trigger a release GitHub workflow which does the following: Checks the latest release tag and corresponding version Builds and tags images for the affected microservice(s) Pushes release artifacts to registry (docker images) Promoting to Upper Environments It's not best practice to immediately deploy to production after a new release was made. You will want to deploy to staging environment first, then have QA team check and approve the release. If everything is ok, then you will move forward and deploy to production. Promoting a new release to upper environments should happen in a controlled manner via PRs, and it is described below. Why use PRs? Because you can create atomic commits, and revert the same way in case something goes bad. I. Promoting to staging environment: Prerequisites: A new GitHub release for the project was made. Release artifacts already published. Steps: Create a new PR with Kustomize changes for the staging overlay. New release tag is included in the images section. A Kustomize validation workflow is automatically triggered. Manual review is also required. If all checks pass, PR is closed, and code merged in the main branch. ArgoCD picks the changes and deploys new application artifacts to staging environment. II. Promoting to production environment: Prerequisites: A new GitHub release for the project was made. Release artifacts already published. Application was already tested and QA team approved it for staging environment. Steps: Create a new PR with Kustomize changes for the prod overlay. New release tag is included in the images section. A Kustomize validation workflow is automatically triggered. Manual review is also required. If all hecks pass, PR is closed, and code merged in the main branch. ArgoCD picks the changes and deploys new application artifacts to prod environment. Rolling Back a Release In case something goes bad, you can immediately revert the associated PR for the respective environment. This approach ensures atomic changes and rollbacks. Next, ArgoCD will pick the changes automatically and rolls back to previous deployment. Other Concerns Application configuration drift issues ? Reasoning: Everything is tracked via the main branch which is constantly being developed. How can we ensure application configuration stays immutable? The affected part is Kustomize. Possible solution: Use Git tags. Then, point ArgoCD to use a release tag.","title":"Set up the release process"},{"location":"05-ci-cd/setup-release-process/#managing-application-releases","text":"The release process is pretty straightforward, and it's based on semantic versioning: Semantic versioning is used to distinguish between each release. First version starts at v1.0.0 . For each release the version is increased by a single value . For each release a corresponding git tag is created. Using tags helps verify changes introduced by a specific version in time. Also, in case something goes bad you can rebuild the assets at that point in time. A dedicated workflow runs for tag events which builds and pushes the project docker images using release tag version. Releasing a new version for a project is a separate process, and it is usually triggered manually. GitHub offers the possibility to create and manage releases via the web interface in a very straightforward manner. Releasing a new version for the online boutique sample application consists of: Creating a new GitHub release: Use tagging based on semantic versioning (e.g. v1.0.0) Generate release changelog Automatically trigger a release GitHub workflow which does the following: Checks the latest release tag and corresponding version Builds and tags images for the affected microservice(s) Pushes release artifacts to registry (docker images)","title":"Managing Application Releases"},{"location":"05-ci-cd/setup-release-process/#promoting-to-upper-environments","text":"It's not best practice to immediately deploy to production after a new release was made. You will want to deploy to staging environment first, then have QA team check and approve the release. If everything is ok, then you will move forward and deploy to production. Promoting a new release to upper environments should happen in a controlled manner via PRs, and it is described below. Why use PRs? Because you can create atomic commits, and revert the same way in case something goes bad. I. Promoting to staging environment: Prerequisites: A new GitHub release for the project was made. Release artifacts already published. Steps: Create a new PR with Kustomize changes for the staging overlay. New release tag is included in the images section. A Kustomize validation workflow is automatically triggered. Manual review is also required. If all checks pass, PR is closed, and code merged in the main branch. ArgoCD picks the changes and deploys new application artifacts to staging environment. II. Promoting to production environment: Prerequisites: A new GitHub release for the project was made. Release artifacts already published. Application was already tested and QA team approved it for staging environment. Steps: Create a new PR with Kustomize changes for the prod overlay. New release tag is included in the images section. A Kustomize validation workflow is automatically triggered. Manual review is also required. If all hecks pass, PR is closed, and code merged in the main branch. ArgoCD picks the changes and deploys new application artifacts to prod environment.","title":"Promoting to Upper Environments"},{"location":"05-ci-cd/setup-release-process/#rolling-back-a-release","text":"In case something goes bad, you can immediately revert the associated PR for the respective environment. This approach ensures atomic changes and rollbacks. Next, ArgoCD will pick the changes automatically and rolls back to previous deployment.","title":"Rolling Back a Release"},{"location":"05-ci-cd/setup-release-process/#other-concerns","text":"Application configuration drift issues ? Reasoning: Everything is tracked via the main branch which is constantly being developed. How can we ensure application configuration stays immutable? The affected part is Kustomize. Possible solution: Use Git tags. Then, point ArgoCD to use a release tag.","title":"Other Concerns"}]}